{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fc20f10",
   "metadata": {},
   "source": [
    "# Few shot prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92b35966",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\transformers\\utils\\generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\transformers\\utils\\generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\transformers\\utils\\generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "# from huggingface_hub import hf_hub_download \n",
    "import torch\n",
    "from datasets import load_dataset, load_from_disk\n",
    "import evaluate\n",
    "import transformers\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSequenceClassification\n",
    "from transformers import TrainingArguments, Trainer\n",
    "#from peft import LoraConfig, TaskType, get_peft_model\n",
    "#from peft import PeftConfig, PeftModel\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb18f771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.35.2\n"
     ]
    }
   ],
   "source": [
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ebbe1dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0+cu118\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd6f5ac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2022 NVIDIA Corporation\n",
      "Built on Wed_Sep_21_10:41:10_Pacific_Daylight_Time_2022\n",
      "Cuda compilation tools, release 11.8, V11.8.89\n",
      "Build cuda_11.8.r11.8/compiler.31833905_0\n"
     ]
    }
   ],
   "source": [
    "! nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc22d145",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3377791e",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_repo_dir = 'D:/projects/LLM'\n",
    "cache_dir = '/cygdrive/d/projects/LLM/.cache'\n",
    "os.environ['TRANSFORMERS_CACHE'] = cache_dir\n",
    "os.environ['HF_HOME'] = cache_dir + '/huggingface'\n",
    "os.environ['XDG_CACHE_HOME'] = cache_dir\n",
    "os.environ['HF_DATASETS_CACHE'] = cache_dir\n",
    "\n",
    "## mistral\n",
    "#model_name = \"mistralai/Mistral-7B-v0.1\"\n",
    "#model_path = cache_dir + ''\n",
    "\n",
    "## falcon\n",
    "model_name = \"tiiuae/falcon-7b-instruct\"\n",
    "model_path = cache_dir + '/models--tiiuae--falcon-7b-instruct/snapshots/cf4b3c42ce2fdfe24f753f0f0d179202fea59c99'\n",
    "\n",
    "## gpt2\n",
    "#model_name = 'gpt2'\n",
    "#model_path = cache_dir + '/models--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e'\n",
    "\n",
    "## try gpt-neo\n",
    "\n",
    "\n",
    "dataset_path = cache_dir + '/parquet/yelp_polarity' # cache_dir + '/parquet/yelp_review_full-e22176106d6e7534'\n",
    "dataset_name = 'yelp_polarity' # yelp_review_full\n",
    "#tokenized_data_path = cache_dir + '/tokenized_dataset_yelp_polarity_gpt2'\n",
    "#tokenized_data_path = cache_dir + '/tokenized_dataset_yelp_polarity_mistral7b'\n",
    "tokenized_data_path = cache_dir + '/tokenized_dataset_yelp_polarity_falcon7b'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "36b5125b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "252c515a3194426494ae112b1990a67c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/287 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\huggingface_hub\\file_download.py:148: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in D:\\cygdrive\\d\\projects\\LLM\\.cache\\models--tiiuae--falcon-7b-instruct. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7eb82b63b5f4806832c922ed964ee71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.73M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dc2e042b7e74ce0be758f7d0197ce65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/281 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cd174bccc384ca990486cf60570e7f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.05k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a891b6e944d9490eb546b73e47cce2ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin.index.json:   0%|          | 0.00/16.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c49b58356eee4a3ea75db0bd8c6df7be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "688ea66569b64d52b4d269dd2288be3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00001-of-00002.bin:   0%|          | 0.00/9.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "849e1e5705b940809989e440f52ac335",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00002-of-00002.bin:   0%|          | 0.00/4.48G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c9015962ab6481e88aba72189767468",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c06b612a93054ce0bea7dfb1e791f213",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/117 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You shouldn't move a model that is dispatched using accelerate hooks.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "`.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 21\u001b[0m\n\u001b[0;32m     17\u001b[0m     model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_path, \n\u001b[0;32m     18\u001b[0m                                              cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[0;32m     19\u001b[0m                                              local_files_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     20\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 21\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# use GPU\u001b[39;00m\n",
      "File \u001b[1;32mD:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\accelerate\\big_modeling.py:456\u001b[0m, in \u001b[0;36mdispatch_model.<locals>.add_warning.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    454\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m param\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    455\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt move a model that has some modules offloaded to cpu or disk.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\transformers\\modeling_utils.py:2249\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2245\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule\u001b[38;5;241m.\u001b[39mto)\n\u001b[0;32m   2246\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   2247\u001b[0m     \u001b[38;5;66;03m# Checks if the model has been loaded in 8-bit\u001b[39;00m\n\u001b[0;32m   2248\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantization_method\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m QuantizationMethod\u001b[38;5;241m.\u001b[39mBITS_AND_BYTES:\n\u001b[1;32m-> 2249\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2250\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2251\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m model has already been set to the correct devices and casted to the correct `dtype`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2252\u001b[0m         )\n\u001b[0;32m   2253\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantization_method\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m QuantizationMethod\u001b[38;5;241m.\u001b[39mGPTQ:\n\u001b[0;32m   2254\u001b[0m         \u001b[38;5;66;03m# For GPTQ models, we prevent users from casting the model to another dytpe to restrict unwanted behaviours.\u001b[39;00m\n\u001b[0;32m   2255\u001b[0m         \u001b[38;5;66;03m# the correct API should be to load the model with the desired dtype directly through `from_pretrained`.\u001b[39;00m\n\u001b[0;32m   2256\u001b[0m         dtype_present_in_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`."
     ]
    }
   ],
   "source": [
    "if not os.path.isfile(model_path + '/tokenizer.json'):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, \n",
    "                                              cache_dir=cache_dir,\n",
    "                                              #torch_dtype=torch.bfloat16,\n",
    "                                              device_map='auto',\n",
    "                                              load_in_4bit=True) # to load tokenizer to cache\n",
    "else:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path, \n",
    "                                              cache_dir=cache_dir, \n",
    "                                              local_flies_only=True)\n",
    "if not os.path.isfile(model_path + '/model.safetensors'):\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                                 cache_dir=cache_dir,\n",
    "                                                 device_map='auto',\n",
    "                                                 load_in_4bit=True)\n",
    "else:\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_path, \n",
    "                                             cache_dir=cache_dir,\n",
    "                                             local_files_only=True)\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "# model.to(device) # use GPU. Do not need this if using load_in_4bit as it's already been set to the correct devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fdc3ed38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset yelp_polarity (D:/cygdrive/d/projects/LLM/.cache/parquet/yelp_polarity/plain_text/1.0.0/14f90415c754f47cf9087eadac25823a395fef4400c7903c5897f55cfaaa6f61)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb5641c8dce2463bb6f24217c594e8ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if not os.path.isdir(dataset_path):\n",
    "    dataset = load_dataset(dataset_name, cache_dir=cache_dir + '/parquet')\n",
    "else:\n",
    "    dataset = load_dataset(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b8f7ea55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd36806d199f4229a9316a731ca51c80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1d58117818948079ad5919dbff67ae3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(cache_dir + '/parquet/yelp_polarity')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a6afc7",
   "metadata": {},
   "source": [
    "## Using model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d2cd8259",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\" \n",
    "Classify the text into neutral, negative or positive.\n",
    "Text: This movie is definitely one of my favorite movies of its kind. The interaction between respectable and morally strong characters is an ode to chivalry and the honor code amongst thieves and policemen.\n",
    "Sentiment: positive\n",
    "\n",
    "###\n",
    "\n",
    "Text: This movie was actually pretty funny.\n",
    "Sentiment: positive\n",
    "\n",
    "###\n",
    "\n",
    "Text: I hated this movie, it sucks.\n",
    "Sentiment: negative\n",
    "\n",
    "###\n",
    "\n",
    "Text: A Perfect Spy, published in 1986, is an excellent example of the genre and an exceptionally well-written novel.\n",
    "Sentiment: \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "982a9e59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\bitsandbytes\\nn\\modules.py:426: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn(\n",
      "The current implementation of Falcon calls `torch.scaled_dot_product_attention` directly, this will be deprecated in the future in favor of the `BetterTransformer` API. Please install the latest optimum library with `pip install -U optimum` and call `model.to_bettertransformer()` to benefit from `torch.scaled_dot_product_attention` and future performance optimizations.\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\transformers\\models\\falcon\\modeling_falcon.py:469: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = F.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Classify the text into neutral, negative or positive.\n",
      "Text: This movie is definitely one of my favorite movies of its kind. The interaction between respectable and morally strong characters is an ode to chivalry and the honor code amongst thieves and policemen.\n",
      "Sentiment: positive\n",
      "\n",
      "###\n",
      "\n",
      "Text: This movie was actually pretty funny.\n",
      "Sentiment: positive\n",
      "\n",
      "###\n",
      "\n",
      "Text: I hated this movie, it sucks.\n",
      "Sentiment: negative\n",
      "\n",
      "###\n",
      "\n",
      "Text: A Perfect Spy, published in 1986, is an excellent example of the genre and an exceptionally well-written novel.\n",
      "Sentiment: \n",
      "\n",
      "Classification:\n",
      "\n",
      "1. Neutral:\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "generated_ids = model.generate(input_ids, \n",
    "                               do_sample=True, \n",
    "                               temperature=0.2, \n",
    "                               max_new_tokens=10)\n",
    "generated_text = tokenizer.decode(generated_ids[0])\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9fbc3acb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Classify the text into neutral, negative or positive.\n",
      "Text: This movie is definitely one of my favorite movies of its kind. The interaction between respectable and morally strong characters is an ode to chivalry and the honor code amongst thieves and policemen.\n",
      "Sentiment: positive\n",
      "\n",
      "###\n",
      "\n",
      "Text: This movie was actually pretty funny.\n",
      "Sentiment: positive\n",
      "\n",
      "###\n",
      "\n",
      "Text: I hated this movie, it sucks.\n",
      "Sentiment: negative\n",
      "\n",
      "###\n",
      "\n",
      "Text: A Perfect Spy, published in 1986, is an excellent example of the genre and an exceptionally well-written novel.\n",
      "Sentiment: \n",
      "###\n",
      "\n",
      "Text: In this book, he\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "generated_ids = model.generate(input_ids, \n",
    "                               do_sample=True, \n",
    "                               max_new_tokens=10,\n",
    "                               top_k=10)\n",
    "generated_text = tokenizer.decode(generated_ids[0])\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "62f7b0aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Classify the text into neutral, negative or positive.\n",
      "Text: This movie is definitely one of my favorite movies of its kind. The interaction between respectable and morally strong characters is an ode to chivalry and the honor code amongst thieves and policemen.\n",
      "Sentiment: positive\n",
      "\n",
      "###\n",
      "\n",
      "Text: This movie was actually pretty funny.\n",
      "Sentiment: positive\n",
      "\n",
      "###\n",
      "\n",
      "Text: I hated this movie, it sucks.\n",
      "Sentiment: negative\n",
      "\n",
      "###\n",
      "\n",
      "Text: A Perfect Spy, published in 1986, is an excellent example of the genre and an exceptionally well-written novel.\n",
      "Sentiment: \n",
      "\n",
      "Classification of the text into neutral, negative\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "generated_ids = model.generate(input_ids, \n",
    "                               do_sample=True, \n",
    "                               max_new_tokens=10,\n",
    "                               top_p=10)\n",
    "generated_text = tokenizer.decode(generated_ids[0])\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8e5448bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Classify the text into neutral, negative or positive.\n",
      "Text: This movie is definitely one of my favorite movies of its kind. The interaction between respectable and morally strong characters is an ode to chivalry and the honor code amongst thieves and policemen.\n",
      "Sentiment: positive\n",
      "\n",
      "###\n",
      "\n",
      "Text: This movie was actually pretty funny.\n",
      "Sentiment: positive\n",
      "\n",
      "###\n",
      "\n",
      "Text: I hated this movie, it sucks.\n",
      "Sentiment: negative\n",
      "\n",
      "###\n",
      "\n",
      "Text: A Perfect Spy, published in 1986, is an excellent example of the genre and an exceptionally well-written novel.\n",
      "Sentiment: \n",
      "\n",
      "##\n",
      "\n",
      "Text: This movie symbolically\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "generated_ids = model.generate(input_ids, \n",
    "                               do_sample=True, \n",
    "                               max_new_tokens=10,\n",
    "                               top_p=10,\n",
    "                               top_k=0)\n",
    "generated_text = tokenizer.decode(generated_ids[0])\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ea1ee6",
   "metadata": {},
   "source": [
    "## Using pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52aa7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "print(next(model.parameters()).is_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98300e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"Classify the text into neutral, negative or positive. \n",
    "Text: This movie is definitely one of my favorite movies of its kind. The interaction between respectable and morally strong characters is an ode to chivalry and the honor code amongst thieves and policemen.\n",
    "Sentiment: positive.\n",
    "Text: A Perfect Spy, published in 1986, is an excellent example of the genre and an exceptionally well-written novel.\n",
    "Sentiment: \n",
    "\"\"\"\n",
    "\n",
    "sequences = pipe(\n",
    "    prompt,\n",
    "    max_new_tokens=30,\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    return_full_text = False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc184c5",
   "metadata": {},
   "source": [
    "\n",
    "## Use case: topic modeling\n",
    "\n",
    "### Questions\n",
    "- How to sample topics for prompting?\n",
    "- How to do query rewriting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fe3f15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLMpy38",
   "language": "python",
   "name": "llmpy38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
