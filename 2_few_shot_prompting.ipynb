{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fc20f10",
   "metadata": {},
   "source": [
    "# Few shot prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92b35966",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\transformers\\utils\\generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\transformers\\utils\\generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\transformers\\utils\\generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "# from huggingface_hub import hf_hub_download \n",
    "import torch\n",
    "from datasets import load_dataset, load_from_disk\n",
    "import evaluate\n",
    "import transformers\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSequenceClassification, AutoModelForSeq2SeqLM\n",
    "from transformers import TrainingArguments, Trainer\n",
    "#from peft import LoraConfig, TaskType, get_peft_model\n",
    "#from peft import PeftConfig, PeftModel\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb18f771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.35.2\n"
     ]
    }
   ],
   "source": [
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ebbe1dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0+cu118\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd6f5ac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2022 NVIDIA Corporation\n",
      "Built on Wed_Sep_21_10:41:10_Pacific_Daylight_Time_2022\n",
      "Cuda compilation tools, release 11.8, V11.8.89\n",
      "Build cuda_11.8.r11.8/compiler.31833905_0\n"
     ]
    }
   ],
   "source": [
    "! nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc22d145",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True)\n",
    "\n",
    "def load_tokenizer_llm(AutoModelForClass, model_name, model_path, cache_dir, to_device=True, **kwargs):\n",
    "    \"\"\"\n",
    "    if use load_in_4bit=True, do not set to_device=True\n",
    "    kwargs:\n",
    "        - device_map\n",
    "        - torch_dtype\n",
    "        - load_in_4bit\n",
    "    \"\"\"\n",
    "    if not os.path.isfile(model_path + '/tokenizer.json'):\n",
    "        print('no existing tokenizer found. Download from HF')\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name, \n",
    "                                                  cache_dir=cache_dir,\n",
    "                                                  **kwargs\n",
    "                                                 ) # to load tokenizer to cache\n",
    "    else:\n",
    "        print('existing tokenizer found. Load from local')\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_path, \n",
    "                                                  cache_dir=cache_dir, \n",
    "                                                  local_flies_only=True)\n",
    "    if not os.path.isfile(model_path + '/model.safetensors'):\n",
    "        print('no existing model found. Download from HF')\n",
    "        model = AutoModelForClass.from_pretrained(model_name,\n",
    "                                                     cache_dir=cache_dir,\n",
    "                                                     **kwargs\n",
    "                                                    )\n",
    "    else:\n",
    "        print('existing model found. Load from local')\n",
    "        model = AutoModelForClass.from_pretrained(model_path, \n",
    "                                                 cache_dir=cache_dir,\n",
    "                                                 local_files_only=True)\n",
    "    \n",
    "    if to_device:\n",
    "        device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "        #device.reset()\n",
    "        model.to(device) # use GPU. Do not need this if using load_in_4bit as it's already been set to the correct devices\n",
    "        \n",
    "    return tokenizer, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3377791e",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_repo_dir = 'D:/projects/LLM'\n",
    "cache_dir = '/cygdrive/d/projects/LLM/.cache'\n",
    "os.environ['TRANSFORMERS_CACHE'] = cache_dir\n",
    "os.environ['HF_HOME'] = cache_dir + '/huggingface'\n",
    "os.environ['XDG_CACHE_HOME'] = cache_dir\n",
    "os.environ['HF_DATASETS_CACHE'] = cache_dir\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "## mistral\n",
    "#model_name = \"mistralai/Mistral-7B-v0.1\"\n",
    "#model_path = cache_dir + ''\n",
    "\n",
    "## falcon - decoder only\n",
    "#model_name = \"tiiuae/falcon-7b-instruct\"\n",
    "#model_path = cache_dir + '/models--tiiuae--falcon-7b-instruct/snapshots/cf4b3c42ce2fdfe24f753f0f0d179202fea59c99'\n",
    "\n",
    "## gpt2 - decoder only\n",
    "#model_name = 'gpt2'\n",
    "#model_path = cache_dir + '/models--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e'\n",
    "\n",
    "## T5-small - encoder-decoder\n",
    "#model_name = 'google-t5/t5-small'\n",
    "#model_path = cache_dir + '/models--google-t5--t5-small/snapshots/df1b051c49625cf57a3d0d8d3863ed4d13564fe4'\n",
    "\n",
    "## T5-3b\n",
    "model_name = 'google-t5/t5-3b'\n",
    "model_path = cache_dir + '/models--google-t5--t5-3b/snapshots/bed96aab9ee46012a5046386105ee5fd0ac572f0'\n",
    "\n",
    "dataset_path = cache_dir + '/parquet/yelp_polarity' # cache_dir + '/parquet/yelp_review_full-e22176106d6e7534'\n",
    "dataset_name = 'yelp_polarity' # yelp_review_full\n",
    "#tokenized_data_path = cache_dir + '/tokenized_dataset_yelp_polarity_gpt2'\n",
    "#tokenized_data_path = cache_dir + '/tokenized_dataset_yelp_polarity_mistral7b'\n",
    "#tokenized_data_path = cache_dir + '/tokenized_dataset_yelp_polarity_falcon7b'\n",
    "#tokenized_data_path = cache_dir + '/tokenized_dataset_yelp_polarity_t5small'\n",
    "tokenized_data_path = cache_dir + '/tokenized_dataset_yelp_polarity_t53b'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "db537fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "existing tokenizer found. Load from local\n",
      "existing model found. Load from local\n"
     ]
    }
   ],
   "source": [
    "AutoModelForClass = AutoModelForSeq2SeqLM\n",
    "to_device = True\n",
    "tokenizer, model = load_tokenizer_llm(AutoModelForClass, model_name, model_path, cache_dir, to_device=to_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fdc3ed38",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(dataset_path):\n",
    "    dataset = load_dataset(dataset_name, cache_dir=cache_dir + '/parquet')\n",
    "else:\n",
    "    dataset = load_dataset(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "99f38335",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Unfortunately, the frustration of being Dr. Goldberg's patient is a repeat of the experience I've had with so many other doctors in NYC -- good doctor, terrible staff.  It seems that his staff simply never answers the phone.  It usually takes 2 hours of repeated calling to get an answer.  Who has time for that or wants to deal with it?  I have run into this problem with many other doctors and I just don't get it.  You have office workers, you have patients with medical needs, why isn't anyone answering the phone?  It's incomprehensible and not work the aggravation.  It's with regret that I feel that I have to give Dr. Goldberg 2 stars.\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train']['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "db5cee6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Been going to Dr. Goldberg for over 10 years. I think I was one of his 1st patients when he started at MHMG. He's been great over the years and is really all about the big picture. It is because of him, not my now former gyn Dr. Markoff, that I found out I have fibroids. He explores all options with you and is very patient and understanding. He doesn't judge and asks all the right questions. Very thorough and wants to be kept in the loop on every aspect of your medical health and your life.\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train']['text'][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6118f29a",
   "metadata": {},
   "source": [
    "## zero-shot prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d2cd8259",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\" \n",
    "Summarize: {0}\n",
    "\"\"\".format(dataset['train']['text'][0]).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "995bcbbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Summarize: Unfortunately, the frustration of being Dr. Goldberg's patient is a repeat of the experience I've had with so many other doctors in NYC -- good doctor, terrible staff.  It seems that his staff simply never answers the phone.  It usually takes 2 hours of repeated calling to get an answer.  Who has time for that or wants to deal with it?  I have run into this problem with many other doctors and I just don't get it.  You have office workers, you have patients with medical needs, why isn't anyone answering the phone?  It's incomprehensible and not work the aggravation.  It's with regret that I feel that I have to give Dr. Goldberg 2 stars.\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "982a9e59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". to to to.... I'm not sure if I'll be back. I'm not sure if I'll be recommending him to anyone.\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "generated_ids = model.generate(input_ids, \n",
    "                               do_sample=False,\n",
    "                               max_new_tokens=100)\n",
    "generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9fbc3acb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". to to to.... I wish I could give him 3. I've been a patient of Dr. Goldberg for over 15 years. He is a great doctor.\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "generated_ids = model.generate(input_ids, \n",
    "                               do_sample=True, \n",
    "                               temperature=0.3,\n",
    "                               max_new_tokens=100,\n",
    "                               top_k=10)\n",
    "generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "62f7b0aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". to to to.... I would give him a 3 star rating if I could. Dr. Goldberg is a good doctor, but his staff is awful.\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "generated_ids = model.generate(input_ids, \n",
    "                               do_sample=True,\n",
    "                               temperature=0.3,\n",
    "                               max_new_tokens=100,\n",
    "                               top_p=10)\n",
    "generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8e5448bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". to to to.... I've had a great experience with him and I would recommend him to anyone. I would give him 3 stars if I could.\n",
      "CPU times: total: 35.5 s\n",
      "Wall time: 53.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "generated_ids = model.generate(input_ids, \n",
    "                               do_sample=True, \n",
    "                               temperature=0.3,\n",
    "                               max_new_tokens=100,\n",
    "                               top_p=5,\n",
    "                               top_k=5)\n",
    "generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d90112",
   "metadata": {},
   "source": [
    "## few-shot prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb45eb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d69669cc",
   "metadata": {},
   "source": [
    "## CoT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a816927d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cbc184c5",
   "metadata": {},
   "source": [
    "\n",
    "## Use case: topic modeling\n",
    "\n",
    "Use few-shot prompting to determine (up to three) topic(s) of a comment. Use comments with existing topics as example prompts.\n",
    "\n",
    "**Question**: will the LLM be able to determine a new topic that is outside of the scope of the existing topics in the examples?\n",
    "\n",
    "First experiment with ChatGPT (~125b parameters). If it is able to do a good job, then we can try smaller models (7-70b params)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fe3f15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLMpy38",
   "language": "python",
   "name": "llmpy38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
