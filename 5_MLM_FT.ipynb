{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28487383",
   "metadata": {},
   "source": [
    "## Reference\n",
    "[Fine-Tuning BERT with Masked Language Modeling](https://www.analyticsvidhya.com/blog/2022/09/fine-tuning-bert-with-masked-language-modeling/)\n",
    "\n",
    "For loss function, see [ref](https://discuss.huggingface.co/t/bertformaskedlm-s-loss-and-scores-how-the-loss-is-computed/607/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "e2d9479e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface_hub import hf_hub_download \n",
    "import torch\n",
    "from datasets import load_dataset, load_from_disk\n",
    "import evaluate\n",
    "\n",
    "import transformers\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSequenceClassification, AutoModelForSeq2SeqLM\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import BertTokenizer, TFBertForMaskedLM, BertForMaskedLM\n",
    "import tensorflow as tf\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "from peft import PeftConfig, PeftModel\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "from torch import nn\n",
    "from huggingface_hub import notebook_login\n",
    "from accelerate import init_empty_weights, load_checkpoint_and_dispatch, infer_auto_device_map, dispatch_model\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA, NMF, non_negative_factorization\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import os\n",
    "import glob\n",
    "import itertools\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4ca885ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.35.2\n"
     ]
    }
   ],
   "source": [
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d1e03ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0+cu118\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7fb28090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2022 NVIDIA Corporation\n",
      "Built on Wed_Sep_21_10:41:10_Pacific_Daylight_Time_2022\n",
      "Cuda compilation tools, release 11.8, V11.8.89\n",
      "Build cuda_11.8.r11.8/compiler.31833905_0\n"
     ]
    }
   ],
   "source": [
    "! nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ae44e1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True)\n",
    "\n",
    "def load_tokenizer_llm(AutoModelForClass, model_name, model_path, cache_dir, to_device=True, **kwargs):\n",
    "    \"\"\"\n",
    "    if use load_in_4bit=True, do not set to_device=True\n",
    "    kwargs:\n",
    "        - device_map\n",
    "        - torch_dtype\n",
    "        - load_in_4bit\n",
    "    \"\"\"\n",
    "    if not os.path.isfile(model_path + '/tokenizer.json'):\n",
    "        print('no existing tokenizer found. Download from HF')\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name, \n",
    "                                                  cache_dir=cache_dir,\n",
    "                                                  **kwargs\n",
    "                                                 ) # to load tokenizer to cache\n",
    "    else:\n",
    "        print('existing tokenizer found. Load from local')\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_path, \n",
    "                                                  cache_dir=cache_dir, \n",
    "                                                  local_flies_only=True)\n",
    "    if not bool(glob.glob(model_path + '/model*.safetensors')):\n",
    "        print('no existing model found. Download from HF')\n",
    "        model = AutoModelForClass.from_pretrained(model_name,\n",
    "                                                     cache_dir=cache_dir,\n",
    "                                                     **kwargs\n",
    "                                                    )\n",
    "    else:\n",
    "        print('existing model found. Load from local')\n",
    "        model = AutoModelForClass.from_pretrained(model_path, \n",
    "                                                 cache_dir=cache_dir,\n",
    "                                                 local_files_only=True)\n",
    "    \n",
    "    if to_device:\n",
    "        device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "        #device.reset()\n",
    "        model.to(device) # use GPU. Do not need this if using load_in_4bit as it's already been set to the correct devices\n",
    "        \n",
    "    return tokenizer, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "02986d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_repo_dir = 'D:/projects/LLM'\n",
    "cache_dir = '/cygdrive/d/projects/LLM/.cache'\n",
    "os.environ['TRANSFORMERS_CACHE'] = cache_dir\n",
    "os.environ['HF_HOME'] = cache_dir + '/huggingface'\n",
    "os.environ['XDG_CACHE_HOME'] = cache_dir\n",
    "os.environ['HF_DATASETS_CACHE'] = cache_dir\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "457577c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "existing tokenizer found. Load from local\n",
      "existing model found. Load from local\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /cygdrive/d/projects/LLM/.cache/models--google-bert--bert-base-cased/snapshots/cd5ef92a9fb2f889e972770a36d4ed042daf221e were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model_name = 'google-bert/bert-base-cased'\n",
    "model_path = cache_dir + '/models--google-bert--bert-base-cased/snapshots/cd5ef92a9fb2f889e972770a36d4ed042daf221e'\n",
    "\n",
    "AutoModelForClass = BertForMaskedLM\n",
    "to_device = True\n",
    "tokenizer, model = load_tokenizer_llm(AutoModelForClass, model_name, model_path, cache_dir, to_device=to_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ea57aa69",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = cache_dir + '/parquet/yelp_polarity' # cache_dir + '/parquet/yelp_review_full-e22176106d6e7534'\n",
    "dataset_name = 'yelp_polarity' # yelp_review_full\n",
    "\n",
    "if not os.path.isdir(dataset_path):\n",
    "    dataset = load_dataset(dataset_name, cache_dir=cache_dir + '/parquet')\n",
    "else:\n",
    "    dataset = load_dataset(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ee964294",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_data_path = cache_dir + '/tokenized_dataset_yelp_polarity_bert'\n",
    "\n",
    "tokenized_datasets = load_from_disk(tokenized_data_path)\n",
    "tokenized_datasets = tokenized_datasets.remove_columns([\"text\"])\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
    "\n",
    "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "cc1be737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'datasets.arrow_dataset.Dataset'>\n",
      "(1000, 4)\n"
     ]
    }
   ],
   "source": [
    "print(type(small_train_dataset))\n",
    "print(small_train_dataset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5edc24b",
   "metadata": {},
   "source": [
    "## Masking token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d64c947e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace label with pre-masked input tokens\n",
    "small_train_dataset = small_train_dataset.remove_columns(\"labels\").add_column(\"labels\", small_train_dataset['input_ids'])\n",
    "small_eval_dataset = small_eval_dataset.remove_columns(\"labels\").add_column(\"labels\", small_eval_dataset['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "545efc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_token(dataset, max_length, mask_ratio):\n",
    "    inp_ids = []\n",
    "    for inp_list in small_train_dataset['input_ids']:\n",
    "        inp = np.array(inp_list)\n",
    "        actual_tokens = list(set(range(max_length)) - \n",
    "                             set(np.where((inp == 101) | (inp == 102) \n",
    "                                | (inp == 0))[0].tolist()))\n",
    "        #We need to select mask_ratio random tokens from the given list\n",
    "        num_of_token_to_mask = int(len(actual_tokens)*mask_ratio)\n",
    "        np.random.seed(123)\n",
    "        token_to_mask = np.random.choice(np.array(actual_tokens), \n",
    "                                         size=num_of_token_to_mask, \n",
    "                                         replace=False).tolist()\n",
    "        #Now we have the indices where we need to mask the tokens\n",
    "        inp[token_to_mask] = 103\n",
    "        inp_ids.append(inp)\n",
    "    return inp_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ae87d145",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 512\n",
    "mask_ratio = 0.15\n",
    "small_train_dataset = small_train_dataset.remove_columns(\"input_ids\").add_column(\"input_ids\", \n",
    "                                                                                 mask_token(small_train_dataset, \n",
    "                                                                                            max_length, \n",
    "                                                                                            mask_ratio))\n",
    "small_eval_dataset = small_eval_dataset.remove_columns(\"input_ids\").add_column(\"input_ids\", mask_token(small_eval_dataset, \n",
    "                                                                                            max_length, \n",
    "                                                                                            mask_ratio))\n",
    "small_train_dataset.set_format('torch')\n",
    "small_eval_dataset.set_format('torch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "42e4e1f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (transform_act_fn): GELUActivation()\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=28996, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "912204e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"f1\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "training_args = TrainingArguments(output_dir=\"test_trainer_mlm\", \n",
    "                                  evaluation_strategy=\"epoch\",\n",
    "                                  per_device_train_batch_size=8,\n",
    "                                  per_device_eval_batch_size=8,\n",
    "                                  seed=123)\n",
    "\n",
    "# Using the default loss function in BertForMaskedLM head. That should already be CrossEntropyLoss\n",
    "# TODO: check if the loss function is calculated using only masked tokens\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "399fecf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(small_train_dataset['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443f0632",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='40' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 40/375 22:43 < 3:20:19, 0.03 it/s, Epoch 0.31/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%time\n",
    "trainer.train() # run very slow even running on gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ae0253",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23598323",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLMpy38",
   "language": "python",
   "name": "llmpy38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
