{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30684,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# from huggingface_hub import hf_hub_download \nfrom transformers import pipeline\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSequenceClassification\nfrom transformers import TrainingArguments, Trainer\nimport torch\nfrom datasets import load_dataset, load_from_disk\nimport pandas as pd\nimport numpy as np","metadata":{"execution":{"iopub.status.busy":"2024-04-22T23:08:54.408037Z","iopub.execute_input":"2024-04-22T23:08:54.408940Z","iopub.status.idle":"2024-04-22T23:08:54.415411Z","shell.execute_reply.started":"2024-04-22T23:08:54.408905Z","shell.execute_reply":"2024-04-22T23:08:54.414326Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# Find binary classification data set - Done\n# Fine tune the last layer - When loading the pre-trained model, the last layer is randomly initialized\n# What is LoRA\n# Fine tuning vs no fine tuning\n# Number of trainable layers on small dataset\n# Why LoRA still take long time (autograd?)\n# Why freezing encoders still have millions of parameters - Embedding layer was also trained\n# Whether use class = 2 with softmax or class = 1 with sigmoid for binary classification? - Need to use class = 2 with softmax, cannot run class = 1 \n# How to deal with unbalanced data\n# What is the role of RAM\n# What is weight decay\n# What is gradient accumulation\n# Hyperparameter tuning (hyperopt - Sagar's stuff, bayesian grid search)\n\n### Questions:\n# What are intermediate and output layers in BERT layer? - Low priority","metadata":{"execution":{"iopub.status.busy":"2024-04-17T02:05:56.349991Z","iopub.execute_input":"2024-04-17T02:05:56.350344Z","iopub.status.idle":"2024-04-17T02:05:56.354852Z","shell.execute_reply.started":"2024-04-17T02:05:56.350315Z","shell.execute_reply":"2024-04-17T02:05:56.353662Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"dataset = load_dataset(\"yelp_polarity\")","metadata":{"execution":{"iopub.status.busy":"2024-04-22T23:08:54.417606Z","iopub.execute_input":"2024-04-22T23:08:54.417979Z","iopub.status.idle":"2024-04-22T23:09:06.792700Z","shell.execute_reply.started":"2024-04-22T23:08:54.417948Z","shell.execute_reply":"2024-04-22T23:09:06.791845Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"Downloading data: 100%|██████████| 256M/256M [00:06<00:00, 41.0MB/s] \nDownloading data: 100%|██████████| 17.7M/17.7M [00:00<00:00, 25.1MB/s]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/560000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b78ee90a0f64ef9a170ba3b0d0e7bbd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/38000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d4d43ad20924476eaeaf8f4108d8eae6"}},"metadata":{}}]},{"cell_type":"code","source":"dataset[\"train\"][100]","metadata":{"execution":{"iopub.status.busy":"2024-04-22T23:09:09.937727Z","iopub.execute_input":"2024-04-22T23:09:09.938412Z","iopub.status.idle":"2024-04-22T23:09:09.948362Z","shell.execute_reply.started":"2024-04-22T23:09:09.938350Z","shell.execute_reply":"2024-04-22T23:09:09.947314Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"{'text': \"In general I do like Shake N' Steak, but this location is a hit or miss location!  You never know what kind of quality or service you're going to find here.  A friend and myself went a few weeks back after a movie and it had to be one of the worst trips there EVER!  You can't entirely blame the waitress since she was the only one there for the entire place...poor scheduling on the manager's part. However, while she can't be accountable for the slooooow service, she was accountable for both orders being incorrect.  The burgers were over cooked and the fries were soggie and the milkshake was runny at best...\\\\n\\\\nBy far my worst visit to Steak n' Shake!\",\n 'label': 0}"},"metadata":{}}]},{"cell_type":"code","source":"def tokenize_function(examples):\n    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, return_tensors='tf')\n\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    predictions = np.argmax(logits, axis=-1)\n    return metric.compute(predictions=predictions, references=labels)","metadata":{"execution":{"iopub.status.busy":"2024-04-22T23:09:10.816693Z","iopub.execute_input":"2024-04-22T23:09:10.817421Z","iopub.status.idle":"2024-04-22T23:09:10.823086Z","shell.execute_reply.started":"2024-04-22T23:09:10.817371Z","shell.execute_reply":"2024-04-22T23:09:10.822076Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-cased\")","metadata":{"execution":{"iopub.status.busy":"2024-04-22T23:09:12.152622Z","iopub.execute_input":"2024-04-22T23:09:12.153022Z","iopub.status.idle":"2024-04-22T23:09:20.157394Z","shell.execute_reply.started":"2024-04-22T23:09:12.152991Z","shell.execute_reply":"2024-04-22T23:09:20.156650Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bd376cd89e9d4793ac6251dc564f2e11"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"737e8bd756294c868347320f9e7f5522"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0c838f42060f4b35a1c3c7dada7ca196"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"905634d2c4f941b084dfbc4c5f68b4c8"}},"metadata":{}}]},{"cell_type":"code","source":"tokenized_datasets = dataset.map(tokenize_function, batched=True)\ntokenized_datasets.save_to_disk('./dataset')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenized_datasets = load_from_disk('./dataset')","metadata":{"execution":{"iopub.status.busy":"2024-04-22T23:09:20.158713Z","iopub.execute_input":"2024-04-22T23:09:20.159003Z","iopub.status.idle":"2024-04-22T23:09:20.192553Z","shell.execute_reply.started":"2024-04-22T23:09:20.158978Z","shell.execute_reply":"2024-04-22T23:09:20.191701Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(5000))\nsmall_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000))","metadata":{"execution":{"iopub.status.busy":"2024-04-22T23:17:13.971197Z","iopub.execute_input":"2024-04-22T23:17:13.971590Z","iopub.status.idle":"2024-04-22T23:17:14.005745Z","shell.execute_reply.started":"2024-04-22T23:17:13.971558Z","shell.execute_reply":"2024-04-22T23:17:14.004711Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"# load the model\nmodel = AutoModelForSequenceClassification.from_pretrained(\"google-bert/bert-base-cased\", \n                                                           num_labels=2)","metadata":{"execution":{"iopub.status.busy":"2024-04-22T23:09:59.465564Z","iopub.execute_input":"2024-04-22T23:09:59.465902Z","iopub.status.idle":"2024-04-22T23:10:03.455597Z","shell.execute_reply.started":"2024-04-22T23:09:59.465875Z","shell.execute_reply":"2024-04-22T23:10:03.454837Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/436M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0f37e4088cb145168b877d5b6e4ca4f8"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"model.to('cuda')","metadata":{"execution":{"iopub.status.busy":"2024-04-22T23:10:04.658595Z","iopub.execute_input":"2024-04-22T23:10:04.658995Z","iopub.status.idle":"2024-04-22T23:10:04.796477Z","shell.execute_reply.started":"2024-04-22T23:10:04.658962Z","shell.execute_reply":"2024-04-22T23:10:04.795560Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"BertForSequenceClassification(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (classifier): Linear(in_features=768, out_features=2, bias=True)\n)"},"metadata":{}}]},{"cell_type":"code","source":"def print_trainable_parameters(model):\n    \"\"\"\n    Prints the number of trainable parameters in the model.\n    \"\"\"\n    trainable_params = 0\n    all_param = 0\n    for _, param in model.named_parameters():\n        all_param += param.numel()\n        if param.requires_grad:\n            trainable_params += param.numel()\n    print(\n        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n    )","metadata":{"execution":{"iopub.status.busy":"2024-04-22T23:10:07.233205Z","iopub.execute_input":"2024-04-22T23:10:07.233939Z","iopub.status.idle":"2024-04-22T23:10:07.239890Z","shell.execute_reply.started":"2024-04-22T23:10:07.233906Z","shell.execute_reply":"2024-04-22T23:10:07.238801Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"params = model._modules['bert'].parameters()\nfor param in params:\n    param.requires_grad = False\n\nprint('Only train classifier')\nprint_trainable_parameters(model)","metadata":{"execution":{"iopub.status.busy":"2024-04-22T23:10:07.872895Z","iopub.execute_input":"2024-04-22T23:10:07.873235Z","iopub.status.idle":"2024-04-22T23:10:07.880956Z","shell.execute_reply.started":"2024-04-22T23:10:07.873209Z","shell.execute_reply":"2024-04-22T23:10:07.880023Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Only train classifier\ntrainable params: 1538 || all params: 108311810 || trainable%: 0.0014199744238416845\n","output_type":"stream"}]},{"cell_type":"code","source":"args = TrainingArguments(\n    f\"finetune-bert-base-cased\",\n    evaluation_strategy = \"epoch\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    num_train_epochs=3,\n    weight_decay=0.01,\n)","metadata":{"execution":{"iopub.status.busy":"2024-04-22T23:10:09.361214Z","iopub.execute_input":"2024-04-22T23:10:09.361591Z","iopub.status.idle":"2024-04-22T23:10:09.369596Z","shell.execute_reply.started":"2024-04-22T23:10:09.361563Z","shell.execute_reply":"2024-04-22T23:10:09.368727Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"# Without any training","metadata":{}},{"cell_type":"code","source":"trainer = Trainer(\n    model=model,\n    args=args,\n    train_dataset=small_train_dataset,\n    eval_dataset=small_eval_dataset,\n    tokenizer=tokenizer,\n)","metadata":{"execution":{"iopub.status.busy":"2024-04-22T23:10:11.386330Z","iopub.execute_input":"2024-04-22T23:10:11.386699Z","iopub.status.idle":"2024-04-22T23:10:11.988547Z","shell.execute_reply.started":"2024-04-22T23:10:11.386671Z","shell.execute_reply":"2024-04-22T23:10:11.987749Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \ndataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"raw_output = trainer.predict(small_eval_dataset).predictions","metadata":{"execution":{"iopub.status.busy":"2024-04-22T23:10:14.073640Z","iopub.execute_input":"2024-04-22T23:10:14.073993Z","iopub.status.idle":"2024-04-22T23:11:56.607568Z","shell.execute_reply.started":"2024-04-22T23:10:14.073967Z","shell.execute_reply":"2024-04-22T23:11:56.606600Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}}]},{"cell_type":"code","source":"raw_output","metadata":{"execution":{"iopub.status.busy":"2024-04-22T23:11:56.609160Z","iopub.execute_input":"2024-04-22T23:11:56.609480Z","iopub.status.idle":"2024-04-22T23:11:56.616210Z","shell.execute_reply.started":"2024-04-22T23:11:56.609456Z","shell.execute_reply":"2024-04-22T23:11:56.615104Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"array([[ 0.2001286 , -0.23433197],\n       [ 0.22055243, -0.23368587],\n       [ 0.20732889, -0.25856608],\n       ...,\n       [ 0.1554417 , -0.21593827],\n       [ 0.1378938 , -0.22907202],\n       [ 0.18179026, -0.18574706]], dtype=float32)"},"metadata":{}}]},{"cell_type":"code","source":"sm = torch.nn.Softmax(dim=1)","metadata":{"execution":{"iopub.status.busy":"2024-04-22T23:13:18.357061Z","iopub.execute_input":"2024-04-22T23:13:18.357901Z","iopub.status.idle":"2024-04-22T23:13:18.361871Z","shell.execute_reply.started":"2024-04-22T23:13:18.357870Z","shell.execute_reply":"2024-04-22T23:13:18.360951Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"sm(torch.from_numpy(raw_output))","metadata":{"execution":{"iopub.status.busy":"2024-04-22T23:13:19.903364Z","iopub.execute_input":"2024-04-22T23:13:19.903738Z","iopub.status.idle":"2024-04-22T23:13:19.929673Z","shell.execute_reply.started":"2024-04-22T23:13:19.903712Z","shell.execute_reply":"2024-04-22T23:13:19.928797Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"tensor([[0.6069, 0.3931],\n        [0.6116, 0.3884],\n        [0.6144, 0.3856],\n        ...,\n        [0.5918, 0.4082],\n        [0.5907, 0.4093],\n        [0.5909, 0.4091]])"},"metadata":{}}]},{"cell_type":"markdown","source":"# Fine tune the last layer","metadata":{}},{"cell_type":"code","source":"# model.encoder.layer[-1].apply(model._init_weights)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# scheduler\nargs = TrainingArguments(\n    f\"finetune-bert-base-cased\",\n    evaluation_strategy = \"epoch\",\n    learning_rate=0.001,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    num_train_epochs=10,\n    weight_decay=0.01,\n)","metadata":{"execution":{"iopub.status.busy":"2024-04-23T00:05:45.207341Z","iopub.execute_input":"2024-04-23T00:05:45.208050Z","iopub.status.idle":"2024-04-23T00:05:45.217062Z","shell.execute_reply.started":"2024-04-23T00:05:45.208018Z","shell.execute_reply":"2024-04-23T00:05:45.216050Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"trainer = Trainer(\n    model=model,\n    args=args,\n    train_dataset=small_train_dataset,\n    eval_dataset=small_eval_dataset,\n    tokenizer=tokenizer,\n)","metadata":{"execution":{"iopub.status.busy":"2024-04-23T00:05:45.471009Z","iopub.execute_input":"2024-04-23T00:05:45.471314Z","iopub.status.idle":"2024-04-23T00:05:45.487903Z","shell.execute_reply.started":"2024-04-23T00:05:45.471290Z","shell.execute_reply":"2024-04-23T00:05:45.487047Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \ndataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"# WIP: try larger batch size or smaller learning rate\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-04-23T00:05:48.054794Z","iopub.execute_input":"2024-04-23T00:05:48.055913Z","iopub.status.idle":"2024-04-23T00:28:57.574707Z","shell.execute_reply.started":"2024-04-23T00:05:48.055859Z","shell.execute_reply":"2024-04-23T00:28:57.573859Z"},"trusted":true},"execution_count":38,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='3130' max='3130' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [3130/3130 23:08, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>0.388652</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.513600</td>\n      <td>0.457801</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.513600</td>\n      <td>0.399943</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.499800</td>\n      <td>0.387046</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.466800</td>\n      <td>0.401865</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.466800</td>\n      <td>0.395675</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.456800</td>\n      <td>0.388866</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.455000</td>\n      <td>0.383218</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.455000</td>\n      <td>0.383786</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.461900</td>\n      <td>0.383451</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=3130, training_loss=0.4755889100388597, metrics={'train_runtime': 1389.0914, 'train_samples_per_second': 35.995, 'train_steps_per_second': 2.253, 'total_flos': 1.3155552768e+16, 'train_loss': 0.4755889100388597, 'epoch': 10.0})"},"metadata":{}}]},{"cell_type":"code","source":"raw_output = trainer.predict(small_eval_dataset).predictions","metadata":{"execution":{"iopub.status.busy":"2024-04-23T01:00:23.532991Z","iopub.execute_input":"2024-04-23T01:00:23.533619Z","iopub.status.idle":"2024-04-23T01:00:44.758048Z","shell.execute_reply.started":"2024-04-23T01:00:23.533587Z","shell.execute_reply":"2024-04-23T01:00:44.751499Z"},"trusted":true},"execution_count":39,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}}]},{"cell_type":"code","source":"sm(torch.from_numpy(raw_output))","metadata":{"execution":{"iopub.status.busy":"2024-04-23T01:00:46.883962Z","iopub.execute_input":"2024-04-23T01:00:46.884702Z","iopub.status.idle":"2024-04-23T01:00:46.895260Z","shell.execute_reply.started":"2024-04-23T01:00:46.884669Z","shell.execute_reply":"2024-04-23T01:00:46.893466Z"},"trusted":true},"execution_count":40,"outputs":[{"execution_count":40,"output_type":"execute_result","data":{"text/plain":"tensor([[0.4987, 0.5013],\n        [0.4895, 0.5105],\n        [0.8584, 0.1416],\n        ...,\n        [0.8269, 0.1731],\n        [0.9246, 0.0754],\n        [0.8795, 0.1205]])"},"metadata":{}}]},{"cell_type":"code","source":"raw_output","metadata":{"execution":{"iopub.status.busy":"2024-04-23T01:00:53.215147Z","iopub.execute_input":"2024-04-23T01:00:53.215507Z","iopub.status.idle":"2024-04-23T01:00:53.224421Z","shell.execute_reply.started":"2024-04-23T01:00:53.215480Z","shell.execute_reply":"2024-04-23T01:00:53.223266Z"},"trusted":true},"execution_count":41,"outputs":[{"execution_count":41,"output_type":"execute_result","data":{"text/plain":"array([[-0.01528161, -0.00999662],\n       [-0.02600306,  0.01591472],\n       [ 0.8821367 , -0.91965866],\n       ...,\n       [ 0.7222047 , -0.8412853 ],\n       [ 1.2529233 , -1.2529893 ],\n       [ 0.95214224, -1.0359869 ]], dtype=float32)"},"metadata":{}}]},{"cell_type":"code","source":"trainer.save_model(\"last_layer\")","metadata":{"execution":{"iopub.status.busy":"2024-04-23T01:02:02.040278Z","iopub.execute_input":"2024-04-23T01:02:02.040636Z","iopub.status.idle":"2024-04-23T01:02:02.593924Z","shell.execute_reply.started":"2024-04-23T01:02:02.040610Z","shell.execute_reply":"2024-04-23T01:02:02.592890Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}