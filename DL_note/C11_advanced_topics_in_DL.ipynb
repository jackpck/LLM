{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Topics in Deep Learning\n",
    "\n",
    "## 1. Expressivity of DNN\n",
    "- ### 1.1 Preliminaries\n",
    "    - #### Backpropagation\n",
    "    - #### Gradient vanishing/explosion\n",
    "- ### 1.2 Signal propagation in DNN\n",
    "- ### 1.3 Understand Expressivity through Signal propagation in DNN\n",
    "    - #### Preliminaries: Riemannian geometry\n",
    "    - #### Exponential expressivity in chaotic phase\n",
    "    - #### Forward propagation and expressivity\n",
    "    - #### Backward propagation and gradient vanishing/explosion\n",
    "- ### 1.4 Effects of different initializations and NN architectures\n",
    "    - #### Preliminaries: Jacobian spectrum\n",
    "    - #### Orthogonal vs Gaussian weight initialization\n",
    "    - #### Bounded vs unbounded activation functions [WIP]\n",
    "    - #### Dropout [WIP]\n",
    "    - #### Batch normalization [WIP]\n",
    "    - #### Batch size [WIP]\n",
    "\n",
    "## 2. Optimization\n",
    "- ### 2.1 Proximal optimization\n",
    "    - #### Approximation 1 - natural gradient\n",
    "    - #### Approximation 2 - damped Newton method\n",
    "- ### 2.2 Gauss-Netwon matrix - approximation to Hessian matrix\n",
    "    - #### Benefit of using Gauss-Newton matrix\n",
    "- ### 2.3 Output space gradient descent\n",
    "\n",
    "## 3. Learnability of DNN\n",
    "- ### 3.1 Challenge #1: Gradient vanishing/explosion\n",
    "- ### 3.2 Challenge #2: Proliferation of saddle points\n",
    "- ### 3.3 Challenge #3: Failure of gradient-based algorithms\n",
    "- ### 3.4 Challenge #4: Catastrophic interference/forgetting\n",
    "- ### 3.5 NN tuning best practice [WIP]\n",
    "    - #### Batch size\n",
    "    - #### Learning rate\n",
    "    \n",
    "## 4. Self-attention as rank lowering operation \n",
    "- ### 4.1 Rank collapsing of pure self-attention \n",
    "    - #### MLP\n",
    "    - #### Skip-connection\n",
    "    - #### Layer normalization\n",
    "- ### 4.2 Efficient approximation to self-attention mechanism\n",
    "    - #### Linformer\n",
    "    \n",
    "## Appendix\n",
    "- ### A1 Iterative equations for $q^l$ and $c^l$ [WIP]\n",
    "- ### A2 Gradient descent based optimizers\n",
    "    - #### Gradient descent\n",
    "    - #### Stochastic gradient descent\n",
    "    - #### Newton's method\n",
    "    - #### ADAM\n",
    "- ### A3 Natural gradient descent\n",
    "    - #### Natural gradient with Fisher Information Matrix (FIM)\n",
    "    - #### Derivation of FIM using differential geometry\n",
    "- ### A4 Morse's lemma\n",
    "    - #### Morse index\n",
    "    - #### Morse's lemma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Expressivity of DNN \n",
    "\n",
    "In this section we will summarize works on investigating why NN is so powerful (high expressivity) and why NN is difficult to train (due to gradient explosion/vanishing)\n",
    "\n",
    "### 1.1 Preliminaries\n",
    "\n",
    "#### Backpropagation\n",
    "\n",
    "Given an $L$-layer NN:\n",
    "\n",
    "\\begin{align}\n",
    "&x^{(l)}_i = F\\Big(\\sum_j W_{ij}^{(l)} x^{(l-1)}_j + b^{(l)}_j \\Big) \\\\\n",
    "&\\hat{y} = F\\Big(\\sum_j W_j^{(L)} x^{(L-1)}_j + b^{(L)}_j \\Big) \\\\\n",
    "&loss = \\mathcal{L}(\\vec{y}, \\hat{y}) = \\sum_k \\mathcal{l}(y_k, \\hat{y}_k) \\label{NN_flow}\\tag{Eq 1.1}\n",
    "\\end{align}\n",
    "\n",
    "Where the weight $W^{(l)}_{ij}$ connects the node $x^{(l)}_j$ in the $l$-th layer to node $x^{(l+1)}_i$ in the $l+1$-th layer. The loss function is written in the summation form to highlight the possibility of using stochastic methods (e.g. SGD). To update the weights in the first layer of the NN, one need to compute the following gradient:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial w^{(1)}_{ij}} &= \\frac{\\partial \\mathcal{L}}{\\partial x^{(1)}_{i}}\\frac{\\partial x^{(1)}_{i}}{\\partial w^{(1)}_{ij}}\\\\\n",
    "\\end{align}\n",
    "\n",
    "Note that $\\mathcal{L}$ is not the direct function of $x^{(0)}$ and so in order to compute the first derivative, one need to use chain rule\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial w^{(1)}_{ij}} &= \\frac{\\partial \\mathcal{L}}{\\partial x^{(1)}_{i}}\\frac{\\partial x^{(1)}_{i}}{\\partial w^{(1)}_{ij}}\\\\\n",
    "&= \\sum_{k_1,...k_L}\\frac{\\partial \\mathcal{L}}{\\partial x^{(L)}_{k_L}}\n",
    "\\frac{\\partial x^{(L)}_{k_L}}{\\partial x^{(L-1)}_{k_{L-1}}}\n",
    "\\frac{\\partial x^{(L-1)}_{k_{L-1}}}{\\partial x^{(L-2)}_{k_{L-2}}}...\n",
    "\\frac{\\partial x^{(2)}_{k_2}}{\\partial x^{(1)}_{k_1}}\n",
    "\\frac{\\partial x^{(1)}_{k_1}}{\\partial w^{(1)}_{ij}}\\\\\n",
    "&= \\sum_{k_1,...k_L}\\frac{\\partial \\mathcal{L}}{\\partial x^{(L)}_{k_L}}\n",
    "J^{(L)}_{k_L, k_{L-1}}\\ \\cdot\n",
    "J^{(L-1)}_{k_{L-1}, k_{L-2}}\\ \\cdot...\n",
    "J^{(2)}_{k_2, k_1}\\ \n",
    "\\frac{\\partial x^{(1)}_{k_1}}{\\partial w^{(1)}_{ij}} \\\\\n",
    "&= \\nabla_{x^{(L)}} \\mathcal{L} \\cdot \\prod_{l=1}^L J^{(l)} \\cdot \\frac{\\partial \\pmb x^{(1)}}{\\partial w^{(1)}_{ij}} \\label{chain_rule_1}\\tag{Eq 1.2}\n",
    "\\end{align}\n",
    "\n",
    "where $J^{(l)}_{ij}$ are the elements of the Jacobian matrix at layer $l$. $k_j$ is the number of neurons in layer $j$. The last line in \\ref{chain_rule_1} shows that to compute the gradient $l$-layer deep will involves product of $l$ Jacobian. We will see this is the source of gradient vanishing/explosion [ref](https://arxiv.org/pdf/1211.5063.pdf).\n",
    "\n",
    "#### Gradient vanishing/explosion\n",
    "To illustrates gradient vanishing/explosion from \\ref{chain_rule_1} mathematically, assume the Jacobians are the same. Repeated application of Jacobian results in a stationary solution and the following eigenvalue problem\n",
    "\n",
    "\\begin{equation}\n",
    "J^n \\vec{x} = \\vec{x},\\ \\ \\ \\ n\\to \\infty\n",
    "\\end{equation}\n",
    "\n",
    "Using SVD, $J^n = U\\Sigma^n V^T$ is dominated by the top eigenvalue $\\lambda_1$. If $\\lambda_1>1$, $J^n$ will diverge (gradient explosion). If $\\lambda_1 < 1$, $J^n$ will converge to zero (gradient vanishing).\n",
    "\n",
    "\n",
    "In RNN [ref](https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks), we face the similar gradient explosion/vanishing issue. Consider the following updating equations for RNN:\n",
    "\n",
    "\\begin{align}\n",
    "z^{(l)} &= Wx^{(l)} + b \\\\\n",
    "h^{(l)} &= \\sigma(z^{(l)}) \\\\\n",
    "z^{(l+1)} &= Uh^{(l)} + Vx^{(l+1)} \\label{rnn_eq}\\tag{Eq 1.3}\n",
    "\\end{align}\n",
    "\n",
    "Solving the weights involves computing the following Jacobian matrix\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial h^{(T)}}{\\partial h^{(0)}} = \\frac{\\partial h^{(T)}}{\\partial h^{(T-1)}}\\frac{\\partial h^{(T-1)}}{\\partial h^{(T-2)}}...\\frac{\\partial h^{(1)}}{\\partial h^{(0)}}\n",
    "\\end{equation}\n",
    "\n",
    "which involves product of $T$ terms, hence resulting in the same problem as before. The gradient explosion and vanishing problem can be alleviated by using **residual connection**. Consider the following updating equations for a residual network (**ResNet**)\n",
    "\\begin{align}\n",
    "z^{(l)} &= Wx^{(l)} + b \\\\\n",
    "h^{(l)} &= \\sigma(z^{(l)}) \\\\\n",
    "z^{(l+1)} &= Uh^{(l)} + Vx^{(l)} \\label{resnet_eq}\\tag{Eq 1.4}\n",
    "\\end{align}\n",
    "\n",
    "The subtle difference \\ref{rnn_eq} and \\ref{resnet_eq} is the $x$ term in the last line. For RNN, $x$ is the next variable in the sequence $x^{(l+1)}$ where in ResNet, $x$ is the same variable from the layer $x^{(l)}$. The reason by this helps mitigate gradient vanishing/explosion can be understood by writting \\ref{resnet_eq} in a more general form\n",
    "\n",
    "\\begin{equation}\n",
    "x^{(l+1)} = x^{(l)} + F(x^{(l)})\n",
    "\\end{equation}\n",
    "\n",
    "The Jacobian becomes\n",
    "\n",
    "\\begin{equation}\n",
    "J^{(l+1)}_{ij} = \\frac{\\partial x^{(l+1)}_i}{\\partial x^{(l)}_j} = I + \\frac{\\partial F(x^{(l)})}{\\partial x^{(l)}}\n",
    "\\end{equation}\n",
    "\n",
    "Hence, if $\\frac{\\partial F}{\\partial x^{(l)}}=0$, the product of Jacobian will be close to identity and the gradient will explode/vanish at a lower rate. For a sigmoid activation function $F=\\sigma$, $\\partial F/ \\partial x$ is the largest when $x$ is close to zero. Therefore, batch normalization helps ensure $\\frac{\\partial F}{\\partial x^{(l)}}$ close to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Signal propagation in DNN [ref](https://arxiv.org/pdf/1606.05340.pdf)\n",
    "\n",
    "This section concerns the propagation of input information along layers in a DNN. The framework presented below will help us better understand \n",
    "- why DNN are powerful (i.e. high expressivity) \n",
    "- the source of gradient vanishing/explosion\n",
    "- learning dynamics of DNN\n",
    "- importance of different NN initializations and architectures [ref](https://arxiv.org/pdf/1802.09979.pdf)\n",
    "\n",
    "First let's define the feedforward dynamics of a random DNN:\n",
    "\n",
    "\\begin{align}\n",
    "h_i^{(l)} &= \\sum_j w_{ij}^{(l)} x^{(l-1)}_j + b_i \\\\\n",
    "x^{(l)}_i &= \\phi(h_i^{(l)})\n",
    "\\end{align}\n",
    "\n",
    "where for random DNN, $w_{ij}$ and $b_i$ are drawn independently from a Gaussian distribution of variance $\\sigma_w^2$ and $\\sigma_b^2$ respectively. As we will see, these two parameters are important in determining the feedforward dynamics of a random DNN. To qualitatively understand how signal propagates through the DNN, define the following two variables \n",
    "\n",
    "\\begin{align}\n",
    "q^l &\\equiv \\langle (h^{(l)}_i)^2\\rangle \\\\\n",
    "q^l_{\\alpha \\beta} &\\equiv \\langle h^{(l)}_i(x^0_\\alpha)\\ h^{(l)}_i(x^0_\\beta)\\rangle\n",
    "\\end{align}\n",
    "\n",
    "where $\\alpha$ and $\\beta$ are indices to two samples. The expectation operator $\\langle * \\rangle$ averages over $w_{ij}$ and $b_i$. For DNN with large enough layer width, $\\langle * \\rangle$ can be replaced by the average over neuron in the same layer, i.e. assuming a self-averaging property. The first quantity tracks how the average length of the embedded vector changes along the network. For the second quantity, the normalized form of $q^l_{\\alpha \\beta}$, $c^l_{\\alpha \\beta} = q^l_{\\alpha \\beta}\\ /\\ \\sqrt{q^l_{\\alpha \\alpha} q^l_{\\beta \\beta}}$ then measures the average correlation at layer $l$ of two inputs $x^0_\\alpha$ and $x^0_\\beta$. \n",
    "\n",
    "It was shown that the changes in $q^l$ and $q^l_{\\alpha \\beta}$ along the layers are governed by the following recursive equations\n",
    "\n",
    "\\begin{align}\n",
    "q^l &= \\mathcal{V}(q^{l-1} \\mid \\sigma_w, \\sigma_b) \\label{q_recursive}\\tag{Eq 1.2.1}\\\\\n",
    "c^l_{\\alpha \\beta} &= \\mathcal{C}(c^{l-1}_{\\alpha \\beta}, q^l_{\\alpha \\alpha}=q^*, q^l_{\\beta \\beta}=q^* \n",
    "\\mid \\sigma_w, \\sigma_b) \\label{c_recursive}\\tag{Eq 1.2.2}\n",
    "\\end{align}\n",
    "\n",
    "where \\ref{q_recursive} converges to a fixed point $q^*$ as $l\\to\\infty$. The recursive equation for $c^l$, \\ref{c_recursive} sets the norm of $q^l_{\\alpha \\alpha}$ and $q^l_{\\beta \\beta}$ equals to $q^*$ so they do not change during the iteration. The following summarize different fixed points of $q^l$ as a function of $\\sigma_w$ and $\\sigma_b$\n",
    "\n",
    "|$\\sigma_w$|$\\sigma_b$|$q^*$|stability|\n",
    "|-----|-----|--------|-----|\n",
    "|<1 |0  |0| stable |\n",
    "|>1  |0  |0| unstable |\n",
    "|>1  |0  |>0| stable |\n",
    "|>0  |$\\neq 0$  |>0| stable |\n",
    "\n",
    "For $c^l$, it is shown that $c^*=1$ is always a fixed point. To quantify the stability of $c^l$, we need to describe how this correlation changes over layers, define\n",
    "\n",
    "\\begin{align}\n",
    "\\chi = \\frac{\\partial c^l_{\\alpha \\beta}}{\\partial c^{l-1}_{\\alpha \\beta}}\\bigg|_{c=c^*=1}\n",
    "\\end{align}\n",
    "\n",
    "If $\\chi > 1$, $c^*$ is unstable (if $c^{l-1}$ decreases, $c^l$ will further move away from 1) and there exists another fixed point $c^*<1$. If $\\chi < 1$, $c^*=1$ is stable (if $c^{l-1}$ decreases, $c^l$ will move back to 1). In the $\\sigma_w$-$\\sigma_b$ space, $\\chi=1$ represents the critical line that separate the unstable, 'chaotic' (it's more like a simple bifurcation instead of a true chaos) phase and the stable, ordered phase. In the 'chaotic' phase, the correlation of two inputs will decrease over layers. In the ordered phase, the two inputs will end up to be the same over enough layers.\n",
    "\n",
    "It is shown in [ref](https://arxiv.org/pdf/1711.00165.pdf) that a deep, infinite-wide NN can be expressed as a Gaussian Process (GP), whose kernel at layer $l$ and $l-1$ are related by a recursive formula in the same form as \\ref{c_recursive}."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Understand Expressivity through Signal propagation in DNN\n",
    "\n",
    "#### Preliminaries: Riemannian geometry\n",
    "\n",
    "Imagine the feature input data $\\{\\pmb x_i\\}_{i=1}^N$ forms a $D_M$ dimensional manifold in a $D$ dimensional space where $D$ is simply the dimension of $\\pmb x_i$. Let's parametrize a subset of data points on this manifold by a scalar parameter $\\theta$ so that if traces a 1D curve $\\pmb x(\\theta)$ on the manifold. As $\\pmb x_i$ is passed along the DNN, the manifold will be morphed into a different shape. In layer $l$, the 1D curve will be mapped to a different curve $\\pmb h^{(l)}(\\theta) = \\pmb h^{(l)}(\\pmb x(\\theta))$. One can define the characteristics of the curve by two metrics\n",
    "\n",
    "\\begin{align}\n",
    "g^E(\\theta) &= \\partial_\\theta \\pmb h(\\theta) \\cdot \\partial_\\theta \\pmb h(\\theta) = \\pmb v(\\theta)\\cdot \\pmb v(\\theta) \\label{euclidean_metric}\\tag{Eq 1.3.3}\\\\\n",
    "g^G(\\theta) &= \\partial_\\theta \\pmb{\\hat{v}}(\\theta)\\cdot \\partial_\\theta\\pmb{\\hat{v}}(\\theta) \n",
    "\\label{gauss_metric}\\tag{Eq 1.3.4}\\\\\n",
    "\\end{align}\n",
    "\n",
    "\\ref{euclidean_metric} (Euclidean metric) can be understood as the speed along the curve at point $\\theta$ and \\ref{gauss_metric} (Gauss metric) as the acceleration along the curve at point $\\theta$. Note that $\\pmb v(\\theta)$ points tangentially along the curve and $\\partial_\\theta\\pmb v(\\theta)$ points perpendicularly to the curve. The two metrics are related by the curvature $\\kappa(\\theta)$ \n",
    "\n",
    "\\begin{align}\n",
    "g^G(\\theta) = \\kappa^2(\\theta) g^E(\\theta) \\label{euclidean_gauss_curvature}\\tag{Eq 1.3.5}\n",
    "\\end{align}\n",
    "\n",
    "As a simple example to understand \\ref{euclidean_gauss_curvature}, consider $\\pmb h(\\theta)$ is a circle. Under linear expansion $\\pmb h(\\theta) \\to \\chi \\pmb h(\\theta)$, $g^E(\\theta)$ increases by a scaler $\\chi$ whereas the curvature $\\kappa$ decreases by $1/\\sqrt{\\chi}$. Therefore $g^G(\\theta)$ remains unchanged. This is simply understood as when we expand the radius of the circle, the curvature decreases.\n",
    "\n",
    "#### Exponential expressivity in the chaotic phase\n",
    "\n",
    "In the ordered phase $\\chi < 1$, since the fixed point $c^*=1$, the curve will eventually collapse to a single point (all inputs have maximum correlation). In the chaotic phase $\\chi > 1$, the propagation of the curve $\\pmb h(\\theta)$ behaves very differently. Even as $g^E(\\theta)$ expands exponentially in depth, $\\kappa$ does not necessarily decrease. Its growth depends on curvature of single neuron nonlinearity. Therefore the Gaussian metric also grows exponentially. Intuitively, the stretching of the curve happens together with the increasing convolution of the curves in other dimensions, thus filling up the hidden representation space.\n",
    "\n",
    "The expressivity of DNN can now be understood in two ways: The last layer of the DNN is essentially a linear regression and therefore have a linear decision boundary $\\pmb w \\cdot \\pmb h^{(L)} - b = 0$ for the input $\\pmb h^L(\\theta)$, which as discussed above, has a complicated geometry. Even the decision boundary has simple geometry, the input has a complicated geometry. This is similar to the concept of kernel SVM.\n",
    "\n",
    "The second way to understand expressivity is to look at the first layer of the DNN where the input $\\pmb x(\\theta)$ is relatively smooth but the decision boundary $\\pmb w \\cdot \\pmb h^{(L)}(\\pmb x) - b = 0$ is no longer linear (nonlinearity coming from $\\pmb h^{(L)}(\\pmb x)$). In fact, it can be shown the geometry of the decision boundary becomes increasing complicated. Define the decision boundary $G(\\pmb x)$ to be a collection of point where\n",
    "\n",
    "\\begin{align}\n",
    "G(\\pmb x) = \\{\\pmb x \\mid \\pmb w \\cdot \\pmb h^{(L)}(\\pmb x) - b = 0\\}\n",
    "\\end{align}\n",
    "\n",
    "Intuitively, a point $x^*$ on the decision boundary manifold can be approximated by a paraboloid with a quadratic form $H$ (the normalized Hessian matrix) whose $N-1$ eigenvalues are the principal curvatures. Numerically, it is shown that a subset of the principal curvatures grow exponentially with depth. This implies the decision boundary manifold becomes more complex and curved.\n",
    "\n",
    "#### Forward propagation and expressivity [ref](https://arxiv.org/pdf/1611.01232.pdf)\n",
    "\n",
    "As discussed in Section 1.3, both $q^l$ and $c^l$ will converge to their respective fixed points at deep layers. The rate at which the quantities converge describe the depth scales of how far information propagates in a DNN. Intuitively, the slower the quantities converge, the further information propagates along the depth. It is shown that both quantities converge exponentially\n",
    "\n",
    "\\begin{align}\n",
    "| q^l_{\\alpha \\alpha} - q^* | &\\sim e^{-l/\\xi_q} \\\\\n",
    "| c^l_{\\alpha \\beta} - c^* | &\\sim e^{-l/\\xi_c}\n",
    "\\end{align}\n",
    "\n",
    "Similar to the gradient length scale $\\xi_\\nabla$, $\\xi_c$ has the following form\n",
    "\n",
    "\\begin{align}\n",
    "\\xi_c^{-1} = -\\log \\chi\n",
    "\\end{align}\n",
    "\n",
    "and therefore has a order-to-chaos transition at $\\chi=1$. $\\xi_c$ diverges and information persists indefinitely with depth.\n",
    "\n",
    "#### Backward propagation and gradient vanishing/explosion [ref](https://arxiv.org/pdf/1611.01232.pdf)\n",
    "\n",
    "In section 1.3.1, we discussed forward propagation of signal. Here, we show the duality between the forward propagation of signal and backprop of gradient. This allows us to intuitively understand the source of gradient vanishing/explosion.\n",
    "\n",
    "Reiterating the DNN forward equations\n",
    "\n",
    "\\begin{align}\n",
    "h_i^{(l)} &= \\sum_j w_{ij}^{(l)} x^{(l-1)}_j + b_i \\\\\n",
    "x^{(l)}_i &= \\phi(h_i^{(l)})\n",
    "\\end{align}\n",
    "\n",
    "Let's rewrite the backprop equation in a recursive way\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial w^{(l)}_{ij}} &= \\frac{\\partial \\mathcal{L}}{\\partial h^{(l)}_{i}}\\frac{\\partial h^{(l)}_{i}}{\\partial w^{(l)}_{ij}} = \\delta^{(l)}_i \\phi(h^{(l-1)}_j)\\\\\n",
    "\\delta^{(l)}_i &= \\phi'(h^{(l)}_i)\\sum_j \\delta^{(l+1)}_j w^{(l+1)}_{ji} \\label{backprop_recursive}\\tag{Eq 1.3.6}\n",
    "\\end{align}\n",
    "\n",
    "The second line of \\ref{backprop_recursive} shows the iterative map of $\\delta^{(l)}_i$ with depth. Define \n",
    "\n",
    "\\begin{align}\n",
    "\\tilde q_{\\alpha \\alpha}^l \\equiv \\mathbb{E}[(\\delta^{(l)}_i)^2]\n",
    "\\end{align}\n",
    "\n",
    "Roughly speaking, applying expectation to the first line of \\ref{backprop_recursive}\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbb{E}\\Big[\\Big(\\frac{\\partial \\mathcal{L}}{\\partial w^{(l)}_{ij}}\\Big)^2\\Big] &\\approx  \\mathbb{E}[(\\delta^{(l)}_i)^2] \\mathbb{E}[\\phi^2(h^{(l-1)}_j)] = \\tilde q_{\\alpha \\alpha}^l \\mathbb{E}[\\phi^2(h^{(l-1)}_j)] \n",
    "\\end{align}\n",
    "\n",
    "where the approximation relationship is due to using mean-field approximation (no correlation between $(\\delta^{(l)}_i)^2$ and $\\phi^2(h^{(l-1)}_j)$). The expected magnitude of the gradient is then proportional to $\\tilde q_{\\alpha \\alpha}^l$. It is shown that\n",
    "\n",
    "\\begin{align}\n",
    "\\tilde q_{\\alpha \\alpha}^l = \\tilde q_{\\alpha \\alpha}^{l+1}\\frac{N_{l+1}}{N_l}\\chi\n",
    "\\end{align}\n",
    "\n",
    "which implies $\\tilde q_{\\alpha \\alpha}^l$ grow/shrink exponentially as one backprop towards the first layer. Therefore $\\tilde q_{\\alpha \\alpha}^l$ can be written as\n",
    "\n",
    "\\begin{align}\n",
    "\\tilde q_{\\alpha \\alpha}^l &= \\tilde q_{\\alpha \\alpha}^L e^{-(L-l)/\\xi_\\nabla}\\\\\n",
    "\\xi_\\nabla^{-1} &= -\\log \\chi\n",
    "\\end{align}\n",
    "\n",
    "Now gradient vanishing/explosion can be understood as follow. In the ordered phase $\\chi < 1$ and $\\xi_\\nabla > 0$, the magnitude of the gradient decreases exponentially as one backprop towards the first layer and is therefore expected to vanish. On the other hand, in the chaotic phase $\\chi > 1$, $\\xi_\\nabla < 0$, the magnitude of the gradient increases exponentially and hence explodes. At the critical line $\\chi = 1$, the system reaches a fine balance between gradient explosion and vanishing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Effects of different initializations and NN architectures\n",
    "\n",
    "#### Preliminaries: Jacobian spectrum\n",
    "\n",
    "Another way to understand $\\chi$ is as follow. from \\ref{chain_rule_1}, the input-to-output Jacobian can be written as\n",
    "\n",
    "\\begin{align}\n",
    "J &\\equiv \\prod_{l=1}^L J^{(l)} \\\\\n",
    "&= \\prod_{l=1}^L D^{(l)} W^{(l)}\n",
    "\\end{align}\n",
    "\n",
    "where $D^{(l)}_{ij} = \\phi'(h^{(l)}_i)\\delta_{ij}$. The distribution of singular value of $J$ encodes information about feedforward dynamics of the NN. For example, the second moment of the distribution of the squared singular value of $J^{(l)}$ is identical to $\\chi$, which can be written in the following form\n",
    "\n",
    "\\begin{align}\n",
    "\\chi = \\frac{1}{N}\\langle \\text{Tr} (DW)^\\top(DW) \\rangle\n",
    "\\end{align}\n",
    "\n",
    "here since $D^{(l)}$ and $W^{(l)}$ are independent of $l$, we simply denote layerwise $D$ and $W$ without superscript. For the full input-to-output Jacobian $J = \\prod_{l=1}^L J^{(l)}$, the second moment of the distribution of the squared singular value is $\\chi^L$. Therefore, the wider the squared singular value distribution, the larger the gradient explodes.\n",
    "\n",
    "#### Orthogonal vs Gaussian initialization [ref](https://arxiv.org/pdf/1802.09979.pdf)\n",
    "\n",
    "The first initialization concerns the distribution of the weights: should we start from a Gaussian distributed weights or orthogonal weights? [Ref](https://arxiv.org/pdf/1802.09979.pdf) showed that orthogonal weight initialization is preferred as it produces a more stable Jacobian spectrum of a very deep network.\n",
    "\n",
    "#### Bounded vs unbounded activation functions \n",
    "\n",
    "#### Dropout\n",
    "\n",
    "#### Batch normalization\n",
    "\n",
    "Batch normalization is first proposed to avoid **internal covariate shift** which is understood as a shift in the distribution of the data in the deeper layer of the network due to the change in weights in the earlier layers. Consider layer $l$ of the NN of the $t$-th iteration\n",
    "\n",
    "\\begin{align}\n",
    "x^{(l)}_i &= F\\Big(\\sum_j W_{ij}^{(l)}(t) x^{(l-1)}_j + b^{(l)}_j \\Big) \\\\\n",
    "&= G(W^{(1)}(t),W^{(2)}(t),...W^{(l)}(t), b^{(1)}(t),b^{(2)}(t),...b^{(l)}(t), x^{(0)})\n",
    "\\end{align}\n",
    "\n",
    "where $G$ is the composite of $l$ number of $F$, and is a function of the input $x^{(0)}$. Even with the same $x^{(0)}$, i.e. data drawn from the same batch, $x^{(l+1)}$ will be different due to the changes in $W_{ij}^{(k)}(t), \\ \\forall k < l$ over time. Consequently, the distribution of $x^{(l+1)}$ will tend to shift over time. Consider the optimization of the subsequent layers (layer $>l$), the loss function will take the form\n",
    "\n",
    "\\begin{align}\n",
    "\\mathcal{L} &= \\hat{\\mathcal{L}}(W^{(l+1)},W^{(l+2)},...W^{(L)}, b^{(l+1)},b^{(l+2)},...b^{(L)}, x^{(l+1)})\n",
    "\\end{align}\n",
    "\n",
    "The argument is that by normalizing the data in each batch, we improve the stability of the loss function $\\hat{\\mathcal{L}}$ as $x^{(l+1)}$ will not change much after the normalization. Therefore, ICS can be defined as\n",
    "\n",
    "\\begin{align}\n",
    "\\text{ICS} = \\| &\\nabla_{W^{(l)}(t)}\\mathcal{L}(W^{(1)}(t),W^{(2)}(t),...,W^{(l)}(t),...,W^{(L)}(t); x, y) - \\\\\n",
    "&\\nabla_{W^{(l)}(t)}\\mathcal{L}(W^{(1)}(t+1),W^{(2)}(t+1),...,W^{(l-1)}(t+1),W^{(l)}(t),...,W^{(L)}(t); x, y)\\|\n",
    "\\end{align}\n",
    "\n",
    "##### Pros and cons of batch norm\n",
    "As the activation function is mostly flat far away from the bias, covariate shift can lead to gradient vanishing problem. By placing batch morm between $W$ and $\\phi$, the data is re-centering before the activation function and therefore **improve the gradient vanishing problem**. \n",
    "\n",
    "Batch norm also **improves the robustness of NN to different hyperparameters**. This is because if the loss function is more smooth [ref](https://arxiv.org/pdf/1805.11604.pdf), the gradients will be more predictable, i.e. the path to minimum would not involves too much sharp turns. This allows for using a wider range of values of learning rate without risking overshooting, which might cause oscillation or divergence.\n",
    "\n",
    "Batch normalization also **speeds up SGD convergence** by reseting the scale of the variables in the loss function so that the (fixed) learning rate will not result in oscillations or slow convergence (similar to the idea of normalizing the data for any GD algorithm). *For algorithm with adaptive learning rate, will batch normalization still be useful? For example, in Newton's method, the learning rate is adaptive and changes depending on the Hessian matrix.*\n",
    "\n",
    "The downside of batch normalization is it forces the data to distributed around the linear region of the activation function and will **reduce the representational power of the network**. One way to improve that is to introduce two *learnable* parameters to further rescale and re-center the hidden layer outputs.\n",
    "\n",
    "Note that the ICS view of batch norm is challenged by [ref](https://arxiv.org/pdf/1805.11604.pdf), which made the claim using Deep Linear Network and fully batch training data (i.e. no stochasticity).\n",
    "\n",
    "##### Batch norm and signal propagation depth\n",
    "\n",
    "It is hypothesized [ref](https://arxiv.org/pdf/1611.01232.pdf) that applying batch norm helps increasing the depth scales by controlling the variance of the network weights. Since the scale of the input to each layer is normalized, this in turn control the variance of the weights in that layer. A network with lower network weight variance is less likely to be in the chaotic region where the network is more susceptible to gradient explosion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Optimization\n",
    "\n",
    "### 2.1 Proximal optimization\n",
    "\n",
    "[ref](https://www.cs.toronto.edu/~rgrosse/courses/csc2541_2021/slides/lec03.pdf)\n",
    "\n",
    "Given a function $f(\\theta)$, one way to find the minimum of the function is to use gradient descent. Starting from a given point $\\theta_0$, we can derive the direction of parameter update using first-order Taylor expansion\n",
    "\n",
    "\\begin{align}\n",
    "f(\\theta) &\\approx f(\\theta_0) + \\nabla f(\\theta^0)^\\top (\\theta - \\theta_0)\\\\\n",
    "\\theta^* &= \\arg \\min_\\theta( f(\\theta_0) + \\nabla f(\\theta^0)^\\top (\\theta - \\theta_0)) \\label{proximal_taylor_1}\\tag{Eq 2.1.1}\n",
    "\\end{align}\n",
    "\n",
    "The optimal $\\theta^*$ in \\ref{proximal_taylor_1} is infinite in the direction of $-\\nabla f(\\theta^0)$ as the optimization problem is unbounded due to the linearization of the function. To control of runaway solution, one can enforce that the step size taken should also be small be introducting an distance term to the optimization problem\n",
    "\n",
    "\\begin{align}\n",
    "\\theta^* &= \\arg \\min_\\theta( f(\\theta_0) + \\nabla f(\\theta^0)^\\top (\\theta - \\theta_0)) + \\|\\theta - \\theta_0\\|^2 \\label{proximal_taylor_1_dist}\\tag{Eq 2.1.2}\n",
    "\\end{align}\n",
    "\n",
    "To find $\\theta^*$ in \\ref{proximal_taylor_1_dist}, one take the gradient on the objective function and will recover the gradient descent equation\n",
    "\n",
    "\\begin{align}\n",
    "\\theta_{t+1} = \\theta_t - \\nabla f(\\theta_t)\n",
    "\\end{align}\n",
    "\n",
    "In general, the distance constraint does not have to be a Euclidean distance in \\ref{proximal_taylor_1_dist}. Any distance measure $\\rho(\\delta, \\theta^k)$, with a Lagrange multiplier $\\lambda$ can be used.\n",
    "\n",
    "#### Approximation 1 - natural gradient\n",
    "By taking the infinitestimal step size limit ($\\lambda \\to 0$), taking the first-order Taylor expansion of $f(\\theta)$ and second-order Taylor expansion of the distance function $\\rho$ will give the natural gradient equation\n",
    "\n",
    "\\begin{align}\n",
    "\\delta^* &= \\arg \\min_\\delta( f(\\theta_0) + \\nabla f(\\theta^0)^\\top \\delta + \\frac{\\lambda}{2} \\delta^\\top G \\delta)\\\\\n",
    "\\delta^* &= -\\lambda^{-1}G^{-1}\\nabla f(\\theta_0) \\label{proximal_natural_gradient}\\tag{Eq 2.1.3}\n",
    "\\end{align}\n",
    "\n",
    "where $G = \\nabla_\\delta^2 \\rho(\\delta, \\theta^k)$\n",
    "\n",
    "#### Approximation 2 - damped Newton method\n",
    "For the next approximation, take second-order Taylor expansion of both $f(\\theta)$ and $\\rho$. This results in the following update rule\n",
    "\n",
    "\\begin{align}\n",
    "\\delta^* = -(H + \\lambda G)^{-1}\\nabla f(\\theta_0) \\label{proximal_damped_newton}\\tag{Eq 2.1.4}\n",
    "\\end{align}\n",
    "\n",
    "where when Euclidean metric is used for $\\rho$, $G=I$ and the update becomes the **damped Newton method (trusted region method)**. \n",
    "\n",
    "Note that adaptive gradient algorithms (Appendix A2) has a very similar form to \\ref{proximal_damped_newton}, except (1) the way the Hessian is approximated (using the diagonal of the empirical FIM) and (2) the gradient is scaled by $H^{-1/2}$ instead of $H^{-1}$ in \\ref{proximal_damped_newton}. To the first point, empirical FIM is in general not a good approximation to the Hessian (it is only a good approximation if the model is optimal). The second point is to address the impact of the first point by rescaling the Hessian approximation by a square root [ref](https://www.cs.toronto.edu/~rgrosse/courses/csc2541_2021/readings/L05_normalization.pdf).\n",
    "\n",
    "### 2.2 Gauss-Newton matrix - approximation to Hessian matrix\n",
    "\n",
    "[ref](https://www.cs.toronto.edu/~rgrosse/courses/csc2541_2021/readings/L02_Taylor_approximations.pdf)\n",
    "\n",
    "The motivation for building a Gauss-Newton matrix is to approximate the Hessian matrix of a loss function in the weight space using the Hessian matrix of the loss function in the output space. To understand this, the former loss function is of the form\n",
    "\n",
    "\\begin{align}\n",
    "\\mathscr{L}(\\theta) =  L(y, f(x, \\theta))\n",
    "\\end{align}\n",
    "\n",
    "whereas the latter loss function is of the form\n",
    "\n",
    "\\begin{align}\n",
    "\\mathcal{L}(y, z) =  L(y, z)\n",
    "\\end{align}\n",
    "\n",
    "with $z = f(x, \\theta)$. Therefore, chain-rule is needed to derive the Hessian in the weight space, $\\nabla_\\theta^2 \\mathscr{L}$. Using chain rule, the Hessian matrix of $\\mathscr{L}$ becomes\n",
    "\n",
    "\\begin{align}\n",
    "\\nabla_\\theta^2 \\mathscr{L} = J_{z\\theta}^\\top H_z J_{z\\theta} + \\sum_i \\frac{\\partial L}{\\partial z_i}\\nabla_\\theta^2 [f(x, \\theta)]_i \\label{Hessian_approx_chain_rule}\\tag{Eq 2.2.1}\n",
    "\\end{align}\n",
    "\n",
    "where $J_{z\\theta}$ is the Jacobian matrix $J_{z\\theta} = \\partial z/\\partial \\theta = \\partial f/\\partial \\theta$ and $H_z$ is the Hessian matrix in the output space, i.e. $H_z = \\nabla^2_z L$. \\ref{Hessian_approx_chain_rule} consists of two terms. The first term can be understood as linearizing the model function $f$ at $\\theta$ and performing a quadratic approximation to the loss function $L$ at $\\theta$ as it only involves Jacobian (first-order term) of $z$ and Hessian (second-order term) of the loss function. The first term \n",
    "\n",
    "\\begin{align}\n",
    "G \\equiv J_{z\\theta}^\\top H_z J_{z\\theta}\n",
    "\\end{align}\n",
    "\n",
    "is known as the **Gauss-Newton matrix** and the approximation of the weight-space Hessian using the Gauss-Newton matrix is known as the Gauss-Newton approximation.\n",
    "\n",
    "<img src=\"figures/C11/gauss_newton_approx.png\" width=500>\n",
    "\n",
    "\n",
    "The justification of the Gauss-Newton approximation is the assumption that the second term is negligible, which will be the case if all training samples fit the model perfectly. If this happen, $\\partial L/\\partial z_i$ will be zero for all $i$ and the second term can therefore be neglected. This implies that Gauss-Newton approximation loses its validity for weak models.\n",
    "\n",
    "#### Benefit of using Gauss-Newton matrix\n",
    "\n",
    "Since $G$ only involves first-order derivative of the function $f$, activation function such as ReLU can be used. This is not the case when computing $H_\\theta$ since it will involves second-order derivative of the activation function, which in the case of ReLU, will give zero.\n",
    "\n",
    "### 2.3 Output space gradient descent\n",
    "\n",
    "The optimization problem in the weight space is usually much more difficult than that in the output space. One famous example is the optimization of the Rosenbrock function [ref](https://www.cs.toronto.edu/~rgrosse/courses/csc2541_2021/slides/lec03.pdf). GD in the parameter space is shown to have difficulty in finding the global minimum whereas GD in the output space finds the global minimum easily.\n",
    "\n",
    "#### Pullback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Learnability of DNN\n",
    "\n",
    "### 3.1 Challenge #1: Gradient vanishing/explosion\n",
    "\n",
    "[ref](https://www.cs.toronto.edu/~rgrosse/courses/csc2541_2021/slides/lec08.pdf)\n",
    "\n",
    "As mentioned in the Section 1, gradient vanishing/explosion is akin to the largest singular value of the Jacobian matrix. Since repeat multiplication of Jacobian matrix is dominated by the largest singular value, its value determine whether gradient will vanish or explode.\n",
    "\n",
    "Another view of gradient vanishing/explosion can be understood by its relationship to sharpness of the minima, which is quantified by the largest eigenvalue of the Hessian matrix, of the loss function landscape. To understand this, consider the Gauss-Newton approximation of the Hessian matrix\n",
    "\n",
    "\\begin{align}\n",
    "H \\approx G = \\mathbb{E}[J_{zw} H_z J_{zw}]\n",
    "\\end{align}\n",
    "\n",
    "where $H_z=\\nabla_z \\mathcal{L}$. For a square error loss, $H_z = \\pmb 1$ and $G$ becomes the classical Gauss-Newton matrix\n",
    "\n",
    "\\begin{align}\n",
    "H \\approx \\mathbb{E}[J_{zw}J_{zw}]\n",
    "\\end{align}\n",
    "\n",
    "It is speculated that the largest eigenvalue of $H$ is the largest singular value of $J_{zw}$. This relates large singular value of $J$, which indicates the network susceptibility to input perturbation and potential of gradient explosion, to the sharpness of the minima of the loss function. It is shown in [ref](https://arxiv.org/pdf/1609.04836.pdf) that sharp minima are less able to generalize (to unseen data) then flat minima. Hence there is a preference of converging to a flatter minimum (see the batch size discussion in section 3.5)\n",
    "\n",
    "### 3.2 Challenge #2: Proliferation of saddle points\n",
    "[ref](https://ganguli-gang.stanford.edu/pdf/14.SaddlePoint.NIPS.pdf)\n",
    "\n",
    "It is mentioned in [ref](https://ganguli-gang.stanford.edu/pdf/14.SaddlePoint.NIPS.pdf) that saddle points are common in high dimensional space. This could be understood using Random Matrix Theory (RMT). Assuming the Hessian matrix of a given critical point in the energy manifold is a Gaussian random matrix, the distribution of eigenvalues follow a semi-circle law, centered at zero. This implies a critical point is likely a saddle point (mixed of positive and negative eigenvalues). As the energy level of the critical point decreases, the distribution shifted far to the right, hence the critical point is more likely to be a global minimum (no direction with negative curvature). On the other hand, as the energy level of the critical point increases, the distribution shifted to the left, with a concentration at the value zero. This implies the proliferation of saddle point with plateau surrounding it (implied by the zero eigenvalues). The plateau makes gradient descent very slow. Below we summarized how different optimizers behave near a saddle point.\n",
    "\n",
    "#### (Vanilla) gradient descent\n",
    "- slow descent when the gradient is small\n",
    "- always move away from saddle points\n",
    "\n",
    "#### Newton method\n",
    "- gradient ascent when curvature is negative, therefore will move towards saddle points\n",
    "\n",
    "#### Trusted region approach\n",
    "- damp the Hessian by adding a constant $\\alpha$ to the diagonal, effectively removing negative curvatures\n",
    "- with large damping factor $\\alpha$ can result in slow GD\n",
    "\n",
    "#### Truncated Newton method, BFGS approximation\n",
    "- ignore negative curvature directions\n",
    "- cannot escape saddle point since negative curvature directions are ignored\n",
    "\n",
    "#### Natural gradient descent\n",
    "- natural gradient favors moves that result in small change to the model outcome, see A3 for detail\n",
    "\n",
    "### 3.3 Challenge #3: Failure of Gradient based algorithms \n",
    "[ref](https://arxiv.org/pdf/1703.07950.pdf)\n",
    "\n",
    "### 3.4 Challenge #4: Catastrophic intereference/forgetting\n",
    "[ref1](https://www.pnas.org/doi/10.1073/pnas.1611835114), [ref2](https://en.wikipedia.org/wiki/Catastrophic_interference#Elastic_weight_consolidation)\n",
    "\n",
    "### 3.5 NN tuning best practices\n",
    "[ref](https://www.cs.toronto.edu/~rgrosse/courses/csc2541_2021/readings/L05_normalization.pdf)\n",
    "\n",
    "\n",
    "#### Batch size\n",
    "\n",
    "- Large batch vs small batch size\n",
    "\n",
    "||small batch size | large batch size |\n",
    "|---|---|---|\n",
    "|gradient| noisy | less noisy |\n",
    "|weight recency per descent| more recent weight | older weight |\n",
    "|# weight update required| more | less |\n",
    "|parallelism| less | more$^*$ |\n",
    "|effective learning rate$^{**}$| small | large | \n",
    "\n",
    "$^*\\ $ the benefit of larger batch size to parallelism plateau for large enough batch size (called **maximal data parallelism**)\n",
    "\n",
    "$^{**}$ Algorithms that approximate Hessian matrix (e.g. ADAM, RMSprop). This is due to batch empirical FIM inversely proportional batch size.\n",
    "\n",
    "- When using SGD, use large learning rate early to get close to the optimum, then reduce learning rate to reduce fluctuation [ref](https://www.cs.toronto.edu/~rgrosse/courses/csc2541_2021/slides/lec07.pdf)\n",
    "- Smaller batch size make use of more recent update weight, hence is better than larger batch size. For example, \\begin{align} \\theta^{(k)} &\\leftarrow \\theta^{(k-1)} - \\alpha \\nabla \\mathcal{L}^{(k)}(\\theta^{(k-1)})\\\\ \\theta^{(1)} &\\leftarrow \\theta^{(0)} - \\frac{\\alpha}{S} \\sum_{k=1}^{S}\\nabla\\mathcal{L}^{(k)}(\\theta^{(0)})\\end{align} where the first line is updated $S$ (batch size) times and the second line is updated once. At each step, a one-sample batch GD (first line) uses the most recent weight from the previous step to evaluate the loss function.\n",
    "- The number of FLOP operations are independent of batch size. However, larger batch size can make good use of parallel computing (parallelize matrix multiplication). \n",
    "- With BN, the batch size affects the amount of stochastic regularization.\n",
    "- It is shown in [ref](https://arxiv.org/pdf/1609.04836.pdf) that using a larger batch with a SGD type optimizer leads to the convergence of sharper minima, which has poor generalization. This implies that noise in gradient has an implicit regularization effect, i.e. converging to flat minima. This is however challenged in [ref](https://arxiv.org/pdf/1811.03600.pdf) which highlight two confounders in the literature:\n",
    "    - batch norm creates explicit regularization effect and is more pronounced for smaller batches\n",
    "    - some papers fixed the number of epochs, so models with larger batch are trained with fewer iterations\n",
    "    \n",
    "#### Learning rate\n",
    "- For Adam, RMSprop, etc., \n",
    "    - larger batch sizes result in more gradient noise, and hence smaller steps\n",
    "    - larger batch sizes increase the effective learning rate (batch empirical FIM inversely proportional batch size)\n",
    "- With homogeneous normalizers such as BN, WN, and LN, there is **implicit learning rate decay**\n",
    "    - the norm of the weights $\\|w^{(k)}\\|^2$ increase like $\\sqrt{k}$ over $k$-GD steps\n",
    "    - the norm of the weights affects the effective learning rate. This can be understood in the following figure <img src=\"figures/C11/BN_effective_LR.png\" width=500> where the following identity of homogeneous function, which is true for BN, WN and LN, is exploited \\begin{align} \\nabla \\mathcal{L}(\\gamma \\pmb w) = \\gamma^{-1} \\nabla \\mathcal{L}(\\pmb w)\\end{align}\n",
    "    - the learning rate decays implicitly like $k^{-1/2}$. This turns out to be the form of learning rate scheduler for popular optimizer such as Adagrad (Appendix A2)\n",
    "    - explicit learning rate only have transient effect and is only relevant in the beginning of the learning phase\n",
    "    \n",
    "    \n",
    "\n",
    "- Different optimizers (SGD, Adam, K-FAC, etc.) have different effective learning rate schedules when combined with homogeneous normalizers.\n",
    "- For many architectures, weight decay fundamentally affects the training dynamics, so it can’t be tuned independently of the optimizer (as we might expect for a regularization hyperparameter).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Self-attention as rank-lowering operation \n",
    "[ref1](https://proceedings.mlr.press/v139/dong21a/dong21a.pdf)\n",
    "[ref2](https://arxiv.org/pdf/2206.03126.pdf)\n",
    "[ref3](https://arxiv.org/pdf/2006.04768.pdf)\n",
    "[ref4](https://arxiv.org/pdf/2109.04553.pdf)\n",
    "\n",
    "### 4.1 Rank collapsing of pure self-attention\n",
    "\n",
    "It was shown in [ref](https://proceedings.mlr.press/v139/dong21a/dong21a.pdf) that without MLP and skip-connection, a pure attention architecture will lead to rank-collapsing , which is a phenomenon where the all of the embedded token are the same (and hence the output embedded matrix is a rank-one matrix). Rank collapsing is therefore the most extreme case of rank-lowering. \n",
    "\n",
    "The first paper to this phenomenon [ref](https://proceedings.mlr.press/v139/dong21a/dong21a.pdf) pointed out a doubly exponential convergence with depth $L$ of the embedding matrix to a rank-1 matrix. The exponential convergence was hypothesized to be due to the fact that the self-attention heads mix tokens faster when formed from a low-rank matrix, leading to a cascading effect of rank-reduction.\n",
    "\n",
    "It was discussed in [ref](https://arxiv.org/pdf/2206.03126.pdf) that rank-collapsing causes the gradients of the queries and keys to vanish at initialization. This is reminiscent to the ordered phase in DNN, where the input manifold is contracted into a single point $(c^l \\to c^* = 1)$.\n",
    "\n",
    "#### Skip-connection\n",
    "Skip (or residual) connection (with depth-dependent scaling) is shown to help approximately preserve the cosine similarity of the tokens since it just simply an identity operator. It is argued in [ref](https://proceedings.mlr.press/v139/dong21a/dong21a.pdf) that the existence of skip connection factorially increases the number of paths of length $l$ from\n",
    "\n",
    "\\begin{align}\n",
    "|\\mathcal{P}_l| = H^l\n",
    "\\end{align}\n",
    "\n",
    "for a $l=L$-layer pure self-attention network with $H$ heads in each layer to\n",
    "\n",
    "\\begin{align}\n",
    "|\\mathcal{P}_l| = {L \\choose l} H^l\n",
    "\\end{align}\n",
    "\n",
    "for a $L$-layer self-attention network with $H$ heads and skip-connection in each layer. $l$ is the number of layers that do not pass the skip-connection. It was hypothesized that the presense of paths that utilize skip-connections helps reduce rank collapsing.\n",
    "\n",
    "[Ref](https://arxiv.org/pdf/2206.03126.pdf) further investigated the effect of skip-connection on the average cosine similarity of a pair of tokens (similar to $c^l_{12}$ in Section 1). Instead of just considering using skip-connection, they introduce two parameters ($\\alpha_1$ for after self-attention head, $\\alpha_2$ for after the MLP) to control for the degree of skip-connections. They showed that by scaling down the signal through the self-attention head and MLP at layer $l$ by a factor of $1/\\sqrt{l}$ (hence increasing residual connection proportionally) allows for the preseverance of the cosine similarity of the pair of tokens at deep depth $L\\to\\infty$:\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbb{E}[C(\\pmb X^{(L)})] &= (1+\\alpha_1^2)^L(1+\\alpha_2^2)^L C(\\pmb X)\\\\\n",
    "\\lim_{L\\to \\infty}\\mathbb{E}[C(\\pmb X^{(L)})] &= \\Big(1+\\Big(\\alpha_1/\\sqrt{L}\\Big)^2\\Big)^L \\Big(1+\\Big(\\alpha_2/\\sqrt{L}\\Big)^2\\Big)^L C(\\pmb X) \\\\\n",
    "&= e^{\\tilde \\alpha_1 + \\tilde \\alpha_2}C(\\pmb X)\n",
    "\\end{align}\n",
    "\n",
    "where $\\tilde \\alpha_1 \\equiv \\alpha_1/\\sqrt{L}$ and $C(\\pmb X) = \\sum_{k,k'}\\langle \\pmb X^l_k \\pmb X^l_{k'}\\rangle$. Similar conclusion was reached for the preseverance of the token norm at deep depth. In fact, they also concluded that without the depth-dependent scaling $1/\\sqrt{l}$, the correlation of the tokens will increase and again result in rank collapsing. The $1/\\sqrt{L}$ scaling term has been proposed in previous studies [ref](https://arxiv.org/pdf/1803.01719.pdf) to stablize the residual networks.\n",
    "\n",
    "#### MLP\n",
    "\n",
    "It is shown that MLP slightly helps reducing the rank-1 convergence speed. The upper bound of how severe rank-collapsing happens in layer $L$ is inflated by $\\lambda^{\\frac{3L-1}{2}}$, where $\\lambda$ is the Lipschitz constant of the MLP. Therefore, the more powerful/expressive the MLP, the more it helps reducing the rank-1 convergence speed. \n",
    "\n",
    "From a signal propagation perspective, an more expressive an MLP, the more likely it is in the chaotic region, which corresponds to when gradient is more likely to explode. Since rank-reduction is a consequence of gradient vanishing, the gradient explosion effect in a more expressive MLP counteracts the gradient vanishing effect in the self-attention network.\n",
    "\n",
    "#### Layer normalization\n",
    "\n",
    "Layer norm is shown [ref](https://proceedings.mlr.press/v139/dong21a/dong21a.pdf) to not useful in avoiding rank collapsing. Mathematically, layer norm does not change the way self-attention is written as a product of matrices and since elementary row/column operation (layer norm involves multiplying a diagonal matrix, therefore is an elementary matrix) does not change the rank of a matrix, it is argued that layer norm does not help avoiding rank collapsing.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Efficient approximations to self-attention mechanism\n",
    "\n",
    "It was discussed in [ref](https://arxiv.org/pdf/2006.04768.pdf) and [ref](https://arxiv.org/pdf/2109.04553.pdf) that due to the rank-lowering characteristic of self-attention mechanism, it can be approximated by matrix factorization and by projecting the context mapping matrix into a lower dimensional space first, which usually take less time ($O(n^2)$) and space complexity then self-attention mechanism.\n",
    "\n",
    "Recall that the self-attention mechanism can be written as\n",
    "\n",
    "\\begin{equation}\n",
    "h_i^{(l+1)} = \\sum_{ij}a_{ij} V h_j^{(l)}\n",
    "\\end{equation}\n",
    "\n",
    "where the attention function $a_{ij}$ is \n",
    "\n",
    "\\begin{equation}\n",
    "a_{ij} = \\sum_{ij} \\text{softmax}(Q^{(l)} h_i^{(l)} \\cdot K^{(l)} h_j^{(l)})\n",
    "\\end{equation}\n",
    "\n",
    "and in matrix form\n",
    "\n",
    "\\begin{align}\n",
    "H^{l+1} &= \\text{softmax}\\Big(\\frac{Q^{(l)} H^{(l)} \\cdot K^{(l)} H^{(l)}}{\\sqrt{d_k}}\\Big)H^{(l)}V \\\\\n",
    "&= PH^{(l)}V \\label{SA_matrix_form}\\tag{Eq 2.1}\n",
    "\\end{align}\n",
    "\n",
    "where $d_k$ is the dimension of the target subspace the operators $Q$ and $K$ project an embedded vector $h^{(l)}$ (of dimension $d_m$) into. It is shown in [ref](https://arxiv.org/pdf/2006.04768.pdf) that $P$ is a low-rank matrix whose spectrum is skewed. This suggests $P$ can be approximated by the top subset of singular values. The spectrum distribution in higher layers is more skewed than in lower layers, meaning that, in higher layers, more information is concentrated in the largest singular values and the rank of $P$ is lower. This insight is used to proposed approximations to more efficiently compute self-attention.\n",
    "\n",
    "#### Linformer [ref](https://arxiv.org/pdf/2006.04768.pdf)\n",
    "\n",
    "A linear self-attention mechanism aims to \n",
    "\n",
    "(1) project the $n\\times n$ context mapping matrix $P$ into a lower dimensional space such that the $\\dim(\\tilde P)=n\\times k$ through a projection matrix $E^{(l)}$; \n",
    "\n",
    "(2) project a $n \\times d$-dimension $K^{(l)} H^{(l)}$ onto a $k \\times d$-dimension $F^{(l)}H^{(l)}V$ through a projection matrix $F^{(l)}$. \\ref{SA_matrix_form} is modified to\n",
    "\n",
    "\\begin{align}\n",
    "H^{(l+1)} &= \\text{softmax}\\Big(\\frac{Q^{(l)} H^{(l)} \\cdot E^{(l)}  K^{(l)}H^{(l)}}\n",
    "{\\sqrt{d_k}}\\Big)F^{(l)}H^{(l)}V\\\\\n",
    "\\end{align}\n",
    "\n",
    "The time complexity is reduced from $O(n^2)$ to $O(nk)$.\n",
    "\n",
    "It was mentioned in [ref](https://proceedings.mlr.press/v139/dong21a/dong21a.pdf) that due to the imposed low-rankness of the Linformer, rank-collapse happens even faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A1 iterative equations for $q^l$ and $c^l$\n",
    "\n",
    "Reiterating the DNN forward equations\n",
    "\n",
    "\\begin{align}\n",
    "h_i^{(l)} &= \\sum_j w_{ij}^{(l)} x^{(l-1)}_j + b_i \\\\\n",
    "x^{(l)}_i &= \\phi(h_i^{(l)}) \\label{DNN_forward_pass}\\tag{Eq A1.1}\n",
    "\\end{align}\n",
    "\n",
    "Define \n",
    "\n",
    "\\begin{align}\n",
    "q^l &\\equiv \\langle (h^{(l)}_i)^2\\rangle \\\\\n",
    "&= \\Big\\langle \\sum_{jk} w_{ij}^{(l)}w_{ik}^{(l)} x^{(l-1)}_j x^{(l-1)}_k \\Big\\rangle + \\langle b_i^2\\rangle \\\\\n",
    "&\\approx \\sum_{jk} \\langle w_{ij}^{(l)}w_{ik}^{(l)}\\rangle \\langle x^{(l-1)}_j x^{(l-1)}_k\\rangle + \\langle b_i^2\\rangle \\\\\n",
    "&= \\sum_{j=1}^{N_{l-1}} \\frac{\\sigma_w^2}{N_{l-1}} (x^{(l-1)}_j)^2 + \\sigma_b^2 \\\\\n",
    "&= \\frac{\\sigma_w^2}{N_{l-1}}\\sum_{j=1}^{N_{l-1}} \\phi^2(h_j^{(l-1)}) + \\sigma_b^2 \\label{q_iterative_1}\\tag{Eq A1.2}\n",
    "\\end{align}\n",
    "\n",
    "where the approximation in the third line is from the mean-field ansatz. The reduction of the two sums into one from line 3 to 4 is due to the identity $\\langle w_{ij}^{(l)}w_{ik}^{(l)}\\rangle = \\delta_{jk}\\sigma_w^2/N_{l-1}$. Finally, the first line in \\ref{DNN_forward_pass} is used in the last line in \\ref{q_iterative_1}.\n",
    "\n",
    "In the large layer width limit $N_{l-1}\\gg 1$, the sum can be approximated by an integral. The random variable in the sum is $h_j^{(l-1)}$, which follows a Gaussian with zero mean and variance $q^{l-1}$. The last line in \\ref{q_iterative_1} can be written as\n",
    "\n",
    "\\begin{align}\n",
    "q^l &= \\sigma_w^2\\int dh N(0, q^{l-1})  \\phi^2(h) + \\sigma_b^2 \\\\\n",
    "&= \\sigma_w^2\\int dh \\frac{1}{\\sqrt{2\\pi q^{l-1}}}e^{-h^2/q^{l-1}}  \\phi^2(h) + \\sigma_b^2 \\\\\n",
    "q^l &= \\sigma_w^2\\int dz \\frac{e^{-z^2}}{\\sqrt{2\\pi}} \\phi^2\\Big(\\sqrt{q^{l-1}}z\\Big) + \\sigma_b^2 \\\\\n",
    "q^l &= \\sigma_w^2\\int Dz \\phi^2\\Big(\\sqrt{q^{l-1}}z\\Big) + \\sigma_b^2 \n",
    "\\end{align}\n",
    "\n",
    "where the change of variable $ h^2/q^{l-1} = z^2$ is used in the last line. To simplify the notation, define $Dz=e^{-z^2}/\\sqrt{2\\pi}$\n",
    "\n",
    "Similarly for $q^l_{12}$\n",
    "\n",
    "\\begin{align}\n",
    "q^l_{12} = \\sigma_w^2\\int Dz_1 Dz_2 \\phi(u_1)\\phi(u_2) + \\sigma_b^2\n",
    "\\end{align}\n",
    "\n",
    "where \n",
    "\n",
    "\\begin{align}\n",
    "u_1 &= \\sqrt{q^{l-1}_{11}}z_1 \\\\\n",
    "u_2 &= \\sqrt{q^{l-1}_{22}}\\Big[c^{l-1}_{12} z_1 + \\sqrt{1- (c^{l-1}_{12})^2} z_2 \\Big]\n",
    "\\end{align}\n",
    "\n",
    "$u_2$ is constructed in this way to capture the correlation between the two random variables $z_1$ and $z_2$. The magnitude of the correlation is quantified by $c^{l-1}_{12}$.\n",
    "\n",
    "From the previous definition of the recursive equation for $c^l_{12}$\n",
    "\n",
    "\\begin{align}\n",
    "c^l_{12} &= \\mathcal{C}(c^{l-1}_{12}, q^l_{11}=q^*, q^l_{22}=q^* \n",
    "\\mid \\sigma_w, \\sigma_b) \\sim q^l_{12}\n",
    "\\end{align}\n",
    "\n",
    "The quantity $\\chi$ can then be derived\n",
    "\n",
    "\\begin{align}\n",
    "\\chi &= \\frac{\\partial c^l_{12}}{\\partial c^{l-1}_{12}}\\bigg|_{c=c^*=1} \\\\\n",
    "&= \\sigma_w^2\\int Dz_1 Dz_2 \\phi(u_1) \\frac{\\partial}{\\partial c^{l-1}_{12}} \\phi(u_2) \\\\\n",
    "&= \\sigma_w^2\\int Dz_1 Dz_2 \\phi(u_1) \\phi'(u_2)\\frac{\\partial u_2}{\\partial c^{l-1}_{12}} \\\\\n",
    "&= \\sigma_w^2\\int Dz_1 Dz_2 \\phi(u_1) \\phi'(u_2)\\Big( z_1 - \\frac{c^{l-1}_{12}}{\\sqrt{1-(c^{l-1}_{12})^2}} z_2\\Big)\\\\\n",
    "&= \\ ...\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A2 Gradient descent based optimizers\n",
    "\n",
    "This section will review the most popular optimizers used to train NNs\n",
    "\n",
    "#### Gradient Descent\n",
    "\n",
    "\\begin{equation}\n",
    "\\theta(t+1) \\leftarrow \\theta(t) -\\eta_t \\nabla_\\theta E(\\theta)\n",
    "\\end{equation}\n",
    "\n",
    "where $\\eta_t$ is the learning rate. Usually it will be a function ('scheduler') of iteration $t$ to ensure the algorithm converges to the solution. Since the change depends on the slope of $E(\\theta)$, the steeper the loss function, the larger the update step will be. This is usually not ideal, and will likely move $\\theta$ to a point that is not on the path to the minimum. To solve this issue, we need second-order information of the loss function.\n",
    "\n",
    "#### Stochastic gradient descent\n",
    "\n",
    "The stochasticity of SGD comes from the random subsample of data (a batch) used to compute the gradient. The main advantage of using SGD is that instead of calculating the full loss function landscape using the full dataset, only a random sample of the loss function (based on the random batch) is calculated. Since loss function for NN is highly non-convex, doing GD in the full loss function is susceptable to getting stuck at local minima. Since the loss function changes from batch to batch, it is less likely the parameter will get stuck at a local minima as a minimum point in one batch could be a non-minimum point in another batch. Another advantage computation of gradient using batches of data allows parallelization.\n",
    "\n",
    "#### Newton's method\n",
    "\n",
    "Taylor expand the loss function to second order near $\\theta$\n",
    "\n",
    "\\begin{equation}\n",
    "E(\\theta + v) \\approx E(\\theta) + \\nabla_\\theta E(\\theta) \\cdot v + \\frac{1}{2}v^T H v\n",
    "\\end{equation}\n",
    "\n",
    "where $H$ is the Hessian matrix, defined as $H_{ij} = \\partial^2 E /\\partial \\theta_i \\partial \\theta_j$. By choosing the optimal $v$ such that $\\theta$ moves to the optimal solution in the next step and keeping up to the second order term\n",
    "\n",
    "\\begin{align}\n",
    "\\nabla_\\theta E(\\theta + v_\\text{opt}) &\\approx \\nabla_\\theta E(\\theta) + H \\cdot v_\\text{opt}\\\\\n",
    "\\nabla_\\theta E(\\theta + v_\\text{opt}) &= 0 \\\\\n",
    "v_\\text{opt} &= H^{-1}\\nabla_\\theta E(\\theta)\n",
    "\\end{align}\n",
    "\n",
    "which is similar to the equation of gradient descent. Therefore, the inverse of $H^{-1}$ can be interpreted as the optimal learning rate. Intuitively, $H$ capture the curature of the loss function at a point in different directions. Therefore, the more curvature the loss function is in certain direction, the slower $\\theta$ will move in that direction.\n",
    "\n",
    "The downside of Newton's method is that computing $H^{-1}$ is computationly very expensive as $H$ has the dimension of square of total number of parameters. Since $H$ can be viewed as the Jacobian of the gradient, one can use forward mode of autodiff on a computation graph computed from the reverse mode autodiff to compute the Hessian-vector products in linear time. This trick is called **forward-over-reverse** [ref](https://www.cs.toronto.edu/~rgrosse/courses/csc2541_2021/readings/L02_Taylor_approximations.pdf). However, this trick requires the existence of the computation graph from backprop.\n",
    "\n",
    "A geometric understanding of Newton's method is as follow. At a given point of the loss function, approximate it with a elliplical parabola. Newton's method gives the direction that is directly pointing from that point to the minimum of the 'imagined' ellipical parabola. This means the GD vector in general does not point in the same direction as the gradient at that point due to $H^{-1}$. Note that if the parabola is spherical instead of elliplical, $H$ will be diagonal and the GD vector will point in the same direction of the gradient at that point. \n",
    "\n",
    "In the jargon of Riemannian geometry, $H^{-1}\\nabla_\\theta E(\\theta)$ is called the *natural gradient* (to be discussed in A3) with $H$ acts as the metric tensor $g(\\theta)$ of the Riemannian manifold.\n",
    "\n",
    "#### RMSprop\n",
    "\n",
    "\\begin{align}\n",
    "g(t) &= \\nabla_\\theta E(\\theta) \\\\\n",
    "s(t) &= \\beta s(t-1) + (1-\\beta) g^2(t) \\\\\n",
    "\\theta(t) &= \\theta(t-1) - \\alpha \\frac{g(t)}{\\sqrt{s(t) + \\epsilon 1}}\n",
    "\\end{align}\n",
    "\n",
    "RMSprop tries to update each weight by a constant magnitude $\\alpha$. This is because for ordinary SGD, individual derivatives (of different weights) might be very large or very small, resulting in taking a very large or very small step size in the GD. By ensuring that each update has magnitude approximately $\\alpha$ (e.g. $10^{-3}$), we ensure that each weight is changed by only a little bit in each iteration, but over many (e.g. 1000) iterations, the weights still have the opportunity to move a long distance. The learning rate $\\alpha$ acts as the free parameter to the algorithm and so RMSprop is not completely 'adaptive'.\n",
    "\n",
    "#### Adagrad\n",
    "\n",
    "\\begin{align}\n",
    "g(t) &= \\nabla_\\theta E(\\theta) \\\\\n",
    "s(t) &= s(t-1) + g^2(t) \\\\\n",
    "\\theta(t) &= \\theta(t-1) - \\alpha \\frac{g(t)}{\\sqrt{s(t) + \\epsilon 1}}\n",
    "\\end{align}\n",
    "\n",
    "Adagrad is very similar to RMSprop, except instead of keeping a running exponential average of the square of the gradient, it keeps the sum of the square of the gradient. This naturally inflates the factor $\\sqrt{s(t) + \\epsilon 1}$ over time and therefore act as an implicit learning rate decay. [ref](https://www.cs.toronto.edu/~rgrosse/courses/csc2541_2021/readings/L05_normalization.pdf) note that RMSprop and ADAM is preferred to Adagrad because of the lack of implicit learning rate decay.\n",
    "\n",
    "#### ADAM\n",
    "\n",
    "ADAM incorporates momentum in the optimization algorithm. Momentum allows a speed up under a persistent and slightly sloped terrain, which will otherwise results in small update step in GD and SGD (first-order algorithm). This is done by keeping a running average of first and second moment of the gradient:\n",
    "\n",
    "\\begin{align}\n",
    "g(t) &= \\nabla_\\theta E(\\theta) \\\\\n",
    "m(t+1) &= \\alpha m(t) + (1-\\alpha) g(t) \\\\\n",
    "s(t+1) &= \\beta s(t) + (1-\\beta) g^2(t) \\\\\n",
    "\\hat m(t) &= \\frac{m(t)}{1-\\alpha^t} \\\\\n",
    "\\hat s(t) &= \\frac{s(t)}{1-\\beta^t} \\\\\n",
    "\\theta(t+1) &= \\theta(t) - \\eta_t \\frac{\\hat m(t)}{\\sqrt{\\hat s(t) + \\epsilon}}\n",
    "\\end{align}\n",
    "\n",
    "Therefore, the effective learning rate is reduced (increased) in directions where the gradients are consistently large (small).\n",
    "\n",
    "Technically, ADAM is still a first order algorithm as it does not explicitly compute the Hessian matrix. It approximates it by computing the running average $s(t+1)$. The advantage of using ADAM over first order algorithm such as GD in high dimensional optimization is that ADAM is able to break from a saddle point, which is proliferate in high dimension, than GD.\n",
    "\n",
    "It was mentioned in [ref](https://arxiv.org/pdf/1412.6980.pdf) and [ref](https://arxiv.org/pdf/1301.3584.pdf) that ADAM is similar to natural gradient descent (see A3) in terms of employing a preconditioner that adapts to the geometry of the data since $\\hat s(t)$ is an approximation to the diagonal of the Fisher Information Matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A3 Natural gradient\n",
    "\n",
    "[ref1](https://agustinus.kristia.de/techblog/2018/03/14/natural-gradient/), [ref2](https://andrewcharlesjones.github.io/journal/natural-gradients.html), [ref3](https://arxiv.org/pdf/1301.3584.pdf)\n",
    "\n",
    "The natural gradient could be understood as follow. For a vanilla GD, the gradient points in the direction of largest change in the loss function per unit change in the parameter. The change is defined in terms of Euclidean distance. Natural gradient generalizes this by allowing considering any distance defined by a given metric $g(\\theta)$.\n",
    "\n",
    "Consider a loss function of a model over a domain of parameters $\\pmb w \\in \\mathbb{R}^d$ \n",
    "\n",
    "\\begin{equation}\n",
    "\\mathcal{L}(\\pmb w)\n",
    "\\end{equation}\n",
    "\n",
    "The goal in machine learning is to derive the optimal value for $\\pmb w$ that minimize the above loss function. This can be achieved by performing gradient descent. In general, the domain of the loss function can be considered as a manifold and the loss function is a function that lives on the manifold. One can equip a metric $g$ to the manifold to give the definition of distance between two points on the manifold. \n",
    "\n",
    "\\begin{equation}\n",
    "(ds)^2 = \\sum_{ij} g_{ij}(\\pmb w)dw_i dw_j\n",
    "\\end{equation}\n",
    "\n",
    "A manifold with an endowed metric is called a **Riemannian manifold** $\\Omega(g, \\mathbb{R}^d)$. Usually when considering the loss function, we choose $g = \\pmb I$. In general, $g$ can be any $N \\times N$ symmetric matrix. It is shown in [ref](http://www.yaroslavvb.com/papers/amari-why.pdf) that for a given metric $g$ in $\\Omega$, the gradient is modified to\n",
    "\n",
    "\\begin{equation}\n",
    "-g^{-1}(\\pmb w) \\nabla \\mathcal{L}(\\pmb w)\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "#### Natural gradient with Fisher information matrix\n",
    "\n",
    "One can define the distance between two points in the parameter space not by the actual difference in the parameter value but in the difference in the model predictions. KL divergence of the likelihood distribution under two different parameters ($\\pmb \\theta$ and $\\pmb \\theta + \\pmb \\delta$) can therefore be used.\n",
    "\n",
    "\\begin{equation}\n",
    "\\Delta s^2 = D_{KL}\\big[P(y\\mid x, \\pmb \\theta) \\| P(y\\mid x, \\pmb \\theta + \\pmb \\delta)\\big] \\label{KL_dist}\\tag{Eq 2.1}\n",
    "\\end{equation}\n",
    "\n",
    "for small difference in the parameter value, it is shown that KL divergence is roughly symmetric and the metric tensor can be approximated by the Fisher information matrix. \n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\Delta s^2 \\approx \\frac{1}{2}\\sum_{ij} F_{ij}\\delta_i \\delta_j\n",
    "\\end{equation}\n",
    "\n",
    "where\n",
    "\n",
    "\\begin{equation}\n",
    "F = \\Big\\langle\\nabla_{\\pmb\\theta}\\log P(y\\mid x,\\pmb\\theta)\\ \\nabla_{\\pmb\\theta}\\log P(y\\mid x,\\pmb\\theta)^\\top \\Big\\rangle_{x \\sim P(x),\\ y \\sim P(y\\mid x)}\n",
    "\\end{equation}\n",
    "\n",
    "Note that the expectation is over $x$ and $\\hat{y} \\mid x$ (also known as true FIM) instead of over $x$ and the empirical $y$ (empirical FIM). \n",
    "\n",
    "The GD equation becomes\n",
    "\n",
    "\\begin{equation}\n",
    "\\pmb\\theta_{t+1} = \\pmb\\theta_t - \\eta F^{-1}\\nabla_\\theta \\mathcal{L}(\\pmb \\theta)\n",
    "\\end{equation}\n",
    "\n",
    "Therefore, for a parameter with a strong effect on the outcome probability of the model, the natural gradient will penalize movement in that direction due to the inverse of the metric. Consequently, natural gradient chooses a direction that minimizes the loss function (first-order Taylor expansion) while maintaining constant distance (second-order Taylor expansion, as measured by KL divergence) [ref](https://arxiv.org/pdf/1301.3584.pdf). \n",
    "\n",
    "#### Derivation of the Fisher Information Matrix using differential geometry\n",
    "\n",
    "Traditionally, an ML model is a function $f$, controlled by some parameters $\\theta$, that takes in input $x$ and produces output $y$: $f(\\theta): x \\to y$. In the probabilistic formulism, the function is implicitly defined. I.e. the output probability density $p(x\\ \\mid \\theta)$ is defined instead. The set of density function forms a functional manifold $\\mathcal{F}$. Each element on $\\mathcal{F}$ is parameterized by $\\theta \\in \\mathbb{R}^p$. One of the properties of manifold is the existence of a *local* map $\\varphi$ that maps element on $\\mathcal{F}$ to an infinite dimension vector (function) space $V$\n",
    "\n",
    "\\begin{align}\n",
    "\\varphi: \\mathcal{F} \\to V\n",
    "\\end{align}\n",
    "\n",
    "the output probability density $p(x\\ \\mid \\theta)$ lives in the vector space $V$. \n",
    "\n",
    "So far, the manifold does not give a sense of elevation or curvature. To equip the manifold with such properties, define a function $\\mathcal{L}$ on $\\mathcal{F}$ \n",
    "\n",
    "\\begin{align}\n",
    "\\mathcal{L}: \\mathcal{F} \\to \\mathbb{R}\n",
    "\\end{align}\n",
    "\n",
    "$\\mathcal{L}$ is a loss function. The following graph describes the relationship between the $\\varphi$ map and the loss function\n",
    "\n",
    "<img src=\"figures/C11/data_manifold_map.png\" width=200>\n",
    "\n",
    "The loss function can be explicitly written as \n",
    "\n",
    "\\begin{align}\n",
    "\\mathcal{L} \\circ \\varphi^{-1}(\\theta) = \\langle - \\ln p(z \\mid \\theta) \\rangle_z\n",
    "\\end{align}\n",
    "\n",
    "Once the loss function is defined, the distance on the manifold can be defined as the change in the loss function due to change in $\\theta$\n",
    "\n",
    "\\begin{align}\n",
    "\\Delta s^2 = \\langle (-\\ln p(z \\mid \\theta+\\delta) + \\ln p(z \\mid \\theta))^2 \\rangle_z \n",
    "\\end{align}\n",
    "\n",
    "First-order Taylor expand $\\ln p(z \\mid \\theta+\\delta)$ gives one of the forms of the Fisher Information Matrix $F$\n",
    "\n",
    "\\begin{align}\n",
    "\\Delta s^2 &= \\delta^\\top \\langle (\\nabla_\\theta \\ln p) (\\nabla_\\theta \\ln p)^\\top \\rangle_z \\delta \\\\\n",
    "&= \\delta^\\top F \\delta \\label{FIM_1}\\tag{Eq A3.1}\n",
    "\\end{align}\n",
    "\n",
    "\\ref{FIM_1} is expressed the square of the first-order derivative. Note that the FIM is **positive semi-definite** due to the square expression in \\ref{FIM_1}. Another form of FIM is to express it in terms of second-order derivative. This can be obtained by using the following relationships\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial^2}{\\partial \\theta^2} \\ln p = \\frac{1}{p}\\frac{\\partial^2 p}{\\partial \\theta^2} - \\Big(\\frac{\\partial}{\\partial \\theta} \\ln p \\Big)^2\n",
    "\\end{align}\n",
    "\n",
    "and \n",
    "\n",
    "\\begin{align}\n",
    "\\Big\\langle\\frac{1}{p}\\frac{\\partial^2 p}{\\partial \\theta^2}\\Big\\rangle_z &= \\int dz p(z\\mid \\theta)\\frac{1}{p(z\\mid \\theta)}\\frac{\\partial^2 p}{\\partial \\theta^2}\\\\\n",
    "&= \\frac{\\partial^2 }{\\partial \\theta^2} \\int dz p(z \\mid \\theta)\\\\\n",
    "&= 0\n",
    "\\end{align}\n",
    "\n",
    "With the above relationships, one can write \\ref{FIM_1} in another form\n",
    "\n",
    "\\begin{align}\n",
    "F = \\langle -\\nabla_\\theta^2 \\ln p(z \\mid \\theta) \\rangle_z \\label{FIM_2}\\tag{Eq A3.2}\n",
    "\\end{align}\n",
    "\n",
    "Finally, \\ref{FIM_2} can also be derived by defining the distance metric to be the KL divergence, i.e. $g(\\theta) = D_{KL}(p(z \\mid \\theta) \\| p(\\theta + \\delta))$ and second-order Taylor expand $\\ln p(z \\mid \\theta+\\delta)$ and Taylor expand $ \\ln(1+x) \\approx x$ by assuming $\\delta$ is small:\n",
    "\n",
    "\\begin{align}\n",
    "&D_{KL}(p(z \\mid \\theta) \\| p(\\theta + \\delta))\\\\\n",
    "&= \\int dz p(z \\mid \\theta) \\ln \\Big(\\frac{p(z \\mid \\theta)}{p(z \\mid \\theta + \\delta)} \\Big)\\\\\n",
    "&= \\int dz p(z \\mid \\theta) \\ln p(z \\mid \\theta) - \\int dz p(z \\mid \\theta) \\ln p(z \\mid \\theta + \\delta)\\\\\n",
    "&= \\int dz p \\ln p - \\int dz p \\ln \\Big(p + \\frac{\\partial p}{\\partial \\theta}\\delta + \\frac{1}{2}\\frac{\\partial^2 p}{\\partial \\theta^2}\\delta^2 + O(\\delta^3)\\Big)\\\\\n",
    "&\\approx \\int dz p \\ln p - \\int dz p \\ln \\Big(p \\Big(1 + \\frac{1}{p}\\frac{\\partial p}{\\partial \\theta}\\delta + \\frac{1}{2p}\\frac{\\partial^2 p}{\\partial \\theta^2}\\delta^2\\Big)\\Big)\\\\\n",
    "&\\approx -\\int dz p\\Big(\\delta \\frac{\\partial}{\\partial \\theta}\\ln p + \\delta^2\\frac{1}{2p}\\frac{\\partial^2 p}{\\partial \\theta^2} - \n",
    "\\frac{1}{2}\\Big(\\delta\\frac{\\partial}{\\partial \\theta}\\ln p + \\delta^2\\frac{1}{2p}\\frac{\\partial^2 p}{\\partial \\theta^2}\\Big)^2 + O(\\delta^5)\\Big)\\\\\n",
    "&= -\\int dz p \\Big( \\delta\\frac{\\partial}{\\partial \\theta} \\ln p + \\delta^2\\frac{1}{2p}\\frac{\\partial^2 p}{\\partial \\theta^2} - \\frac{1}{2}\\Big(\\delta \\frac{\\partial^2}{\\partial \\theta^2} \\ln p\\Big)^2\\Big) + O(\\delta^3)\\\\\n",
    "&\\approx -\\Big\\langle -\\frac{1}{2}\\Big(\\frac{\\partial}{\\partial \\theta}\\ln p\\Big)^2\\delta^2\\Big\\rangle\\\\\n",
    "&= -\\Big\\langle \\frac{1}{2}\\frac{\\partial^2}{\\partial \\theta^2}\\ln p\\Big\\rangle\n",
    "\\end{align}\n",
    "\n",
    "which is identical to \\ref{FIM_2}. Note that, FIM is only a (second-order) approximation to KL divergence.\n",
    "\n",
    "The significance of \\ref{FIM_2} is that it can be used to approximate the Hessian matrix in any second-order optimization algorithms. The reason why \\ref{FIM_2} is only an approximation of the true Hessian matrix is in the expectation function. For \\ref{FIM_2}, the expectation is over the empirical distribution for the input $x$ and the distribution of the model prediction $p(y \\mid x, \\theta)$. For the real Hessian, the expectation is over the empirical distribution for $(x,y)$ [ref](https://arxiv.org/pdf/1412.1193.pdf). Also, from \\ref{FIM_1}, FIM is PSD, which is not true for the real Hessian matrix (existence of saddle points and plateaus)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A4 Morse Lemma\n",
    "[ref](https://encyclopediaofmath.org/wiki/Morse_lemma)\n",
    "\n",
    "#### Morse index\n",
    "\n",
    "The Morse index of a critical point $p$ of a smooth function $f$ on a manifold $M$ is equal, by definition, to the negative index of inertia of the Hessian of $f$ at $p$ (cf. Hessian of a function), that is, the dimension of the maximal subspace of the tangent space $T_p M$ of $M$ at $p$ on which the Hessian is negative definite. Such maximal subspace is spanned by negative eigenvalue eigenvectors at point $p$. Therefore, it is simply the number of negative eigenvalue at that point.\n",
    "\n",
    "Note that the above interpretation defines a surface as a function on a simple manifold $M$ (e.g. $\\mathbb{R}^N$). Therefore if the $M=\\mathbb{R}^N$, the tangent space of a point $p$ exists on the manifold $M$ itself. Another way to define a surface is the manifold (e.g. a sphere) itself, with a local mapping function $\\phi$ to a Cartesian space $\\mathbb{R}^N$. The link from the second to the first interpretation is established by the Nash embedding theorem.\n",
    "\n",
    "#### Morse Lemma\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A5 Mathematical formalism of Transformers [ref](https://arxiv.org/pdf/2312.10794.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A6 Other resources\n",
    "[CSC2541 Winter 2021 Topics in Machine Learning: Neural Net Training Dynamics](https://www.cs.toronto.edu/~rgrosse/courses/csc2541_2021/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
