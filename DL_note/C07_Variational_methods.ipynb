{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 7: Variational methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Modeling interactions using hidden variables\n",
    "\n",
    "The complex interactions among observed variables can be modeled/encoded by hidden (latent) variables. Marginalizing/intergating over the hidden variables can induce complex correlations in the remaining (observed) variables. In clustering, the observed variable is the data itself. The latent variables are the class assignment of the data. In topic modeling, the observed variables are the words in the documents. The latent variables are the topic assignment of a word in a document or topic distribution of a document.\n",
    "\n",
    "\n",
    "## 2. Expectation Maximization algorithm\n",
    "\n",
    "### 2.1 Mathematical formulation\n",
    "\n",
    "Assume the unobserved $z$ and observed variables $x$ follow the true distribution $p(z, x \\mid \\theta)$ and we hope to approximate it with a variational distribution $q(z\\mid x)$. The free energy $F_q(\\theta, x)$ can be written as\n",
    "\n",
    "\\begin{align}\n",
    "F_q(\\theta, x) &= -\\langle \\log p(z, x \\mid \\theta)\\rangle_q - H_q \\\\\n",
    "&= -\\int dz q(z\\mid x) \\log p(z, x \\mid \\theta) + \\int dz q(z\\mid x)\\log q(z\\mid x)\n",
    "\\end{align}\n",
    "\n",
    "where the negative log likelihood acts as the energy term of the free energy. The entropy term $H_q$ can be understood as we want to maximize the log likelihood using a distribution with enough variation (more uncertainty in the random variable). The (true) free energy $F_p(\\theta, x)$ can be written as\n",
    "\n",
    "\\begin{equation}\n",
    "F_p(\\theta, x) = -\\log p(x \\mid \\theta)\n",
    "\\end{equation}\n",
    "\n",
    "There is no entropy term since there is no variational distribution. The $x$ in the free energies can be thought of as quenched disorder $J$ in spin model and $z$ is like the spin itself. Note that the term free energy is used loosely here - despite having resemblance with the free energy in statistical physics, they are not strictly the same. \n",
    "\n",
    "It can be shown that the variational distribution $q(z\\mid x)$ is the distribution that minimize $F_q(\\theta, x)$, given a value of $\\theta^{(t-1)}$ and has the form\n",
    "\n",
    "\\begin{align}\n",
    "q(z\\mid x) = \\arg\\min_q F_q(\\theta^{(t-1)}, x) = p(z \\mid x, \\theta^{(t-1)}) \\label{E_step}\\tag{Eq 2.1.1}\n",
    "\\end{align}\n",
    "\n",
    "where $p(z \\mid x, \\theta) = p(z,x\\mid \\theta)/p(x\\mid \\theta)$ via Bayes' theorem. The above equation is the Expectation step of the EM algorithm. Once $q(z\\mid x)$ is derived/updated, $\\theta$ can also be updated,\n",
    "\n",
    "\\begin{align}\n",
    "\\theta^{(t)} = \\arg\\min_\\theta F_q(\\theta, x)\n",
    "\\end{align}\n",
    "\n",
    "which is the Maximization step of EM. \n",
    "\n",
    "Even the variational distribution $q$ in \\ref{E_step} is equated with the conditional probability of the true distribution $p$, the distribution is not 'true' since $\\theta$ is not optimized. Therefore, $q$ is variational in the sense that it is only an approxiation of $p$ given $\\theta \\neq \\theta^*$. Technically, the above example does not involve a variational distribution since functionally it is still the same as the true probability. As we will see in the spin glass and LDA example, the variational distribution can also have a very different functional form from the true probability. \n",
    "\n",
    "The difference between $F_p(\\theta, x)$ and $F_q(\\theta, x)$ is related by the KL divergence\n",
    "\n",
    "\\begin{equation}\n",
    "D_{KL}(q(z\\mid x) \\mid\\mid p(z\\mid x, \\theta)) = F_q(\\theta, x) - F_p(\\theta, x) \\label{KL_free_energy}\\tag{Eq 2.1.2}\n",
    "\\end{equation}\n",
    "\n",
    "Since the variational free energy $F_q$ is always the upper bound of the true free energy $F_p$, minimizing $F_q$ using the above EM algorithm is equivalent to minimizing the KL divergence. This ensures $F_q$ evolves closer to the true free energy. Note that direct computation of KL divergence involves computing the marginal distribution $p(x\\mid \\theta)$ (due to Bayes' rule $p(z\\mid x,\\theta)=p(z,x\\mid \\theta)/p(x\\mid \\theta)$), which is intractable. However, minimizing KL divergence is the same as minimizing the variational free energy $F_q$, which involves the joint distribution $p(x,z\\mid \\theta)$, which does not require marginalization and is thus tractable.\n",
    "\n",
    "### 2.2 Example 1: Mean-Field approximation in spin glass model\n",
    "\n",
    "The probability distribution of an spin glass model spin configuration $s$ is  \n",
    "\n",
    "\\begin{align}\n",
    "p(s)&=\\exp(-\\beta E(s))/Z, \\ \\ \\text{where}  \\\\\n",
    "Z&=\\text{Tr}_{\\{s\\}}\\exp(-\\beta E(s)) \\\\\n",
    "E(s) &= \\sum_{i,j}J_{ij}s_i s_j - \\sum_{i}h_i s_i\n",
    "\\end{align}\n",
    "\n",
    "$Z$ is in general computationally intractable (number of terms $\\sim O(2^N)$). Therefore $p(s)$ is difficult in compute. The purpose of variational inference is to approximate $p(s)$ with a simplier (factorized) *variational* distribution $q(s,\\theta)$:  \n",
    "\n",
    "$q(s,\\theta)=\\exp(\\sum_i s_i\\theta_i)/Z_q$.  \n",
    "\n",
    "The variational distribution assumes there is no correlation among $s_i$ and the distribution is thus factorizable. This is equivalent to the assumption of MFT. The variational inference of $q(s, \\theta)$ is done by minimizing the difference between the free energy under $p$, $F_p(\\beta)$ and the free energy under $q$, $F_q(\\theta)$. The two free energies are related via the KL-divergence:  \n",
    "\n",
    "$F_q(\\theta) = F_p(\\beta) + D_{KL}(p\\mid\\mid q)$  \n",
    "\n",
    "Therefore this is equivalent to minimizing the KL-divergence. The EM algorithm results in the follow pair of equations\n",
    "\n",
    "\\begin{align}\n",
    "&\\theta_i=\\beta \\sum_j J_{ij}s_j + h_i \\\\\n",
    "&s_i=\\tanh(\\theta_i)\n",
    "\\end{align}\n",
    "\n",
    "The first equation is the *maximization* step and the second the *expectation* step. The two equations are updated asynchronous fashion until convergence.\n",
    "\n",
    "### 2.3 Example 2: LDA\n",
    "\n",
    "\n",
    "#### Inference problem\n",
    "\n",
    "The inference problem in LDA is to figure out the probability of topic $z_i$ of each word $w_i$\n",
    "\n",
    "\\begin{align}\n",
    "p(z,\\theta \\mid w,\\alpha, \\beta) = \\frac{p(z, w, \\theta \\mid\\alpha, \\beta)}{p(w\\mid\\alpha, \\beta)}\n",
    "\\end{align}\n",
    "\n",
    "$p(w \\mid \\alpha, \\beta)$ is marginalized over the hidden variables $\\theta$ and $z$ and is intractable due to the coupling between $\\beta$ and $\\theta$. The above equation can be solved by assuming a variational distribution which does not mix $\\beta$ and $\\theta$. Instead of calculating $p(z,\\theta \\mid w,\\alpha, \\beta)$, we instead calculate the following variational distribution\n",
    "\n",
    "\\begin{align}\n",
    "q(z,\\theta \\mid \\gamma, \\phi) = q(\\theta \\mid \\gamma)\\prod_{j=1}^N q(z_j \\mid \\phi_j)\n",
    "\\end{align}\n",
    "\n",
    "$\\gamma$ and $\\phi$ can be solved by minimizing the KL divergence between $q(z,\\theta \\mid \\gamma, \\phi)$ and $p(z,\\theta \\mid w,\\alpha, \\beta)$ using EM algorithm. From the previous discussion, minimizing KL divergence directly is intractable. However, this is equivalent to minimizing $F_q$\n",
    "\n",
    "\\begin{equation}\n",
    "F_q = -\\langle P(z, w, \\theta \\mid \\alpha, \\beta)\\rangle_q - \\langle \\log q(\\theta, z)\\rangle_q\n",
    "\\end{equation}\n",
    "\n",
    "which only involves the joint probability $P(z, w, \\theta \\mid \\alpha, \\beta)$ which does not involve marginalization and is thus tractable.\n",
    "\n",
    "#### parameter estimation\n",
    "\n",
    "The parameter estimation problem in LDA is to estimate $\\alpha$ and $\\beta$ by maximizing the likelihood of observing all the words\n",
    "\n",
    "\\begin{equation}\n",
    "l(\\alpha, \\beta) = \\sum_{d=1}^D \\log p(w_d \\mid \\alpha, \\beta)\n",
    "\\end{equation}\n",
    "\n",
    "Again, the computation of $p(w_d \\mid \\alpha, \\beta)$ is intractable and variational methods used in the inference problem can be leveraged here.\n",
    "\n",
    "Note that EM works for parametric models (number of parameters does not scale with the number of data). In case of non-parametric models (e.g. DP, dimension of $\\theta$ scales with $N$), other techniques such as MCMC can be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix\n",
    "\n",
    "### KL divergence and free energy\n",
    "\n",
    "Another derivation of the relationship between KL divergence and free energy is as follow. Starting from the expression of KL divergence measuring the distance between $q(x)$ and $p(x)$\n",
    "\n",
    "\\begin{align}\n",
    "D_{KL}(q(x)\\| p(x)) = \\sum_{x} q(x) \\ln\\Big(\\frac{q(x)}{p(x)}\\Big)\n",
    "\\end{align}\n",
    "\n",
    "Assuming the distribution $p(x)$ follows a Boltzmann distribution $p(x) = \\exp(-\\beta E(x))/Z$\n",
    "\n",
    "\\begin{align}\n",
    "D_{KL}(q(x)\\| p(x)) &= \\sum_{x} q(x) \\ln\\Big(\\frac{Z q(x)}{\\exp(-\\beta E(x))}\\Big) \\\\\n",
    "&= \\sum_{x} q(x) \\Big(\\ln Z + \\ln q(x) -\\beta E(x)\\Big) \\\\\n",
    "&= -\\Big(\\sum_{x} q(x)E(x) - \\sum_{x} q(x)\\ln q(x)\\Big) + \\ln Z \\\\\n",
    "&= F + \\ln Z\n",
    "\\end{align} \n",
    "\n",
    "Therefore, KL divergence is minimum when the Helmholtz free energy $F$ equals to $-\\ln Z$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
