{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19dc693a",
   "metadata": {},
   "source": [
    "# Bayesian Matrix Factorization + Document Embedding Prior"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d247f026",
   "metadata": {},
   "source": [
    "## Intuition\n",
    "\n",
    "Sentence or document embedding captures the contextual meaning of document by representing documents of similar meaning with similar vectors. In theory, if two documents have similar words, they should have similar meaning and therefore should be represented by similar vectors. However, the association between word co-occurance and document similarity is implicit. \n",
    "\n",
    "On the other hand, count-based technique such as matrix factorization and LDA explicitly model the similarity of documents using the co-occurance of words by creating latent variables, called topics that is shared by both word generation and document decomposition.\n",
    "\n",
    "To marry these two concepts, let's start with factorizing the probability of a word-document matrix $w_{dn}$\n",
    "\n",
    "\\begin{equation}\n",
    "\\Pr(w_{dn}) = \\sum_k \\Pr(w_n \\mid k) \\Pr(k \\mid d)\n",
    "\\end{equation}\n",
    "\n",
    "where $\\Pr(w_n \\mid k)$ means the likelihood of seeing word $w_n$ in topic $k$, and $\\Pr(k \\mid d)$ means the likelihood of topic $k$ exists in document $d$. In topic modeling, we are interested in inferring the topic mix of a document, i.e. calculating $\\Pr(k \\mid d)$. The place where we infuse the document embedding is $\\Pr(w_n \\mid k)$. Even though, by clustering the document embedding, a document belongs to cluster/topic $k$, it can include another topic $k'$ because the document also contains words that show up in topic $k'$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab58bfd7",
   "metadata": {},
   "source": [
    "## Bayesian Matrix Factorization\n",
    "\n",
    "Let $\\{w_{dn}\\}_{d=1, n=1}^{D, N_d}$ denotes a corpus of $D$ documents, each with $N_d$ words. The probability of observing such corpus is simply\n",
    "\n",
    "\\begin{equation}\n",
    "\\prod_{d=1}^D\\prod_{n=1}^{N_d}\\Pr(w_{dn})\n",
    "\\end{equation}\n",
    "\n",
    "To do topic modeling, we assume the above equation can be parametrized by $\\vec{\\theta}_d$, which is a vector of dimension $K$ that describes the probability of topic $1$ to $K$ being assigned to document $d$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\prod_{d=1}^D\\prod_{n=1}^{N_d}\\Pr(w_{dn}) = \\prod_{d=1}^D\\prod_{n=1}^{N_d}\\int d\\vec{\\theta}_d\\Pr(w_{dn}\\mid \\vec{\\theta}_d)\\Pr(\\vec{\\theta}_d) \\label{full_likelihood_2}\\tag{Eq 2}\n",
    "\\end{equation}\n",
    "\n",
    "In LDA, $\\Pr(\\vec{\\theta}_d)$ is further assumed to follow a Dirichlet distribution $\\text{Dir}(\\vec{\\theta}_d \\mid \\alpha)$. $\\Pr(w_{dn}\\mid \\vec{\\theta}_d)$ can further be rewritten in terms of topic $k$ as \n",
    "\n",
    "\\begin{equation}\n",
    "\\Pr(w_{dn}\\mid \\vec{\\theta}_d) = \\sum_k \\Pr(w_{dn}\\mid k)\\Pr(k \\mid \\vec{\\theta}_d) \\label{MF}\\tag{Eq 3}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "The above equation becomes \\ref{full_likelihood_2} becomes\n",
    "\n",
    "\\begin{equation}\n",
    "\\prod_{d=1}^D\\prod_{n=1}^{N_d}\\Pr(w_{dn} \\mid \\alpha) = \\prod_{d=1}^D\\prod_{n=1}^{N_d}\\int d\\vec{\\theta}_d\\sum_k \\Pr(w_{dn}\\mid k)\\Pr(k \\mid \\vec{\\theta}_d) \\text{Dir}(\\vec{\\theta}_d \\mid \\alpha) \\label{full_likelihood_3}\\tag{Eq 4}\n",
    "\\end{equation}\n",
    "\n",
    "$\\Pr(k \\mid \\vec{\\theta}_d)$ is essentially the $k$-th component of $\\vec{\\theta}_d$. We will denote this as $\\theta_{dk}$. $\\Pr(w_{dn}\\mid k)$ can be understood as the likelihood of observing the $n$-th word in document $d$ given the document is assigned topic $k$. This is where the sentence embedding is infused into the Bayesian MF framework.\n",
    "\n",
    "## Document embedding and topic assignment\n",
    "\n",
    "A document $\\vec{w}_d$, which consists of a vector of words in the LDA world, can be expressed in a more abstract, but context aware space using embedding techniques, such as Sentence BERT. Assuming each document is now embedded in a high dimensional space using Sentence BERT, one can group documents with similar contextual meaning into a cluster. This is usually done by first projecting the high dimensional space into a low dimensional space using technique like PCA, t-SNE and UMAP, then perform any clustering algorithm. Each cluster now consists of a collection of documents. Since a document consists of words, each cluster can be thought of as a collection of words. \n",
    "\n",
    "With this view, one can now express $\\Pr(w_n \\mid k)$ empirically as\n",
    "\n",
    "\\begin{equation}\n",
    "\\Pr(w \\mid k) = \\frac{\\text{number of word $w$ in cluster $k$}}{\\text{total number of words in cluster $k$}}\n",
    "\\end{equation}\n",
    "\n",
    "To borrow the notation used in LDA, we will write the above empirical probability as $\\Pr(w \\mid k) = \\hat{\\phi}_{kw}$. \\ref{full_likelihood_3} now becomes\n",
    "\n",
    "\\begin{equation}\n",
    "\\prod_{d=1}^D\\prod_{n=1}^{N_d}\\Pr(w_{dn} \\mid \\alpha) = \\prod_{d=1}^D\\prod_{n=1}^{N_d}\\int d\\vec{\\theta}_d\\sum_k \\hat{\\phi}_{kw_{dn}} \\theta_{dk} \\text{Dir}(\\vec{\\theta}_d \\mid \\alpha) \\label{full_likelihood_4}\\tag{Eq 5}\n",
    "\\end{equation}\n",
    "\n",
    "## Maximum likelihood and inference\n",
    "\n",
    "To obtain the optimal $\\alpha$, we perform maximum log-likelihood of \\ref{full_likelihood_4}\n",
    "\n",
    "\\begin{equation}\n",
    "\\alpha^* = \\arg\\max_\\alpha \\sum_{d=1}^D\\sum_{n=1}^{N_d} \\log \\Pr(w_{dn} \\mid \\alpha)\n",
    "\\end{equation}\n",
    "\n",
    "with the optimal $\\alpha^*$, we can calculate the following term\n",
    "\n",
    "\\begin{equation}\n",
    "\\Pr(\\vec{\\theta}_d, w_{dn} \\mid \\alpha^*) = \\sum_k \\hat{\\phi}_{kw_{dn}} \\theta_{dk} \\text{Dir}(\\vec{\\theta}_d \\mid \\alpha)\n",
    "\\end{equation}\n",
    "\n",
    "Finally, the topic assignment distribution can be calculated using Bayes' theorem\n",
    "\n",
    "\\begin{equation}\n",
    "\\Pr(\\theta_d \\mid \\vec{w}_d) = \\frac{\\prod_{n=1}^{N_d}\\Pr(\\theta_d, w_{dn} \\mid \\alpha^*)}{\\prod_{d=1}^D\\prod_{n=1}^{N_d}\\Pr(w_{dn} \\mid \\alpha^*)}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00911bc1",
   "metadata": {},
   "source": [
    "## Inference with out-of-bag data\n",
    "\n",
    "WIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c91e406",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
