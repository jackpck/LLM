{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "85d80450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface_hub import hf_hub_download \n",
    "import torch\n",
    "from datasets import load_dataset, load_from_disk, Dataset\n",
    "import evaluate\n",
    "import transformers\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSequenceClassification, AutoModelForSeq2SeqLM\n",
    "from transformers import MistralForCausalLM\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "from peft import PeftConfig, PeftModel\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "from huggingface_hub import notebook_login\n",
    "from accelerate import init_empty_weights, load_checkpoint_and_dispatch, \\\n",
    "                        infer_auto_device_map, dispatch_model, load_checkpoint_in_model\n",
    "\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.pipeline import Pipeline\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import os\n",
    "import glob\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54de3fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    conn = sqlite3.connect(\"../data/PeaTMOSS.db\")    \n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd65adf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now in order to read in pandas dataframe we need to know table name\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "table_name_list = cursor.fetchall()\n",
    "table_name = []\n",
    "for name in table_name_list:\n",
    "    table_name.append(name[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5854151",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "WITH model_tb AS (\n",
    "    SELECT * FROM model \n",
    "    WHERE LOWER(repo_url) LIKE '%huggingface%'\n",
    "),\n",
    "\n",
    "paper_tb AS (\n",
    "    SELECT mtp.model_id, p.title AS paper_title\n",
    "    FROM model_to_paper mtp\n",
    "    LEFT JOIN paper p\n",
    "    ON mtp.paper_id = p.id    \n",
    "),\n",
    "\n",
    "model_task_tb AS (\n",
    "    SELECT mtmt.model_id, mt.name AS model_task\n",
    "    FROM model_to_model_task mtmt\n",
    "    LEFT JOIN model_task mt\n",
    "    ON mtmt.model_task_id = mt.id\n",
    ")\n",
    "\n",
    "SELECT m.context_id, m.downloads, \n",
    "lb.value AS limitation, \n",
    "d.title AS discussion_title,\n",
    "hp.value AS hyperparam, \n",
    "em.test AS test_metric, em.result AS test_result,\n",
    "p.paper_title,\n",
    "mt.model_task\n",
    "FROM model_tb m\n",
    "LEFT JOIN limitation_and_bias lb\n",
    "ON m.id = lb.model_id\n",
    "LEFT JOIN discussion d\n",
    "ON m.id = d.model_id\n",
    "LEFT JOIN hyper_parameters hp\n",
    "ON m.id = hp.model_id\n",
    "LEFT JOIN evaluation_metric em\n",
    "ON m.id = em.model_id\n",
    "LEFT JOIN paper_tb p\n",
    "ON m.id = p.model_id\n",
    "LEFT JOIN model_task_tb mt\n",
    "ON m.id = mt.model_id\n",
    "\"\"\"\n",
    "df_model_discussion = pd.read_sql_query(query, conn)\n",
    "df_model_discussion = df_model_discussion.iloc[6:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a04c220c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>test</th>\n",
       "      <th>dataset_id</th>\n",
       "      <th>result</th>\n",
       "      <th>model_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>SQUAD 1.1 F1/EM</td>\n",
       "      <td>10875</td>\n",
       "      <td>91.0/84.3</td>\n",
       "      <td>118085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Multi NLI Accuracy</td>\n",
       "      <td>10876</td>\n",
       "      <td>86.05</td>\n",
       "      <td>118085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Average</td>\n",
       "      <td>10877</td>\n",
       "      <td>82.3</td>\n",
       "      <td>105564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>SQuAD1.1</td>\n",
       "      <td>6493</td>\n",
       "      <td>90.2/83.2</td>\n",
       "      <td>105564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>SQuAD2.0</td>\n",
       "      <td>7473</td>\n",
       "      <td>82.1/79.3</td>\n",
       "      <td>105564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>96</td>\n",
       "      <td>recall</td>\n",
       "      <td>10906</td>\n",
       "      <td>0.946809</td>\n",
       "      <td>37006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>97</td>\n",
       "      <td>f1</td>\n",
       "      <td>10906</td>\n",
       "      <td>0.941799</td>\n",
       "      <td>37006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>98</td>\n",
       "      <td>precision</td>\n",
       "      <td>10907</td>\n",
       "      <td>0.959818</td>\n",
       "      <td>37006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>99</td>\n",
       "      <td>recall</td>\n",
       "      <td>10907</td>\n",
       "      <td>0.957278</td>\n",
       "      <td>37006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>100</td>\n",
       "      <td>f1</td>\n",
       "      <td>10907</td>\n",
       "      <td>0.958546</td>\n",
       "      <td>37006</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     id                test  dataset_id     result  model_id\n",
       "0     1     SQUAD 1.1 F1/EM       10875  91.0/84.3    118085\n",
       "1     2  Multi NLI Accuracy       10876      86.05    118085\n",
       "2     3             Average       10877       82.3    105564\n",
       "3     4            SQuAD1.1        6493  90.2/83.2    105564\n",
       "4     5            SQuAD2.0        7473  82.1/79.3    105564\n",
       "..  ...                 ...         ...        ...       ...\n",
       "95   96              recall       10906   0.946809     37006\n",
       "96   97                  f1       10906   0.941799     37006\n",
       "97   98           precision       10907   0.959818     37006\n",
       "98   99              recall       10907   0.957278     37006\n",
       "99  100                  f1       10907   0.958546     37006\n",
       "\n",
       "[100 rows x 5 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_sql_query(\"SELECT * FROM evaluation_metric LIMIT 100\", conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "941b73cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.35.2\n",
      "2.3.0+cu118\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(transformers.__version__)\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6723431a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2022 NVIDIA Corporation\n",
      "Built on Wed_Sep_21_10:41:10_Pacific_Daylight_Time_2022\n",
      "Cuda compilation tools, release 11.8, V11.8.89\n",
      "Build cuda_11.8.r11.8/compiler.31833905_0\n"
     ]
    }
   ],
   "source": [
    "! nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "401b4ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_repo_dir = 'D:/projects/LLM'\n",
    "cache_dir = '/cygdrive/d/projects/LLM/.cache'\n",
    "os.environ['TRANSFORMERS_CACHE'] = cache_dir\n",
    "os.environ['HF_HOME'] = cache_dir + '/huggingface'\n",
    "os.environ['XDG_CACHE_HOME'] = cache_dir\n",
    "os.environ['HF_DATASETS_CACHE'] = cache_dir\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "88ec1166",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mistral 7B\n",
    "model_name = 'mistralai/Mistral-7B-v0.1'\n",
    "model_path = cache_dir + '/models--mistralai--Mistral-7B-v0.1/snapshots/26bca36bde8333b5d7f72e9ed20ccda6a618af24'\n",
    "offload_folder = cache_dir + '/models--mistralai--Mistral-7B-v0.1/offload_folder'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5132601c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.embed_tokens.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.0.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.0.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.0.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.0.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.0.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.0.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.0.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.0.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.0.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.1.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.1.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.1.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.1.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.1.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.1.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.1.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.1.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.1.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.2.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.2.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.2.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.2.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.2.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.2.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.2.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.2.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.2.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.3.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.3.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.3.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.3.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.3.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.3.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.3.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.3.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.3.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.4.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.4.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.4.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.4.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.4.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.4.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.4.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.4.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.4.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.5.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.5.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.5.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.5.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.5.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.5.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.5.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.5.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.5.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.6.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.6.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.6.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.6.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.6.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.6.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.6.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.6.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.6.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.7.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.7.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.7.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.7.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.7.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.7.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.7.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.7.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.7.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.8.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.8.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.8.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.8.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.8.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.8.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.8.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.8.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.8.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.9.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.9.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.9.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.9.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.9.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.9.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.9.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.9.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.9.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.10.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.10.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.10.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.10.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.10.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.10.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.10.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.10.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.10.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.11.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.11.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.11.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.11.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.11.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.11.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.11.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.11.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.11.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.12.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.12.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.12.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.12.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.12.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.12.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.12.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.12.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.12.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.13.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.13.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.13.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.13.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.13.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.13.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.13.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.13.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.13.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.14.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.14.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.14.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.14.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.14.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.14.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.14.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.14.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.14.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.15.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.15.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.15.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.15.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.15.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.15.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.15.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.15.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.15.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.16.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.16.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.16.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.16.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.16.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.16.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.16.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.16.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.16.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.17.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.17.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.17.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.17.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.17.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.17.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.17.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.17.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.17.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.18.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.18.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.18.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.18.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.18.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.18.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.18.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.18.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.18.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.19.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.19.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.19.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.19.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.19.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.19.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.19.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.19.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.19.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.20.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.20.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.20.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.20.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.20.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.20.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.20.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.20.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.20.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.21.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.21.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.21.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.21.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.21.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.21.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.21.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.21.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.21.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.22.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.22.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.22.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.22.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 5.28 s\n",
      "Wall time: 5.95 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.22.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.22.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.22.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.22.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.22.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.23.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.23.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.23.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.23.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.23.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.23.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.23.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.23.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.23.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.24.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.24.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.24.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.24.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.24.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.24.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.24.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.24.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.24.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.25.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.25.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.25.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.25.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.25.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.25.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.25.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.25.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.25.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.26.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.26.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.26.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.26.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.26.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.26.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.26.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.26.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.26.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.27.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.27.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.27.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.27.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.27.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.27.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.27.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.27.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.27.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.28.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.28.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.28.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.28.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.28.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.28.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.28.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.28.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.28.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.29.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.29.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.29.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.29.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.29.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.29.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.29.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.29.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.29.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.30.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.30.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.30.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.30.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.30.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.30.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.30.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.30.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.30.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.31.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.31.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.31.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.31.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.31.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.31.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.31.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.31.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.31.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.norm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for lm_head.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "mistral_tokenizer = AutoTokenizer.from_pretrained(model_path, \n",
    "                                          cache_dir=cache_dir, \n",
    "                                          local_flies_only=True,\n",
    "                                          padding_side=\"left\")\n",
    "mistral_tokenizer.pad_token = mistral_tokenizer.eos_token \n",
    "\n",
    "# load empty model to save memory\n",
    "with init_empty_weights():\n",
    "    mistral_model = AutoModelForCausalLM.from_pretrained(model_path, \n",
    "                                             cache_dir=cache_dir,\n",
    "                                             local_files_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a0141b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\cygdrive\\d\\projects\\LLM_py38\\venv\\lib\\site-packages\\accelerate\\utils\\modeling.py:1363: UserWarning: Current model requires 335546880 bytes of buffer for offloaded layers, which seems does not fit any GPU's remaining memory. If you are experiencing a OOM later, please consider using offload_buffers=True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OrderedDict([('model.embed_tokens', 0),\n",
       "             ('model.layers.0', 0),\n",
       "             ('model.layers.1', 0),\n",
       "             ('model.layers.2', 0),\n",
       "             ('model.layers.3', 0),\n",
       "             ('model.layers.4', 0),\n",
       "             ('model.layers.5', 0),\n",
       "             ('model.layers.6', 0),\n",
       "             ('model.layers.7', 0),\n",
       "             ('model.layers.8', 0),\n",
       "             ('model.layers.9', 0),\n",
       "             ('model.layers.10', 0),\n",
       "             ('model.layers.11', 0),\n",
       "             ('model.layers.12.self_attn.q_proj', 0),\n",
       "             ('model.layers.12.self_attn.k_proj', 'cpu'),\n",
       "             ('model.layers.12.self_attn.v_proj', 'cpu'),\n",
       "             ('model.layers.12.self_attn.o_proj', 'cpu'),\n",
       "             ('model.layers.12.self_attn.rotary_emb', 'cpu'),\n",
       "             ('model.layers.12.mlp', 'cpu'),\n",
       "             ('model.layers.12.input_layernorm', 'cpu'),\n",
       "             ('model.layers.12.post_attention_layernorm', 'cpu'),\n",
       "             ('model.layers.13', 'cpu'),\n",
       "             ('model.layers.14', 'cpu'),\n",
       "             ('model.layers.15', 'cpu'),\n",
       "             ('model.layers.16', 'cpu'),\n",
       "             ('model.layers.17', 'cpu'),\n",
       "             ('model.layers.18', 'cpu'),\n",
       "             ('model.layers.19', 'cpu'),\n",
       "             ('model.layers.20', 'cpu'),\n",
       "             ('model.layers.21', 'cpu'),\n",
       "             ('model.layers.22', 'cpu'),\n",
       "             ('model.layers.23', 'cpu'),\n",
       "             ('model.layers.24', 'cpu'),\n",
       "             ('model.layers.25', 'cpu'),\n",
       "             ('model.layers.26', 'cpu'),\n",
       "             ('model.layers.27', 'cpu'),\n",
       "             ('model.layers.28', 'cpu'),\n",
       "             ('model.layers.29', 'cpu'),\n",
       "             ('model.layers.30', 'cpu'),\n",
       "             ('model.layers.31', 'cpu'),\n",
       "             ('model.norm', 'cpu'),\n",
       "             ('lm_head', 'cpu')])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 0: cuda. cpu: RAM\n",
    "device_map = infer_auto_device_map(\n",
    "    mistral_model, \n",
    "    max_memory={0: \"6GB\", 'cpu': \"30GB\"},\n",
    "    no_split_module_classes=[\"OPTDecoderLayer\"], \n",
    "    dtype='float16'\n",
    ")\n",
    "\n",
    "device_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5788a210",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/203 [00:00<?, ?w/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/88 [00:00<?, ?w/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "mistral_model = load_checkpoint_and_dispatch(mistral_model, \n",
    "                         model_path,\n",
    "                         device_map=device_map,\n",
    "                         dtype='float16',\n",
    "                         offload_folder=offload_folder,\n",
    "                         offload_state_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "994b3020",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context_id</th>\n",
       "      <th>limitation</th>\n",
       "      <th>model_task</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>0Tick/e621TagAutocomplete</td>\n",
       "      <td>Since DistilGPT2 is a distilled version of GPT...</td>\n",
       "      <td>text-generation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>0xDEADBEA7/DialoGPT-small-rick</td>\n",
       "      <td>The model may reflect biases present in the da...</td>\n",
       "      <td>conversational</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>0xDEADBEA7/DialoGPT-small-rick</td>\n",
       "      <td>The model may reflect biases present in the da...</td>\n",
       "      <td>text-generation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>1-800-BAD-CODE/punctuation_fullstop_truecase_e...</td>\n",
       "      <td>This model was trained on news data, and may n...</td>\n",
       "      <td>text2text-generation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>1-800-BAD-CODE/sentence_boundary_detection_mul...</td>\n",
       "      <td>This model was trained on `OpenSubtitles`, dat...</td>\n",
       "      <td>token-classification</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323742</th>\n",
       "      <td>zayedupal/movie-genre-prediction_distilbert-ba...</td>\n",
       "      <td>More information needed</td>\n",
       "      <td>text-classification</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323901</th>\n",
       "      <td>zekun-li/geolm-base-toponym-recognition</td>\n",
       "      <td>Significant research has explored bias and fai...</td>\n",
       "      <td>token-classification</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323991</th>\n",
       "      <td>zenham/khemx_m_e4_16h</td>\n",
       "      <td>The model may inherit biases from the training...</td>\n",
       "      <td>conversational</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323992</th>\n",
       "      <td>zenham/khemx_m_e4_16h</td>\n",
       "      <td>The model may inherit biases from the training...</td>\n",
       "      <td>text-generation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325107</th>\n",
       "      <td>zjunlp/zhixi-13b-diff-fp16</td>\n",
       "      <td>Due to time constraints, hardware limitations,...</td>\n",
       "      <td>text-generation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3273 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               context_id  \\\n",
       "76                              0Tick/e621TagAutocomplete   \n",
       "174                        0xDEADBEA7/DialoGPT-small-rick   \n",
       "175                        0xDEADBEA7/DialoGPT-small-rick   \n",
       "267     1-800-BAD-CODE/punctuation_fullstop_truecase_e...   \n",
       "269     1-800-BAD-CODE/sentence_boundary_detection_mul...   \n",
       "...                                                   ...   \n",
       "323742  zayedupal/movie-genre-prediction_distilbert-ba...   \n",
       "323901            zekun-li/geolm-base-toponym-recognition   \n",
       "323991                              zenham/khemx_m_e4_16h   \n",
       "323992                              zenham/khemx_m_e4_16h   \n",
       "325107                         zjunlp/zhixi-13b-diff-fp16   \n",
       "\n",
       "                                               limitation  \\\n",
       "76      Since DistilGPT2 is a distilled version of GPT...   \n",
       "174     The model may reflect biases present in the da...   \n",
       "175     The model may reflect biases present in the da...   \n",
       "267     This model was trained on news data, and may n...   \n",
       "269     This model was trained on `OpenSubtitles`, dat...   \n",
       "...                                                   ...   \n",
       "323742                            More information needed   \n",
       "323901  Significant research has explored bias and fai...   \n",
       "323991  The model may inherit biases from the training...   \n",
       "323992  The model may inherit biases from the training...   \n",
       "325107  Due to time constraints, hardware limitations,...   \n",
       "\n",
       "                  model_task  \n",
       "76           text-generation  \n",
       "174           conversational  \n",
       "175          text-generation  \n",
       "267     text2text-generation  \n",
       "269     token-classification  \n",
       "...                      ...  \n",
       "323742   text-classification  \n",
       "323901  token-classification  \n",
       "323991        conversational  \n",
       "323992       text-generation  \n",
       "325107       text-generation  \n",
       "\n",
       "[3273 rows x 3 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_model_limitation = df_model_discussion[~df_model_discussion['limitation'].isnull()]\\\n",
    "                        [['context_id','limitation','model_task']].drop_duplicates()\n",
    "df_model_limitation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d59d2ea8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Since DistilGPT2 is a distilled version of GPT-2, it is intended to be used for similar use cases with the increased functionality of being smaller and easier to run than the base model. The developers of GPT-2 state in their model card that they envisioned GPT-2 would be used by researchers to better understand large-scale generative language models, with possible secondary use cases including writing assistance, creative writing and art, and entertainment. However, because large-scale language models like GPT-2 do not distinguish fact from fiction, they are not recommended for use-cases that require the generated text to be true. Additionally, language models like GPT-2 reflect the biases inherent to the systems they were trained on, so they should not be deployed into systems that interact with humans unless the deployers first carry out a study of biases relevant to the intended use-case.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_model_limitation['limitation'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9d1547b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Will the following description cause an issue if I want to use a prompt of 10000 words?\n",
      "Description: Since DistilGPT2 is a distilled version of GPT-2, it is intended to be used for similar use cases with the increased functionality of being smaller and easier to run than the base model. The developers of GPT-2 state in their model card that they envisioned GPT-2 would be used by researchers to better understand large-scale generative language models, with possible secondary use cases including writing assistance, creative writing and art, and entertainment. However, because large-scale language models like GPT-2 do not distinguish fact from fiction, they are not recommended for use-cases that require the generated text to be true. Additionally, language models like GPT-2 reflect the biases inherent to the systems they were trained on, so they should not be deployed into systems that interact with humans unless the deployers first carry out a study of biases relevant to the intended use-case. The model can only take input less than 11000 tokens.\n",
      "Response (Yes/No/insufficient information):\n",
      "\n"
     ]
    }
   ],
   "source": [
    "comment_id = 0\n",
    "task = \"I want to use a prompt of 10000 words\"\n",
    "\n",
    "prompt = \"\"\"\n",
    "Will the following description cause an issue if {0}?\n",
    "Description: {1} The model can only take input less than 11000 tokens.\n",
    "Response (Yes/No/insufficient information):\n",
    "\"\"\".format(task.strip(), df_model_limitation['limitation'].iloc[comment_id].strip())\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aa4b98a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Will the following description cause an issue if I want to use a prompt of 10000 words?\n",
      "Description: Since DistilGPT2 is a distilled version of GPT-2, it is intended to be used for similar use cases with the increased functionality of being smaller and easier to run than the base model. The developers of GPT-2 state in their model card that they envisioned GPT-2 would be used by researchers to better understand large-scale generative language models, with possible secondary use cases including writing assistance, creative writing and art, and entertainment. However, because large-scale language models like GPT-2 do not distinguish fact from fiction, they are not recommended for use-cases that require the generated text to be true. Additionally, language models like GPT-2 reflect the biases inherent to the systems they were trained on, so they should not be deployed into systems that interact with humans unless the deployers first carry out a study of biases relevant to the intended use-case. The model can only take input less than 11000 tokens.\n",
      "Response (Yes/No/insufficient information):\n",
      "Yes\n",
      "\n",
      "CPU times: total: 7.02 s\n",
      "Wall time: 20.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "input_ids = mistral_tokenizer([prompt], return_tensors=\"pt\").input_ids.to(device)\n",
    "with torch.no_grad():\n",
    "    generated_ids = mistral_model.generate(input_ids, \n",
    "                                      do_sample=True,\n",
    "                                      temperature=0.1,\n",
    "                                      top_k=3,\n",
    "                                      top_p=3,\n",
    "                                      max_new_tokens=2)\n",
    "    generated_text = mistral_tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "957cb12c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Will the following description cause an issue if I want to use a prompt of 10000 words?\n",
      "Description: Since DistilGPT2 is a distilled version of GPT-2, it is intended to be used for similar use cases with the increased functionality of being smaller and easier to run than the base model. The developers of GPT-2 state in their model card that they envisioned GPT-2 would be used by researchers to better understand large-scale generative language models, with possible secondary use cases including writing assistance, creative writing and art, and entertainment. However, because large-scale language models like GPT-2 do not distinguish fact from fiction, they are not recommended for use-cases that require the generated text to be true. Additionally, language models like GPT-2 reflect the biases inherent to the systems they were trained on, so they should not be deployed into systems that interact with humans unless the deployers first carry out a study of biases relevant to the intended use-case. The model can only take input less than 8000 tokens.\n",
      "Response (Yes/No/insufficient information):\n",
      "\n"
     ]
    }
   ],
   "source": [
    "comment_id = 0\n",
    "task = \"I want to use a prompt of 10000 words\"\n",
    "\n",
    "prompt = \"\"\"\n",
    "Will the following description cause an issue if {0}?\n",
    "Description: {1} The model can only take input less than 8000 tokens.\n",
    "Response (Yes/No/insufficient information):\n",
    "\"\"\".format(task.strip(), df_model_limitation['limitation'].iloc[comment_id].strip())\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8264a7c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Will the following description cause an issue if I want to use a prompt of 10000 words?\n",
      "Description: Since DistilGPT2 is a distilled version of GPT-2, it is intended to be used for similar use cases with the increased functionality of being smaller and easier to run than the base model. The developers of GPT-2 state in their model card that they envisioned GPT-2 would be used by researchers to better understand large-scale generative language models, with possible secondary use cases including writing assistance, creative writing and art, and entertainment. However, because large-scale language models like GPT-2 do not distinguish fact from fiction, they are not recommended for use-cases that require the generated text to be true. Additionally, language models like GPT-2 reflect the biases inherent to the systems they were trained on, so they should not be deployed into systems that interact with humans unless the deployers first carry out a study of biases relevant to the intended use-case. The model can only take input less than 8000 tokens.\n",
      "Response (Yes/No/insufficient information):\n",
      "\n",
      "Yes\n",
      "CPU times: total: 3.33 s\n",
      "Wall time: 9.56 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "input_ids = mistral_tokenizer([prompt], return_tensors=\"pt\").input_ids.to(device)\n",
    "with torch.no_grad():\n",
    "    generated_ids = mistral_model.generate(input_ids, \n",
    "                                      do_sample=True,\n",
    "                                      temperature=0.1,\n",
    "                                      top_k=3,\n",
    "                                      top_p=3,\n",
    "                                      max_new_tokens=2)\n",
    "    generated_text = mistral_tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e8ae959a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context_id</th>\n",
       "      <th>test_metric</th>\n",
       "      <th>test_result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0-hero/led-large-legal-summary</td>\n",
       "      <td>Gen Len</td>\n",
       "      <td>27.633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0-hero/led-large-legal-summary</td>\n",
       "      <td>Loss</td>\n",
       "      <td>2.098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0-hero/led-large-legal-summary</td>\n",
       "      <td>Rouge1</td>\n",
       "      <td>36.855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0-hero/led-large-legal-summary</td>\n",
       "      <td>Rouge2</td>\n",
       "      <td>22.050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0-hero/led-large-legal-summary</td>\n",
       "      <td>RougeL</td>\n",
       "      <td>33.547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0-hero/led-large-legal-summary</td>\n",
       "      <td>RougeLsum</td>\n",
       "      <td>34.607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>09panesara/distilbert-base-uncased-finetuned-cola</td>\n",
       "      <td>Matthews Correlation</td>\n",
       "      <td>0.5406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>0x7194633/roberta-base-spam-detector</td>\n",
       "      <td>eval_accuracy</td>\n",
       "      <td>0.9979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>0x7194633/roberta-base-spam-detector</td>\n",
       "      <td>eval_f1</td>\n",
       "      <td>0.9980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>0x7194633/roberta-base-spam-detector</td>\n",
       "      <td>eval_loss</td>\n",
       "      <td>0.0211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>0x7194633/roberta-base-spam-detector</td>\n",
       "      <td>eval_precision</td>\n",
       "      <td>0.9960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>0x7194633/roberta-base-spam-detector</td>\n",
       "      <td>eval_recall</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>772</th>\n",
       "      <td>2rtl3/mn-bert-base-demo-named-entity</td>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.9757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>773</th>\n",
       "      <td>2rtl3/mn-bert-base-demo-named-entity</td>\n",
       "      <td>F1</td>\n",
       "      <td>0.9139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>774</th>\n",
       "      <td>2rtl3/mn-bert-base-demo-named-entity</td>\n",
       "      <td>Loss</td>\n",
       "      <td>0.1468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>775</th>\n",
       "      <td>2rtl3/mn-bert-base-demo-named-entity</td>\n",
       "      <td>Precision</td>\n",
       "      <td>0.9092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>776</th>\n",
       "      <td>2rtl3/mn-bert-base-demo-named-entity</td>\n",
       "      <td>Recall</td>\n",
       "      <td>0.9187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1544</th>\n",
       "      <td>A-bhimany-u08/bert-base-cased-qqp</td>\n",
       "      <td>accuracy</td>\n",
       "      <td>89%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1691</th>\n",
       "      <td>AG6019/reddit-comment-sentiment-final</td>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.8971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1692</th>\n",
       "      <td>AG6019/reddit-comment-sentiment-final</td>\n",
       "      <td>Loss</td>\n",
       "      <td>0.2564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2069</th>\n",
       "      <td>AK270802/DialoGPT-small-harrypotter</td>\n",
       "      <td>Perplexity</td>\n",
       "      <td>N/A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2374</th>\n",
       "      <td>ARTeLab/it5-summarization-mlsum</td>\n",
       "      <td>Gen Len</td>\n",
       "      <td>32.5268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2376</th>\n",
       "      <td>ARTeLab/it5-summarization-mlsum</td>\n",
       "      <td>Loss</td>\n",
       "      <td>2.0190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2378</th>\n",
       "      <td>ARTeLab/it5-summarization-mlsum</td>\n",
       "      <td>Rouge1</td>\n",
       "      <td>19.3739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2380</th>\n",
       "      <td>ARTeLab/it5-summarization-mlsum</td>\n",
       "      <td>Rouge2</td>\n",
       "      <td>5.9753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2382</th>\n",
       "      <td>ARTeLab/it5-summarization-mlsum</td>\n",
       "      <td>Rougel</td>\n",
       "      <td>16.691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2384</th>\n",
       "      <td>ARTeLab/it5-summarization-mlsum</td>\n",
       "      <td>Rougelsum</td>\n",
       "      <td>16.7862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2410</th>\n",
       "      <td>ARTeLab/mbart-summarization-fanpage</td>\n",
       "      <td>Gen Len</td>\n",
       "      <td>75.2413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2412</th>\n",
       "      <td>ARTeLab/mbart-summarization-fanpage</td>\n",
       "      <td>Loss</td>\n",
       "      <td>2.1833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2414</th>\n",
       "      <td>ARTeLab/mbart-summarization-fanpage</td>\n",
       "      <td>Rouge1</td>\n",
       "      <td>36.5027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2416</th>\n",
       "      <td>ARTeLab/mbart-summarization-fanpage</td>\n",
       "      <td>Rouge2</td>\n",
       "      <td>17.4428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2418</th>\n",
       "      <td>ARTeLab/mbart-summarization-fanpage</td>\n",
       "      <td>Rougel</td>\n",
       "      <td>26.1734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2420</th>\n",
       "      <td>ARTeLab/mbart-summarization-fanpage</td>\n",
       "      <td>Rougelsum</td>\n",
       "      <td>30.2636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2423</th>\n",
       "      <td>ARTeLab/mbart-summarization-mlsum</td>\n",
       "      <td>Gen Len</td>\n",
       "      <td>33.5945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2425</th>\n",
       "      <td>ARTeLab/mbart-summarization-mlsum</td>\n",
       "      <td>Loss</td>\n",
       "      <td>3.3336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2427</th>\n",
       "      <td>ARTeLab/mbart-summarization-mlsum</td>\n",
       "      <td>Rouge1</td>\n",
       "      <td>19.3489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2429</th>\n",
       "      <td>ARTeLab/mbart-summarization-mlsum</td>\n",
       "      <td>Rouge2</td>\n",
       "      <td>6.4028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2431</th>\n",
       "      <td>ARTeLab/mbart-summarization-mlsum</td>\n",
       "      <td>Rougel</td>\n",
       "      <td>16.3497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2433</th>\n",
       "      <td>ARTeLab/mbart-summarization-mlsum</td>\n",
       "      <td>Rougelsum</td>\n",
       "      <td>16.5387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2460</th>\n",
       "      <td>ASCCCCCCCC/distilbert-base-chinese-amazon_zh_2...</td>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.5092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2461</th>\n",
       "      <td>ASCCCCCCCC/distilbert-base-chinese-amazon_zh_2...</td>\n",
       "      <td>Loss</td>\n",
       "      <td>1.1518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2462</th>\n",
       "      <td>ASCCCCCCCC/distilbert-base-multilingual-cased-...</td>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.4406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2463</th>\n",
       "      <td>ASCCCCCCCC/distilbert-base-multilingual-cased-...</td>\n",
       "      <td>Loss</td>\n",
       "      <td>1.3031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2609</th>\n",
       "      <td>Aalaa/opt-125m-wikitext2</td>\n",
       "      <td>Loss</td>\n",
       "      <td>3.3409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2770</th>\n",
       "      <td>Abderrahim2/bert-finetuned-gender_classification</td>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2771</th>\n",
       "      <td>Abderrahim2/bert-finetuned-gender_classification</td>\n",
       "      <td>F1</td>\n",
       "      <td>0.9645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2772</th>\n",
       "      <td>Abderrahim2/bert-finetuned-gender_classification</td>\n",
       "      <td>Loss</td>\n",
       "      <td>0.1484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2773</th>\n",
       "      <td>Abderrahim2/bert-finetuned-gender_classification</td>\n",
       "      <td>Roc Auc</td>\n",
       "      <td>0.9732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2830</th>\n",
       "      <td>Abdulkader/autotrain-medical-reports-summarize...</td>\n",
       "      <td>Gen Len</td>\n",
       "      <td>8.930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2831</th>\n",
       "      <td>Abdulkader/autotrain-medical-reports-summarize...</td>\n",
       "      <td>Loss</td>\n",
       "      <td>1.728</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             context_id           test_metric  \\\n",
       "12                       0-hero/led-large-legal-summary               Gen Len   \n",
       "13                       0-hero/led-large-legal-summary                  Loss   \n",
       "14                       0-hero/led-large-legal-summary                Rouge1   \n",
       "15                       0-hero/led-large-legal-summary                Rouge2   \n",
       "16                       0-hero/led-large-legal-summary                RougeL   \n",
       "17                       0-hero/led-large-legal-summary             RougeLsum   \n",
       "63    09panesara/distilbert-base-uncased-finetuned-cola  Matthews Correlation   \n",
       "161                0x7194633/roberta-base-spam-detector         eval_accuracy   \n",
       "162                0x7194633/roberta-base-spam-detector               eval_f1   \n",
       "163                0x7194633/roberta-base-spam-detector             eval_loss   \n",
       "164                0x7194633/roberta-base-spam-detector        eval_precision   \n",
       "165                0x7194633/roberta-base-spam-detector           eval_recall   \n",
       "772                2rtl3/mn-bert-base-demo-named-entity              Accuracy   \n",
       "773                2rtl3/mn-bert-base-demo-named-entity                    F1   \n",
       "774                2rtl3/mn-bert-base-demo-named-entity                  Loss   \n",
       "775                2rtl3/mn-bert-base-demo-named-entity             Precision   \n",
       "776                2rtl3/mn-bert-base-demo-named-entity                Recall   \n",
       "1544                  A-bhimany-u08/bert-base-cased-qqp              accuracy   \n",
       "1691              AG6019/reddit-comment-sentiment-final              Accuracy   \n",
       "1692              AG6019/reddit-comment-sentiment-final                  Loss   \n",
       "2069                AK270802/DialoGPT-small-harrypotter            Perplexity   \n",
       "2374                    ARTeLab/it5-summarization-mlsum               Gen Len   \n",
       "2376                    ARTeLab/it5-summarization-mlsum                  Loss   \n",
       "2378                    ARTeLab/it5-summarization-mlsum                Rouge1   \n",
       "2380                    ARTeLab/it5-summarization-mlsum                Rouge2   \n",
       "2382                    ARTeLab/it5-summarization-mlsum                Rougel   \n",
       "2384                    ARTeLab/it5-summarization-mlsum             Rougelsum   \n",
       "2410                ARTeLab/mbart-summarization-fanpage               Gen Len   \n",
       "2412                ARTeLab/mbart-summarization-fanpage                  Loss   \n",
       "2414                ARTeLab/mbart-summarization-fanpage                Rouge1   \n",
       "2416                ARTeLab/mbart-summarization-fanpage                Rouge2   \n",
       "2418                ARTeLab/mbart-summarization-fanpage                Rougel   \n",
       "2420                ARTeLab/mbart-summarization-fanpage             Rougelsum   \n",
       "2423                  ARTeLab/mbart-summarization-mlsum               Gen Len   \n",
       "2425                  ARTeLab/mbart-summarization-mlsum                  Loss   \n",
       "2427                  ARTeLab/mbart-summarization-mlsum                Rouge1   \n",
       "2429                  ARTeLab/mbart-summarization-mlsum                Rouge2   \n",
       "2431                  ARTeLab/mbart-summarization-mlsum                Rougel   \n",
       "2433                  ARTeLab/mbart-summarization-mlsum             Rougelsum   \n",
       "2460  ASCCCCCCCC/distilbert-base-chinese-amazon_zh_2...              Accuracy   \n",
       "2461  ASCCCCCCCC/distilbert-base-chinese-amazon_zh_2...                  Loss   \n",
       "2462  ASCCCCCCCC/distilbert-base-multilingual-cased-...              Accuracy   \n",
       "2463  ASCCCCCCCC/distilbert-base-multilingual-cased-...                  Loss   \n",
       "2609                           Aalaa/opt-125m-wikitext2                  Loss   \n",
       "2770   Abderrahim2/bert-finetuned-gender_classification              Accuracy   \n",
       "2771   Abderrahim2/bert-finetuned-gender_classification                    F1   \n",
       "2772   Abderrahim2/bert-finetuned-gender_classification                  Loss   \n",
       "2773   Abderrahim2/bert-finetuned-gender_classification               Roc Auc   \n",
       "2830  Abdulkader/autotrain-medical-reports-summarize...               Gen Len   \n",
       "2831  Abdulkader/autotrain-medical-reports-summarize...                  Loss   \n",
       "\n",
       "     test_result  \n",
       "12        27.633  \n",
       "13         2.098  \n",
       "14        36.855  \n",
       "15        22.050  \n",
       "16        33.547  \n",
       "17        34.607  \n",
       "63        0.5406  \n",
       "161       0.9979  \n",
       "162       0.9980  \n",
       "163       0.0211  \n",
       "164       0.9960  \n",
       "165          1.0  \n",
       "772       0.9757  \n",
       "773       0.9139  \n",
       "774       0.1468  \n",
       "775       0.9092  \n",
       "776       0.9187  \n",
       "1544         89%  \n",
       "1691      0.8971  \n",
       "1692      0.2564  \n",
       "2069         N/A  \n",
       "2374     32.5268  \n",
       "2376      2.0190  \n",
       "2378     19.3739  \n",
       "2380      5.9753  \n",
       "2382      16.691  \n",
       "2384     16.7862  \n",
       "2410     75.2413  \n",
       "2412      2.1833  \n",
       "2414     36.5027  \n",
       "2416     17.4428  \n",
       "2418     26.1734  \n",
       "2420     30.2636  \n",
       "2423     33.5945  \n",
       "2425      3.3336  \n",
       "2427     19.3489  \n",
       "2429      6.4028  \n",
       "2431     16.3497  \n",
       "2433     16.5387  \n",
       "2460      0.5092  \n",
       "2461      1.1518  \n",
       "2462      0.4406  \n",
       "2463      1.3031  \n",
       "2609      3.3409  \n",
       "2770       0.964  \n",
       "2771      0.9645  \n",
       "2772      0.1484  \n",
       "2773      0.9732  \n",
       "2830       8.930  \n",
       "2831       1.728  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_model_discussion[~df_model_discussion['test_result'].isnull()]\\\n",
    "[['context_id','test_metric','test_result']].drop_duplicates().iloc[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5b8e614f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context_id</th>\n",
       "      <th>downloads</th>\n",
       "      <th>limitation</th>\n",
       "      <th>discussion_title</th>\n",
       "      <th>hyperparam</th>\n",
       "      <th>test_metric</th>\n",
       "      <th>test_result</th>\n",
       "      <th>paper_title</th>\n",
       "      <th>model_task</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0-hero/flan-OIG-base</td>\n",
       "      <td>1.0</td>\n",
       "      <td>None</td>\n",
       "      <td>Adding `safetensors` variant of this model</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0-hero/flan-OIG-base</td>\n",
       "      <td>1.0</td>\n",
       "      <td>None</td>\n",
       "      <td>Model</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0-hero/flan-OIG-small</td>\n",
       "      <td>1.0</td>\n",
       "      <td>None</td>\n",
       "      <td>Adding `safetensors` variant of this model</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0-hero/flan-OIG-ul2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0-hero/flan-OIG-xl</td>\n",
       "      <td>1.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325858</th>\n",
       "      <td>zzzzzy/ttttp</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325859</th>\n",
       "      <td>zzzzzz1q/z</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325860</th>\n",
       "      <td>zzzzzzttt/swin-tiny-patch4-window7-224-finetun...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>None</td>\n",
       "      <td>Adding `safetensors` variant of this model</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325861</th>\n",
       "      <td>zzzzzzttt/vit-base-patch16-224-finetuned-eurosat</td>\n",
       "      <td>8.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325862</th>\n",
       "      <td>zzzzzzttt/vit-base-patch16-224-in21k-finetuned...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>325857 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               context_id  downloads  \\\n",
       "6                                    0-hero/flan-OIG-base        1.0   \n",
       "7                                    0-hero/flan-OIG-base        1.0   \n",
       "8                                   0-hero/flan-OIG-small        1.0   \n",
       "9                                     0-hero/flan-OIG-ul2        1.0   \n",
       "10                                     0-hero/flan-OIG-xl        1.0   \n",
       "...                                                   ...        ...   \n",
       "325858                                       zzzzzy/ttttp        0.0   \n",
       "325859                                         zzzzzz1q/z        0.0   \n",
       "325860  zzzzzzttt/swin-tiny-patch4-window7-224-finetun...        8.0   \n",
       "325861   zzzzzzttt/vit-base-patch16-224-finetuned-eurosat        8.0   \n",
       "325862  zzzzzzttt/vit-base-patch16-224-in21k-finetuned...        0.0   \n",
       "\n",
       "       limitation                            discussion_title hyperparam  \\\n",
       "6            None  Adding `safetensors` variant of this model       None   \n",
       "7            None                                       Model       None   \n",
       "8            None  Adding `safetensors` variant of this model       None   \n",
       "9            None                                        None       None   \n",
       "10           None                                        None       None   \n",
       "...           ...                                         ...        ...   \n",
       "325858       None                                        None       None   \n",
       "325859       None                                        None       None   \n",
       "325860       None  Adding `safetensors` variant of this model       None   \n",
       "325861       None                                        None       None   \n",
       "325862       None                                        None       None   \n",
       "\n",
       "       test_metric test_result paper_title model_task  \n",
       "6             None        None        None       None  \n",
       "7             None        None        None       None  \n",
       "8             None        None        None       None  \n",
       "9             None        None        None       None  \n",
       "10            None        None        None       None  \n",
       "...            ...         ...         ...        ...  \n",
       "325858        None        None        None       None  \n",
       "325859        None        None        None       None  \n",
       "325860        None        None        None       None  \n",
       "325861        None        None        None       None  \n",
       "325862        None        None        None       None  \n",
       "\n",
       "[325857 rows x 9 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_model_discussion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "91f335f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context_id</th>\n",
       "      <th>limitation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>0Tick/e621TagAutocomplete</td>\n",
       "      <td>Since DistilGPT2 is a distilled version of GPT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0Tick/e621TagAutocomplete</td>\n",
       "      <td>Since DistilGPT2 is a distilled version of GPT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>0xDEADBEA7/DialoGPT-small-rick</td>\n",
       "      <td>The model may reflect biases present in the da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>0xDEADBEA7/DialoGPT-small-rick</td>\n",
       "      <td>The model may reflect biases present in the da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>1-800-BAD-CODE/punctuation_fullstop_truecase_e...</td>\n",
       "      <td>This model was trained on news data, and may n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323742</th>\n",
       "      <td>zayedupal/movie-genre-prediction_distilbert-ba...</td>\n",
       "      <td>More information needed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323901</th>\n",
       "      <td>zekun-li/geolm-base-toponym-recognition</td>\n",
       "      <td>Significant research has explored bias and fai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323991</th>\n",
       "      <td>zenham/khemx_m_e4_16h</td>\n",
       "      <td>The model may inherit biases from the training...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323992</th>\n",
       "      <td>zenham/khemx_m_e4_16h</td>\n",
       "      <td>The model may inherit biases from the training...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325107</th>\n",
       "      <td>zjunlp/zhixi-13b-diff-fp16</td>\n",
       "      <td>Due to time constraints, hardware limitations,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13350 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               context_id  \\\n",
       "76                              0Tick/e621TagAutocomplete   \n",
       "77                              0Tick/e621TagAutocomplete   \n",
       "174                        0xDEADBEA7/DialoGPT-small-rick   \n",
       "175                        0xDEADBEA7/DialoGPT-small-rick   \n",
       "267     1-800-BAD-CODE/punctuation_fullstop_truecase_e...   \n",
       "...                                                   ...   \n",
       "323742  zayedupal/movie-genre-prediction_distilbert-ba...   \n",
       "323901            zekun-li/geolm-base-toponym-recognition   \n",
       "323991                              zenham/khemx_m_e4_16h   \n",
       "323992                              zenham/khemx_m_e4_16h   \n",
       "325107                         zjunlp/zhixi-13b-diff-fp16   \n",
       "\n",
       "                                               limitation  \n",
       "76      Since DistilGPT2 is a distilled version of GPT...  \n",
       "77      Since DistilGPT2 is a distilled version of GPT...  \n",
       "174     The model may reflect biases present in the da...  \n",
       "175     The model may reflect biases present in the da...  \n",
       "267     This model was trained on news data, and may n...  \n",
       "...                                                   ...  \n",
       "323742                            More information needed  \n",
       "323901  Significant research has explored bias and fai...  \n",
       "323991  The model may inherit biases from the training...  \n",
       "323992  The model may inherit biases from the training...  \n",
       "325107  Due to time constraints, hardware limitations,...  \n",
       "\n",
       "[13350 rows x 2 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_toy = df_model_discussion[~df_model_discussion['limitation'].isnull()][['context_id','limitation']]\n",
    "df_toy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLMpy38",
   "language": "python",
   "name": "llmpy38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
