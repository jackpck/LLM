{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "40436b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface_hub import hf_hub_download \n",
    "import torch\n",
    "from datasets import load_dataset, load_from_disk\n",
    "import evaluate\n",
    "import transformers\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSequenceClassification, AutoModel\n",
    "from transformers.modeling_outputs import TokenClassifierOutput\n",
    "from transformers import TrainingArguments, Trainer, AdamW, get_scheduler, EarlyStoppingCallback\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "from peft import PeftConfig, PeftModel\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf0b4e50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.30.2\n"
     ]
    }
   ],
   "source": [
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1bd8c0da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.0+cu116\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3ac37c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.1.0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import peft\n",
    "peft.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2fa258e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2022 NVIDIA Corporation\n",
      "Built on Wed_Sep_21_10:41:10_Pacific_Daylight_Time_2022\n",
      "Cuda compilation tools, release 11.8, V11.8.89\n",
      "Build cuda_11.8.r11.8/compiler.31833905_0\n"
     ]
    }
   ],
   "source": [
    "! nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05ef7bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del model\n",
    "# del trainer\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2267ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )\n",
    "\n",
    "def load_llm(model_path, num_labels, seed=123):\n",
    "    \"\"\"\n",
    "    run this for different experiments (freezing different params)\n",
    "    \"\"\"\n",
    "    if not os.path.isfile(model_path + '/model.safetensors'):\n",
    "        return 'model does not exist. Create model first'\n",
    "    \n",
    "    torch.manual_seed(seed)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_path, \n",
    "                                                           num_labels=num_labels, \n",
    "                                                           cache_dir=cache_dir, \n",
    "                                                           local_files_only=True)\n",
    "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    print(device)\n",
    "    model.to(device) # use GPU\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8685b687",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_repo_dir = 'D:/projects/LLM'\n",
    "cache_dir = '/cygdrive/d/projects/LLM/.cache'\n",
    "os.environ['TRANSFORMERS_CACHE'] = cache_dir\n",
    "os.environ['HF_HOME'] = cache_dir + '/huggingface'\n",
    "os.environ['XDG_CACHE_HOME'] = cache_dir\n",
    "os.environ['HF_DATASETS_CACHE'] = cache_dir\n",
    "\n",
    "model_path = cache_dir + '/models--google-bert--bert-base-cased/snapshots/cd5ef92a9fb2f889e972770a36d4ed042daf221e'\n",
    "dataset_path = cache_dir + '/parquet/yelp_polarity' # cache_dir + '/parquet/yelp_review_full-e22176106d6e7534'\n",
    "dataset_name = 'yelp_polarity' # yelp_review_full\n",
    "tokenized_data_path = cache_dir + '/tokenized_dataset_yelp_polarity'\n",
    "num_labels = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb8f5300",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedTrainer(Trainer):    \n",
    "    def __init__(self, weights, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        DEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "        self.weights = torch.tensor(weights, dtype=torch.float32).to(DEVICE)\n",
    "        \n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.get('labels')\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs['logits']\n",
    "        loss = nn.CrossEntropyLoss(weight=self.weights)(logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a6f1a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# peft.peft_model.PeftModel\n",
    "# peft.peft_model.PeftModelForSequenceClassification\n",
    "class ModelPeftMLP(peft.peft_model.PeftModel):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_labels = 2\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(768,32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.Linear(32,16),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.Linear(16,8),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.Linear(8, num_labels)\n",
    "        )\n",
    "              \n",
    "    def forward(self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "        task_ids=None,\n",
    "        **kwargs,):\n",
    "        outputs = self.model(input_ids=input_ids, \n",
    "                             attention_mask=attention_mask,\n",
    "                            )\n",
    "        logits = self.mlp(outputs['last_hidden_state'][:,0,:].view(-1, 768))\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            return TokenClassifierOutput(loss=loss,\n",
    "                                         logits=logits,\n",
    "                                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2af1c057",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optuna_hp_space(trial):\n",
    "    return {\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-6, 1e-4, log=True),\n",
    "        \"per_device_train_batch_size\": trial.suggest_categorical(\"per_device_train_batch_size\", [8,16,32,64]),\n",
    "        \"warmup_steps\":trial.suggest_categorical(\"warmup_steps\", [0,10,100])\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d7dab318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert clf exists. Load from local file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /cygdrive/d/projects/LLM/.cache/models--google-bert--bert-base-cased/snapshots/cd5ef92a9fb2f889e972770a36d4ed042daf221e were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.bert.modeling_bert.BertModel'>\n"
     ]
    }
   ],
   "source": [
    "if not os.path.isfile(model_path + '/pytorch_model.bin'):\n",
    "    print('bert clf does not exist. Download model')\n",
    "    model = AutoModel.from_pretrained(\"google-bert/bert-base-cased\", \n",
    "                                      cache_dir=cache_dir,\n",
    "                                      output_attentions=True,\n",
    "                                      output_hidden_states=True)\n",
    "else:\n",
    "    print('bert clf exists. Load from local file')\n",
    "    model = AutoModel.from_pretrained(model_path,  \n",
    "                                      cache_dir=cache_dir, \n",
    "                                      local_files_only=True,\n",
    "                                      output_attentions=True,\n",
    "                                      output_hidden_states=True)\n",
    "    \n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device) # use GPU\n",
    "print(type(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d540d99d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelPeftMLP(\n",
      "  (base_model): LoraModel(\n",
      "    (model): PeftModel(\n",
      "      (base_model): LoraModel(\n",
      "        (model): BertModel(\n",
      "          (embeddings): BertEmbeddings(\n",
      "            (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
      "            (position_embeddings): Embedding(512, 768)\n",
      "            (token_type_embeddings): Embedding(2, 768)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (encoder): BertEncoder(\n",
      "            (layer): ModuleList(\n",
      "              (0): BertLayer(\n",
      "                (attention): BertAttention(\n",
      "                  (self): BertSelfAttention(\n",
      "                    (query): Linear(\n",
      "                      in_features=768, out_features=768, bias=True\n",
      "                      (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      "                      (lora_A): Linear(in_features=768, out_features=32, bias=False)\n",
      "                      (lora_B): Linear(in_features=32, out_features=768, bias=False)\n",
      "                    )\n",
      "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (value): Linear(\n",
      "                      in_features=768, out_features=768, bias=True\n",
      "                      (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      "                      (lora_A): Linear(in_features=768, out_features=32, bias=False)\n",
      "                      (lora_B): Linear(in_features=32, out_features=768, bias=False)\n",
      "                    )\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (output): BertSelfOutput(\n",
      "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (intermediate): BertIntermediate(\n",
      "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                  (intermediate_act_fn): GELUActivation()\n",
      "                )\n",
      "                (output): BertOutput(\n",
      "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (1): BertLayer(\n",
      "                (attention): BertAttention(\n",
      "                  (self): BertSelfAttention(\n",
      "                    (query): Linear(\n",
      "                      in_features=768, out_features=768, bias=True\n",
      "                      (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      "                      (lora_A): Linear(in_features=768, out_features=32, bias=False)\n",
      "                      (lora_B): Linear(in_features=32, out_features=768, bias=False)\n",
      "                    )\n",
      "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (value): Linear(\n",
      "                      in_features=768, out_features=768, bias=True\n",
      "                      (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      "                      (lora_A): Linear(in_features=768, out_features=32, bias=False)\n",
      "                      (lora_B): Linear(in_features=32, out_features=768, bias=False)\n",
      "                    )\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (output): BertSelfOutput(\n",
      "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (intermediate): BertIntermediate(\n",
      "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                  (intermediate_act_fn): GELUActivation()\n",
      "                )\n",
      "                (output): BertOutput(\n",
      "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (2): BertLayer(\n",
      "                (attention): BertAttention(\n",
      "                  (self): BertSelfAttention(\n",
      "                    (query): Linear(\n",
      "                      in_features=768, out_features=768, bias=True\n",
      "                      (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      "                      (lora_A): Linear(in_features=768, out_features=32, bias=False)\n",
      "                      (lora_B): Linear(in_features=32, out_features=768, bias=False)\n",
      "                    )\n",
      "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (value): Linear(\n",
      "                      in_features=768, out_features=768, bias=True\n",
      "                      (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      "                      (lora_A): Linear(in_features=768, out_features=32, bias=False)\n",
      "                      (lora_B): Linear(in_features=32, out_features=768, bias=False)\n",
      "                    )\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (output): BertSelfOutput(\n",
      "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (intermediate): BertIntermediate(\n",
      "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                  (intermediate_act_fn): GELUActivation()\n",
      "                )\n",
      "                (output): BertOutput(\n",
      "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (3): BertLayer(\n",
      "                (attention): BertAttention(\n",
      "                  (self): BertSelfAttention(\n",
      "                    (query): Linear(\n",
      "                      in_features=768, out_features=768, bias=True\n",
      "                      (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      "                      (lora_A): Linear(in_features=768, out_features=32, bias=False)\n",
      "                      (lora_B): Linear(in_features=32, out_features=768, bias=False)\n",
      "                    )\n",
      "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (value): Linear(\n",
      "                      in_features=768, out_features=768, bias=True\n",
      "                      (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      "                      (lora_A): Linear(in_features=768, out_features=32, bias=False)\n",
      "                      (lora_B): Linear(in_features=32, out_features=768, bias=False)\n",
      "                    )\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (output): BertSelfOutput(\n",
      "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (intermediate): BertIntermediate(\n",
      "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                  (intermediate_act_fn): GELUActivation()\n",
      "                )\n",
      "                (output): BertOutput(\n",
      "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (4): BertLayer(\n",
      "                (attention): BertAttention(\n",
      "                  (self): BertSelfAttention(\n",
      "                    (query): Linear(\n",
      "                      in_features=768, out_features=768, bias=True\n",
      "                      (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      "                      (lora_A): Linear(in_features=768, out_features=32, bias=False)\n",
      "                      (lora_B): Linear(in_features=32, out_features=768, bias=False)\n",
      "                    )\n",
      "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (value): Linear(\n",
      "                      in_features=768, out_features=768, bias=True\n",
      "                      (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      "                      (lora_A): Linear(in_features=768, out_features=32, bias=False)\n",
      "                      (lora_B): Linear(in_features=32, out_features=768, bias=False)\n",
      "                    )\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (output): BertSelfOutput(\n",
      "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (intermediate): BertIntermediate(\n",
      "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                  (intermediate_act_fn): GELUActivation()\n",
      "                )\n",
      "                (output): BertOutput(\n",
      "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (5): BertLayer(\n",
      "                (attention): BertAttention(\n",
      "                  (self): BertSelfAttention(\n",
      "                    (query): Linear(\n",
      "                      in_features=768, out_features=768, bias=True\n",
      "                      (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      "                      (lora_A): Linear(in_features=768, out_features=32, bias=False)\n",
      "                      (lora_B): Linear(in_features=32, out_features=768, bias=False)\n",
      "                    )\n",
      "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (value): Linear(\n",
      "                      in_features=768, out_features=768, bias=True\n",
      "                      (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      "                      (lora_A): Linear(in_features=768, out_features=32, bias=False)\n",
      "                      (lora_B): Linear(in_features=32, out_features=768, bias=False)\n",
      "                    )\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (output): BertSelfOutput(\n",
      "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (intermediate): BertIntermediate(\n",
      "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                  (intermediate_act_fn): GELUActivation()\n",
      "                )\n",
      "                (output): BertOutput(\n",
      "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (6): BertLayer(\n",
      "                (attention): BertAttention(\n",
      "                  (self): BertSelfAttention(\n",
      "                    (query): Linear(\n",
      "                      in_features=768, out_features=768, bias=True\n",
      "                      (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      "                      (lora_A): Linear(in_features=768, out_features=32, bias=False)\n",
      "                      (lora_B): Linear(in_features=32, out_features=768, bias=False)\n",
      "                    )\n",
      "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (value): Linear(\n",
      "                      in_features=768, out_features=768, bias=True\n",
      "                      (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      "                      (lora_A): Linear(in_features=768, out_features=32, bias=False)\n",
      "                      (lora_B): Linear(in_features=32, out_features=768, bias=False)\n",
      "                    )\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (output): BertSelfOutput(\n",
      "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (intermediate): BertIntermediate(\n",
      "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                  (intermediate_act_fn): GELUActivation()\n",
      "                )\n",
      "                (output): BertOutput(\n",
      "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (7): BertLayer(\n",
      "                (attention): BertAttention(\n",
      "                  (self): BertSelfAttention(\n",
      "                    (query): Linear(\n",
      "                      in_features=768, out_features=768, bias=True\n",
      "                      (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      "                      (lora_A): Linear(in_features=768, out_features=32, bias=False)\n",
      "                      (lora_B): Linear(in_features=32, out_features=768, bias=False)\n",
      "                    )\n",
      "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (value): Linear(\n",
      "                      in_features=768, out_features=768, bias=True\n",
      "                      (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      "                      (lora_A): Linear(in_features=768, out_features=32, bias=False)\n",
      "                      (lora_B): Linear(in_features=32, out_features=768, bias=False)\n",
      "                    )\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (output): BertSelfOutput(\n",
      "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (intermediate): BertIntermediate(\n",
      "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                  (intermediate_act_fn): GELUActivation()\n",
      "                )\n",
      "                (output): BertOutput(\n",
      "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (8): BertLayer(\n",
      "                (attention): BertAttention(\n",
      "                  (self): BertSelfAttention(\n",
      "                    (query): Linear(\n",
      "                      in_features=768, out_features=768, bias=True\n",
      "                      (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      "                      (lora_A): Linear(in_features=768, out_features=32, bias=False)\n",
      "                      (lora_B): Linear(in_features=32, out_features=768, bias=False)\n",
      "                    )\n",
      "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (value): Linear(\n",
      "                      in_features=768, out_features=768, bias=True\n",
      "                      (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      "                      (lora_A): Linear(in_features=768, out_features=32, bias=False)\n",
      "                      (lora_B): Linear(in_features=32, out_features=768, bias=False)\n",
      "                    )\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (output): BertSelfOutput(\n",
      "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (intermediate): BertIntermediate(\n",
      "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                  (intermediate_act_fn): GELUActivation()\n",
      "                )\n",
      "                (output): BertOutput(\n",
      "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (9): BertLayer(\n",
      "                (attention): BertAttention(\n",
      "                  (self): BertSelfAttention(\n",
      "                    (query): Linear(\n",
      "                      in_features=768, out_features=768, bias=True\n",
      "                      (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      "                      (lora_A): Linear(in_features=768, out_features=32, bias=False)\n",
      "                      (lora_B): Linear(in_features=32, out_features=768, bias=False)\n",
      "                    )\n",
      "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (value): Linear(\n",
      "                      in_features=768, out_features=768, bias=True\n",
      "                      (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      "                      (lora_A): Linear(in_features=768, out_features=32, bias=False)\n",
      "                      (lora_B): Linear(in_features=32, out_features=768, bias=False)\n",
      "                    )\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (output): BertSelfOutput(\n",
      "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (intermediate): BertIntermediate(\n",
      "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                  (intermediate_act_fn): GELUActivation()\n",
      "                )\n",
      "                (output): BertOutput(\n",
      "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (10): BertLayer(\n",
      "                (attention): BertAttention(\n",
      "                  (self): BertSelfAttention(\n",
      "                    (query): Linear(\n",
      "                      in_features=768, out_features=768, bias=True\n",
      "                      (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      "                      (lora_A): Linear(in_features=768, out_features=32, bias=False)\n",
      "                      (lora_B): Linear(in_features=32, out_features=768, bias=False)\n",
      "                    )\n",
      "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (value): Linear(\n",
      "                      in_features=768, out_features=768, bias=True\n",
      "                      (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      "                      (lora_A): Linear(in_features=768, out_features=32, bias=False)\n",
      "                      (lora_B): Linear(in_features=32, out_features=768, bias=False)\n",
      "                    )\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (output): BertSelfOutput(\n",
      "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (intermediate): BertIntermediate(\n",
      "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                  (intermediate_act_fn): GELUActivation()\n",
      "                )\n",
      "                (output): BertOutput(\n",
      "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (11): BertLayer(\n",
      "                (attention): BertAttention(\n",
      "                  (self): BertSelfAttention(\n",
      "                    (query): Linear(\n",
      "                      in_features=768, out_features=768, bias=True\n",
      "                      (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      "                      (lora_A): Linear(in_features=768, out_features=32, bias=False)\n",
      "                      (lora_B): Linear(in_features=32, out_features=768, bias=False)\n",
      "                    )\n",
      "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (value): Linear(\n",
      "                      in_features=768, out_features=768, bias=True\n",
      "                      (lora_dropout): Dropout(p=0.1, inplace=False)\n",
      "                      (lora_A): Linear(in_features=768, out_features=32, bias=False)\n",
      "                      (lora_B): Linear(in_features=32, out_features=768, bias=False)\n",
      "                    )\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (output): BertSelfOutput(\n",
      "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                    (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                )\n",
      "                (intermediate): BertIntermediate(\n",
      "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                  (intermediate_act_fn): GELUActivation()\n",
      "                )\n",
      "                (output): BertOutput(\n",
      "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (pooler): BertPooler(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (activation): Tanh()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=768, out_features=32, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.1, inplace=False)\n",
      "    (3): Linear(in_features=32, out_features=16, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Dropout(p=0.1, inplace=False)\n",
      "    (6): Linear(in_features=16, out_features=8, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Dropout(p=0.1, inplace=False)\n",
      "    (9): Linear(in_features=8, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "trainable params: 1204938 || all params: 109515210 || trainable%: 1.100247171146364\n"
     ]
    }
   ],
   "source": [
    "lora_config = LoraConfig(\n",
    "    #task_type=TaskType.SEQ_CLS, \n",
    "    r=32, # rank of the lower dimensional space\n",
    "    lora_alpha=50, # effectively learning rate\n",
    "    lora_dropout=0.1\n",
    ") \n",
    "model_peft = get_peft_model(model, lora_config)\n",
    "torch.manual_seed(123)\n",
    "model_peft_mlp = ModelPeftMLP(model=model_peft, peft_config=lora_config)\n",
    "print(model_peft_mlp)\n",
    "print_trainable_parameters(model_peft_mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "653d08de",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'small_train_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\jacky\\AppData\\Local\\Temp\\ipykernel_10412\\2570750763.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel_peft_mlp\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining_args\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m     \u001b[0mtrain_dataset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msmall_train_dataset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m     \u001b[0meval_dataset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msmall_eval_dataset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[0mcompute_metrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcompute_metrics\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'small_train_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "##### metric = evaluate.load(\"f1\")\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "def model_init(trials):\n",
    "    torch.manual_seed(123)\n",
    "    model_peft = get_peft_model(model, lora_config)\n",
    "    model_peft_mlp = ModelPeftMLP(model=model_peft, \n",
    "                                  peft_config=model_peft.peft_config['default'])\n",
    "    return model_peft_mlp\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(output_dir=\"test_trainer\", \n",
    "                                  evaluation_strategy=\"epoch\",\n",
    "                                  seed=123,\n",
    "                                  data_seed=123)\n",
    "\n",
    "earlystopping = EarlyStoppingCallback(3, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4be2bf36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset arrow (C:/cygwin64/home/jacky/.cache/huggingface/datasets/arrow/yelp_polarity-fa5030fa747c4f91/0.0.0/74f69db2c14c2860059d39860b1f400a03d11bf7fb5a8258ca38c501c878c137)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e1bfd6ac3674daeae5e9a4029bde933",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenized dataset exists. Load from disk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at D:\\cygdrive\\d\\projects\\LLM\\.cache\\tokenized_dataset_yelp_polarity\\train\\cache-0b983b74d94eba28.arrow\n",
      "Loading cached shuffled indices for dataset at D:\\cygdrive\\d\\projects\\LLM\\.cache\\tokenized_dataset_yelp_polarity\\test\\cache-01e541604b191dc4.arrow\n"
     ]
    }
   ],
   "source": [
    "if not os.path.isdir(dataset_path):\n",
    "    dataset = load_dataset(dataset_name, cache_dir=cache_dir + '/parquet')\n",
    "else:\n",
    "    dataset = load_dataset(dataset_path)\n",
    "    \n",
    "if not os.path.isdir(tokenized_data_path):\n",
    "    print('tokenized dataset does not exist. Download dataset')\n",
    "    if not os.path.isfile(model_path + '/tokenizer.json'):\n",
    "        print('tokenizer does not exist. Create and save tokenized dataset')\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-cased\", \n",
    "                                                  cache_dir=cache_dir) # to load tokenizer to cache\n",
    "    else:\n",
    "        print('tokenizer exists. Load from existing tokenizer')\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_path, \n",
    "                                              cache_dir=cache_dir, \n",
    "                                              local_flies_only=True) # to load tokenizer from cache\n",
    "    tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "    tokenized_datasets.save_to_disk(tokenized_data_path)\n",
    "else:\n",
    "    print('tokenized dataset exists. Load from disk')\n",
    "    tokenized_datasets = load_from_disk(tokenized_data_path)\n",
    "\n",
    "tokenized_datasets = tokenized_datasets.remove_columns([\"text\"])\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "434ab428",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\projects\\LLM\\env\\lib\\site-packages\\transformers\\trainer.py:374: FutureWarning: `Trainer` requires either a `model` or `model_init` argument, but not both. `model_init` will overwrite your model when calling the `train` method. This will become a fatal error in the next release.\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'on_init_end'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\jacky\\AppData\\Local\\Temp\\ipykernel_10412\\4088195334.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mcompute_metrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcompute_metrics\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mmodel_init\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel_init\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mearlystopping\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mstudy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mgc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m )\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\projects\\LLM\\env\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\u001b[0m\n\u001b[0;32m    701\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabel_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdefault_label_names\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabel_names\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabel_names\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    702\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcan_return_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcan_return_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 703\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_init_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    704\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    705\u001b[0m         \u001b[1;31m# Internal variables to keep track of the original batch size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\projects\\LLM\\env\\lib\\site-packages\\transformers\\trainer_callback.py\u001b[0m in \u001b[0;36mon_init_end\u001b[1;34m(self, args, state, control)\u001b[0m\n\u001b[0;32m    347\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    348\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mon_init_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTrainingArguments\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTrainerState\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontrol\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTrainerControl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 349\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall_event\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"on_init_end\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontrol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    350\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    351\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mon_train_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTrainingArguments\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTrainerState\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontrol\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTrainerControl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\projects\\LLM\\env\\lib\\site-packages\\transformers\\trainer_callback.py\u001b[0m in \u001b[0;36mcall_event\u001b[1;34m(self, event, args, state, control, **kwargs)\u001b[0m\n\u001b[0;32m    395\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcall_event\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontrol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    396\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 397\u001b[1;33m             result = getattr(callback, event)(\n\u001b[0m\u001b[0;32m    398\u001b[0m                 \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    399\u001b[0m                 \u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'function' object has no attribute 'on_init_end'"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model_peft_mlp,\n",
    "    args=training_args,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    model_init=model_init,\n",
    "    callbacks=[earlystopping, lambda study, trial: gc.collect()]\n",
    ")\n",
    "\n",
    "best_trials = trainer.hyperparameter_search(\n",
    "    direction='maximize',\n",
    "    backend='optuna',\n",
    "    hp_space=optuna_hp_space,\n",
    "    n_trials=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1826b793",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
