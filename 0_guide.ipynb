{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6aeca91",
   "metadata": {},
   "source": [
    "## 0. Installation\n",
    "\n",
    "[guide reference](https://huggingface.co/docs/huggingface_hub/quick-start)\n",
    "\n",
    "\n",
    "### 0.1 huggingface_hub\n",
    "- ```pip install huggingface_hub```\n",
    "- ```pip install -U \"huggingface_hub[cli]\"```\n",
    "\n",
    "### 0.2 transformers\n",
    "- ```pip install 'transformers[torch]'```  \n",
    "- ```pip install 'transformers[tf-cpu]'```\n",
    "- ```pip install 'transformers[flax]'```\n",
    "\n",
    "### 0.3 others\n",
    "- ```pip install datasets```\n",
    "- ```pip install evaluate```\n",
    "- ```pip install scikit-learn``` (evaluate dependencies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7bbf6c",
   "metadata": {},
   "source": [
    "## 1. Model\n",
    "\n",
    "See [model hub](https://huggingface.co/models) for model investory\n",
    "\n",
    "### 1.1 pipeline\n",
    "\n",
    "A pipeline load a model for a specified NLP task (e.g. zero-shot-classification). User has an option to specify a model. If not specified, a default model (e.g. gpt-2) will be used. See [pipeline](https://huggingface.co/docs/transformers/en/pipeline_tutorial) for more examples. To initiate a pipeline\n",
    "\n",
    "```python\n",
    "model = pipeline('zero-shot-classification', \n",
    "                      model='roberta-large-mnli', \n",
    "                      cache_dir='PATH/TO/CACHE/DIR'\n",
    "                      device=0)\n",
    "```\n",
    "\n",
    "The model will by default saved in ```~/.cache```. To delete cached model, first install\n",
    "\n",
    "```pip install huggingface_hub[\"cli\"]```\n",
    "\n",
    "Then run\n",
    "\n",
    "```huggingface-cli delete-cache```\n",
    "\n",
    "### 1.2 AutoClass\n",
    "\n",
    "Automatically infers and loads the correct architecture from a given checkpoint. Use with the ```from_pretrained()``` method. For NLP, one usually call the **AutoTokenizer** and **AutoModelFor** class. This ensures that the correct architecture is loaded [see ref](https://huggingface.co/docs/transformers/en/autoclass_tutorial#:~:text=instances%20of%20models.-,This%20will%20ensure%20you%20load%20the%20correct%20architecture%20every%20time.%20In%20the%20next%20tutorial%2C%20learn%20how%20to%20use%20your%20newly%20loaded%20tokenizer%2C%20image%20processor%2C%20feature%20extractor%20and%20processor%20to%20preprocess%20a%20dataset%20for%20fine%2Dtuning.,-TensorFlow).\n",
    "\n",
    "For example:\n",
    "\n",
    "```python\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "model_name = 'distilbert/distilbert-base-uncased'\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "```\n",
    "\n",
    "\n",
    "### 1.3 Fine-tuning\n",
    "\n",
    "#### 1.3.1 trainer class\n",
    "\n",
    "train the head of the transformer (see [ref](https://huggingface.co/docs/transformers/v4.39.3/en/training#:~:text=You%20will%20see,model%20to%20it)). Allow specifying the hyperparameters such as gradient accumulation, mixed precision, learning rate scheduler, regularization etc. Another detail example can be found [here](https://towardsdatascience.com/fine-tuning-pretrained-nlp-models-with-huggingfaces-trainer-6326a4456e7b).\n",
    "\n",
    "To freeze some weights of a transformer, do\n",
    "\n",
    "```python\n",
    "for param in list_of_param_to_be_frozen:\n",
    "    param.requires_grad = False\n",
    "```\n",
    "\n",
    "For example, to freeze the embedding layers and the first n transformer layers \n",
    "\n",
    "```python\n",
    "modules = [L1bb.embeddings, *L1bb.encoder.layer[:n]] \n",
    "for module in modules:\n",
    "    for param in module.parameters():\n",
    "        param.requires_grad = False\n",
    "```\n",
    "\n",
    "See [discussion](https://discuss.huggingface.co/t/how-to-freeze-some-layers-of-bertmodel/917#:~:text=modules%20%3D%20%5BL1bb.embeddings%2C%20*L1bb.encoder.layer%5B%3A5%5D%5D%20%23Replace%205%20by%20what%20you%20want%0Afor%20module%20in%20mdoules%3A%0A%20%20%20%20for%20param%20in%20module.parameters()%3A%0A%20%20%20%20%20%20%20%20param.requires_grad%20%3D%20False) for more detail.\n",
    "\n",
    "#### 1.3.2 adapter\n",
    "\n",
    "add additional trainable weights to the transformer, which weights are frozen. This has been shown to be very memory-efficient with lower compute usage while producing results comparable to a fully fine-tuned model. This falls under a more general class of training called *Parameter-Efficient Fine Tuning* (PEFT). See [here](https://huggingface.co/docs/transformers/en/peft) for more detail.\n",
    "\n",
    "```python\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_id = \"facebook/opt-350m\"\n",
    "peft_model_id = \"ybelkada/opt-350m-lora\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "model.load_adapter(peft_model_id)\n",
    "```\n",
    "\n",
    "#### 1.3.3 [training with script](https://huggingface.co/docs/transformers/en/run_scripts)\n",
    "\n",
    "this fine-tunes the model on a more granular (weights) level. Since this involves updating the transformer model itself (rather than changing hyperparameters or adding and training an adapter). There are scripts in [here](https://github.com/huggingface/transformers/tree/main/examples/research_projects) available for user to make necessary changes to adapt to their use cases, such as training on a customer dataset (see [here](https://huggingface.co/docs/transformers/en/run_scripts#:~:text=tmp/tst%2Dsummarization-,Use%20a%20custom%20dataset,-The%20summarization%20script) for more detail).\n",
    "\n",
    "### 1.4 TrainingArguments: hyperparameters\n",
    "**batch size**\n",
    "\n",
    "**learning rate/scheduler**\n",
    "\n",
    "**regularization**\n",
    "\n",
    "### 1.5 Distributed training with [acceleration](https://huggingface.co/docs/accelerate/en/concept_guides/gradient_synchronization)\n",
    "**Gradient synchronization**\n",
    "\n",
    "**Gradient accumulation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ecc6ce",
   "metadata": {},
   "source": [
    "## N. Misc\n",
    "\n",
    "### N.1 Questions\n",
    "- When running pipeline/specify a model, what do we download?\n",
    "- Difference between ```pipeline``` and ```from_pretrained```?\n",
    "- How to run inference using GPU?\n",
    "- When the model is cached, is it stored on RAM? GPU?\n",
    "- In ```pipeline``` one can specify the task e.g. \"text-generation\". When using ```model.generate```, how to specify the task?\n",
    "\n",
    "\n",
    "### N.2 FAQ \n",
    "\n",
    "#### Theory\n",
    "- What's the different between **architecture** and **checkpoint**?\n",
    "    - Architecture refers to the skeleton of the model and checkpoints are the weights for a given architecture. For example, BERT is an architecture, while google-bert/bert-base-uncased is a checkpoint. Model is a general term that can mean either architecture or checkpoint.\n",
    "- What's the difference between **AutoModelForSeq2SeqLM** and **AutoModelForCausalLM**?\n",
    "    - AutoModelForSeq2SeqLM is used for language models with encoder-decoder architecture, like T5 and BART. This architecture is typically used in generative tasks where the output heavily relies on the input (e.g. translation, summarization). AutoModelForCausalLM is used for auto-regressive language models (decoder-only) like all the GPT models. They are used for all other types of generative tasks [(Reference)](https://stackoverflow.com/questions/75549632/difference-between-automodelforseq2seqlm-and-automodelforcausallm). For example. falcon is a decoder-only model and therefore is not suitable for sequence-to-sequence task.\n",
    "- What is the different between **base** and **instruct** prompting?\n",
    "    - Instruct is a checkpoint of a model that is furthered fine-tuned on a specific corpus. Instruct models are more suitable for NLP tasks. Instruct are model trained by instruction fine-tuning. They are likely not suitable for further fine-tuning. To fine-tune a model, use base instead of instruct model [reference](https://huggingface.co/tiiuae/falcon-7b-instruct#:~:text=This%20is%20an%20instruct%20model%2C%20which%20may%20not%20be%20ideal%20for%20further%20finetuning.%20If%20you%20are%20interested%20in%20building%20your%20own%20instruct/chat%20model%2C%20we%20recommend%20starting%20from%20Falcon%2D7B.).\n",
    "- LoRA does not offer much boost in computation time. Why is that?\n",
    "    - TBA, but look into gradient computation, backward and forward passing and see if LoRA reduce those operations. Also take into account of overhead in adding the adapters. See [reference](https://github.com/huggingface/transformers/issues/25760) for PEFT discussion and [reference](https://pytorch.org/docs/stable/notes/autograd.html#:~:text=During%20the%20forward%20pass%2C%20an%20operation%20is%20only%20recorded%20in%20the%20backward%20graph%20if%20at%20least%20one%20of%20its%20input%20tensors%20require%20grad.%20During%20the%20backward%20pass%20(.backward())%2C%20only%20leaf%20tensors%20with%20requires_grad%3DTrue%20will%20have%20gradients%20accumulated%20into%20their%20.grad%20fields.) for the discussion of the mechanism of autograd and ```requires_grad()```.\n",
    "- What is the difference between text-generation, text-summarization and text-to-text generation? How they are trained and how does the performance change if using ```AutoModelFor``` class for on task to perform another task? \n",
    "- Try to do few-shot prompting with T5-3b. The result is significantly worse than ChatGPT. Why is that?\n",
    "    - T5-3b is encoder-decoder, ChatGPT is decoder only?\n",
    "    - T5-3b is much smaller than ChatGPT?\n",
    "    - Using the most efficient hyperparameters?\n",
    "    - ChatGPT use RLHF?\n",
    "- What is initialization scales in deep learning?\n",
    "    - TBA\n",
    "- How does BERT deal with out-of-vocabulary words?\n",
    "    - BERT use subword tokenization algorithm called *WordPiece*. This creates subword embedding instead of word embedding. This could go all the down to character level [reference](https://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial/#:~:text=As%20a%20result%2C%20rather%20than%20assigning%20out%20of%20vocabulary%20words%20to%20a%20catch%2Dall%20token%20like%20%E2%80%98OOV%E2%80%99%20or%20%E2%80%98UNK%2C%E2%80%99%20words%20that%20are%20not%20in%20the%20vocabulary%20are%20decomposed%20into%20subword%20and%20character%20tokens%20that%20we%20can%20then%20generate%20embeddings%20for.).\n",
    "    \n",
    "#### Practical\n",
    "[HF troubleshooting](https://huggingface.co/docs/transformers/v4.18.0/en/troubleshooting)\n",
    "\n",
    "- I'm facing the OSError: Can't load config for \\<model>. Make sure that: - \\<model> is a correct model identifier listed on 'https://huggingface.co/models'.\n",
    "    - Run ```pip install --upgrade transformers```\n",
    "- I got the assertion error: \"AssertionError: Torch not compiled with CUDA enabled\" when trying to use GPU (e.g. ```torch.cuda.is_available()```).\n",
    "    - Run ```pip install torch==1.9.0+cu111 torchvision==0.10.0+cu111 torchaudio==0.9.0 -f https://download.pytorch.org/whl/torch_stable.html``` [see ref](https://github.com/pytorch/pytorch/issues/50032).\n",
    "- What's the difference between ```pipeline``` and ```from_pretrained```?\n",
    "    - ```pipeline``` load up a model. ```from_pretrained``` from **AutoClass** automatically infers and loads up a correct architecture given a checkpoint.\n",
    "- When running pipeline/specify a model, what do we download?\n",
    "    - The model snapshot is downloaded by default to ```~/.cache```. \n",
    "- Do I need to run **AutoTokenizer** before **AutoModelFor**? Why?\n",
    "    - It is recommended to use the **AutoTokenizer** class before the **AutoModelFor** class to load pretrained instances of models. This will ensure you load the correct architecture every time.\n",
    "- ```torch.cuda.is_available()=False```\n",
    "    - Make sure cuda version is compatible with pytorch. E.g. torch 1.9.0 is compatible with cuda 11.1. If upgraded to torch 1.13.1, it is only compatible with cuda 11.6. See [reference](https://saturncloud.io/blog/why-torchcudaisavailable-returns-false-even-after-installing-pytorch-with-cuda/#:~:text=is_available()%20might%20return%20False%20is%20that%20the%20installed%20version,is_available()%20will%20return%20False%20.)\n",
    "- ```peft 0.3.0``` requires with ```torch > 1.13.0```\n",
    "    - TBA\n",
    "- When the model is cached, is it stored on RAM? GPU?\n",
    "    - TBA\n",
    "- Where is HF datasets stored?\n",
    "    - Disk by default. Therefore will not inflate memory usage ([ref](https://huggingface.co/docs/transformers/en/training#:~:text=Remember%20that%20Hugging,the%20entire%20dataset)).\n",
    "- How to run inference/train using GPU?\n",
    "    - TBA\n",
    "- How to use **accelerate** library?\n",
    "    - TBA [reference](https://huggingface.co/blog/accelerated-inference)\n",
    "- How is CPU, memory and GPU used at inference? (using **accelerate**)\n",
    "    - [reference1](https://huggingface.co/docs/accelerate/usage_guides/big_modeling)\n",
    "    - [reference2](https://huggingface.co/docs/accelerate/concept_guides/big_model_inference)\n",
    "- What are the alternatives to huggingface?\n",
    "    - OpenAI, Cohere\n",
    "- Cuda driver API vs runtime? Difference between ```nvidia-smi``` and ```nvcc --version```?\n",
    "    - TBA\n",
    "- Facing ```AttributeError: module 'signal' has no attribute 'SIGALRM'``` when using ```AutoModelForCausalLM.from_pretrained()```\n",
    "    - This error happens if you are using Windows. Go to the ```.py``` file that raise this error. Replace ```signal.SIGALRM``` with one of the attributes [here](https://forum.huawei.com/enterprise/en/error-message-attributeerror-module-signal-has-no-attribute-sigchld/thread/667249083749908480-667213860102352896) that is compatible with Windows. If needed, comment out ```signal.alarm()``` as well. [Reference](https://www.reddit.com/r/StableDiffusion/comments/yq1z7n/stable_diffusion_install_error/)\n",
    "- How to install the right pytorch version?\n",
    "    - first check the cuda version by running ```nvcc --version```\n",
    "    - go to [this website](https://pytorch.org/get-started/previous-versions/) and find the ```pip install``` command for the corresponding cuda version. For example, for cuda=1.16, the command is ``` pip install torch==1.12.0+cu116 torchvision==0.13.0+cu116 torchaudio==0.12.0 --extra-index-url https://download.pytorch.org/whl/cu116```.\n",
    "- KeyError: 'mistral' when trying to load Mistral model\n",
    "    - Need to upgrade ```transformers``` to ```4.35.2```. This requires python version >=3.8\n",
    "- Changing python version: how to deal with the virtual environment?\n",
    "    - If using pycharm, the simpliest way is to create and initiate another repo using the new python version as the interpreter. Once it's initialized, install ```ipykernel``` and all the other packages. This will create an ipykernel for this venv. This way, if you want to switch to a new environement from an existing repo (initiated under an older python version), you can just use the ipykernel created in the new repo without moving any env folder. **If you need to install new package, go to the LLMpy38 repo and pip install directly without source venv/bin/active**\n",
    "- What is cache and how it is different from SSD, RAM and CPU memory?\n",
    "    - TBA\n",
    "- Getting CUDA OOM error when doing inference/forward passing.\n",
    "    - use ```no_grad()``` to avoid building the computation graph (only used for backprop). See [reference](https://stackoverflow.com/questions/55322434/how-to-clear-cuda-memory-in-pytorch#:~:text=Basically%2C%20what%20PyTorch,for%20my%20model.)\n",
    "- OSError: You are trying to access a gated repo. Asking for token\n",
    "    ``` python \n",
    "    from huggingface_hub import notebook_login\n",
    "    notebook_login()\n",
    "    ```\n",
    "    This will prompt for a token. Following the link shown.\n",
    "- Why if HF loading model/running inference much slower than Ollama?\n",
    "    - GGUF vs safetensors\n",
    "- Loading model takes up too much RAM\n",
    "    - use ```init_empty_weights()``` to init an empty skeleton of the model. This will not take up any RAM. [reference](https://huggingface.co/docs/accelerate/usage_guides/big_modeling)\n",
    "- Taking too much time at inference\n",
    "    - Probably this is because the whole model (weights) is loaded on GPU, RAM and disk. A technique that leaves a smaller memory footprint is to dynamically load the weights for each layer. See [reference](https://huggingface.co/docs/accelerate/usage_guides/big_modeling#:~:text=What%20will%20happen,on%20your%20GPU.) for detail explanation of the mechanism.\n",
    "- device mapping, offloading issue\n",
    "    - [reference](https://github.com/huggingface/accelerate/issues/362)\n",
    "- what does the second progress bar after each epoch means when running ```trainer.train()```?\n",
    "    - TBA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98184c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
