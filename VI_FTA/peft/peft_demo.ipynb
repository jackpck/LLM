{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PEFT Tutorial\n",
    "*(A bulk of the material of this tutorial is taken from Sebastian Raschka's [Code Lora from Scratch](https://lightning.ai/lightning-ai/studios/code-lora-from-scratch).)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from functools import partial\n",
    "\n",
    "import lightning as L\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from custom_lightning_module import CustomLightningModule\n",
    "from datasets import load_dataset\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuring Paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIR = \"../../data/imdb/\"\n",
    "SAVED_MODEL_DIR = \"/projects/fta_bootcamp/trained_models/peft_demo/\"\n",
    "OUTPUT_DIR = \"../../scratch/peft/\" # main directory of the the demo output\n",
    "CHECKPOINT_DIR = f\"{OUTPUT_DIR}checkpoints\" # where to save checkpoints\n",
    "MODEL_NAME = \"distilbert-base-uncased\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our Custom LoRA Layer <a id=\"LoRA_Anchor\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "Ncv--YQFv3zM"
   },
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision(\"medium\")\n",
    "\n",
    "class LoRALayer(torch.nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, rank, alpha):\n",
    "        super().__init__()\n",
    "        std_dev = 1 / torch.sqrt(torch.tensor(rank).float())\n",
    "        self.W_a = torch.nn.Parameter(torch.randn(in_dim, rank) * std_dev)\n",
    "        self.W_b = torch.nn.Parameter(torch.zeros(rank, out_dim))\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, x):\n",
    "        ### TODO: TODO: implement the forward pass of lora ###\n",
    "        return self.alpha*torch.matmul(x, torch.matmul(self.W_a, self.W_b))\n",
    "\n",
    "\n",
    "class LinearWithLoRA(torch.nn.Module):\n",
    "    def __init__(self, linear, rank, alpha):\n",
    "        super().__init__()\n",
    "        self.linear = linear\n",
    "        self.lora = LoRALayer(\n",
    "            linear.in_features, linear.out_features, rank, alpha,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        ### TODO: TODO: implement the forward pass of lora layer ###\n",
    "        return self.linear(x) + self.lora(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iKLiB8_Py7kT",
    "outputId": "24ac20cf-f044-45a5-d342-48c119e35091"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5745]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "# a simple linear layer with 10 inputs and 1 output\n",
    "# requires_grad=False makes it non-trainable\n",
    "with torch.no_grad():\n",
    "    linear_layer = torch.nn.Linear(10, 1)\n",
    "\n",
    "# a simple example input\n",
    "x = torch.rand((1, 10))\n",
    "\n",
    "linear_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KNB_hhGu0pyL",
    "outputId": "50d64706-93c0-400c-d8af-c304751991b3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5745]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lora_layer = LinearWithLoRA(linear=linear_layer, rank=8, alpha=1)\n",
    "lora_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cnB2Q5Yg06oF",
    "outputId": "28a3dde6-a127-4d28-afa5-f4556db58f0e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5863, -0.5758, -0.5779, -0.5800, -0.5814, -0.5766, -0.5887, -0.5811,\n",
       "         -0.5859, -0.5892]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lora_layer.lora.W_b = torch.nn.Parameter(lora_layer.lora.W_b + 0.01 * x[0])\n",
    "lora_layer(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cI2eU3XG5NlV"
   },
   "source": [
    "## Loading the Dataset into DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "W2UlNJPV7HCp"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e94f4789d34e44d8a914910082156c8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49dd6908734a436cb056a5c74c1c24de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "541567ede20d444d8402429334200b97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['index', 'text', 'label'],\n",
      "        num_rows: 35000\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['index', 'text', 'label'],\n",
      "        num_rows: 5000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['index', 'text', 'label'],\n",
      "        num_rows: 10000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "imdb_dataset = load_dataset(\n",
    "    \"csv\",\n",
    "    data_files={\n",
    "        \"train\": os.path.join(DATASET_DIR, \"train.csv\"),\n",
    "        \"validation\": os.path.join(DATASET_DIR, \"val.csv\"),\n",
    "        \"test\": os.path.join(DATASET_DIR, \"test.csv\"),\n",
    "    },\n",
    ")\n",
    "\n",
    "print(imdb_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "pbcTZXBl7nXW",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs01/projects/fta_bootcamp/envs/finetune_demo/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cb31726f35541db8019258406ae3542",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "236bfb012f9443e9a83748e90d53c218",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d19210469244f31b64d66fcc6010237",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d2f33ffe5a740448b33ceb2e7bad2a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer input max length: 512\n",
      "Tokenizer vocabulary size: 30522\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "print(\"Tokenizer input max length:\", tokenizer.model_max_length)\n",
    "print(\"Tokenizer vocabulary size:\", tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "prwgIhGT7rKr"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1c84ba8e7e14b038f5a3e32f0929e22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/35000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d879ebfb5f9d440c9bbb3f1eace3bdea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ee830ee26d84578985585f5b7c2edc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_text(batch):\n",
    "    return tokenizer(batch[\"text\"], truncation=True, padding=True)\n",
    "\n",
    "imdb_tokenized = imdb_dataset.map(tokenize_text, batched=True, batch_size=None)\n",
    "del imdb_dataset\n",
    "imdb_tokenized.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NYIXdGrh75jd"
   },
   "source": [
    "## Setting Up DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "1ZV9sxEZ74gw"
   },
   "outputs": [],
   "source": [
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, dataset_dict, partition_key=\"train\"):\n",
    "        self.partition = dataset_dict[partition_key]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.partition[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.partition.num_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "e8etLSmv8AKz"
   },
   "outputs": [],
   "source": [
    "train_dataset = IMDBDataset(imdb_tokenized, partition_key=\"train\")\n",
    "val_dataset = IMDBDataset(imdb_tokenized, partition_key=\"validation\")\n",
    "test_dataset = IMDBDataset(imdb_tokenized, partition_key=\"test\")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=12,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    dataset=val_dataset,\n",
    "    batch_size=12,\n",
    "    num_workers=4,\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=12,\n",
    "    num_workers=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting Number of Trainable Parameters Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R7tPqkMu9QpM"
   },
   "source": [
    "## Finetunning Last Two Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "OH-ZTf4I8FAm"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of trainable parameters for the base model: 66,955,010\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME, num_labels=2)\n",
    "\n",
    "print(f\"Total number of trainable parameters for the base model: {count_parameters(model):,}\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fv3ssM808Te6"
   },
   "source": [
    "Freeze all the layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "2hzpYG5x8brd"
   },
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X0lCrJJP8h4g"
   },
   "source": [
    "Unfreeze the last two layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "B_R3AwvP9JH0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of trainable parameters: 592,130\n"
     ]
    }
   ],
   "source": [
    "for param in model.pre_classifier.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "for param in model.classifier.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "print(f\"Total number of trainable parameters: {count_parameters(model):,}\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "7j3LUEJH9VLa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "lightning_model = CustomLightningModule(model)\n",
    "callbacks = [\n",
    "    ModelCheckpoint(\n",
    "        dirpath=CHECKPOINT_DIR,\n",
    "        filename=\"last_two\",\n",
    "        save_top_k=1, # save top 1 model\n",
    "        mode=\"max\",\n",
    "        monitor=\"val_acc\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "logger = CSVLogger(save_dir=\"logs/\", name=\"my-model\")\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    max_epochs=3,\n",
    "    callbacks=callbacks,\n",
    "    accelerator=\"gpu\",\n",
    "    precision=\"16-mixed\",\n",
    "    devices=1,\n",
    "    logger=logger,\n",
    "    log_every_n_steps=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "ejzK98ii9kcU"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Missing logger folder: logs/my-model\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name     | Type                                | Params\n",
      "-----------------------------------------------------------------\n",
      "0 | model    | DistilBertForSequenceClassification | 67.0 M\n",
      "1 | val_acc  | MulticlassAccuracy                  | 0     \n",
      "2 | test_acc | MulticlassAccuracy                  | 0     \n",
      "-----------------------------------------------------------------\n",
      "592 K     Trainable params\n",
      "66.4 M    Non-trainable params\n",
      "67.0 M    Total params\n",
      "267.820   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78ac820b10264eecb836d403a0f65942",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cde3c7a19e2421dac2b7a00d555ccdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a732d9ae87f245df8b5ae86c9ba881c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ba425867fa24bc8918a40318f419a85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1935609ed8ff4e2085875f537260af1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed 11.46 min\n"
     ]
    }
   ],
   "source": [
    "# Comment cell below if you don't want to go through the training process. You can just load a trained model in the next cell.\n",
    "\n",
    "start = time.time()\n",
    "trainer.fit(model=lightning_model,\n",
    "            train_dataloaders=train_loader,\n",
    "            val_dataloaders=val_loader)\n",
    "\n",
    "end = time.time()\n",
    "elapsed = end - start\n",
    "print(f\"Time elapsed {elapsed/60:.2f} min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from a saved model\n",
    "lightning_model = CustomLightningModule.load_from_checkpoint(checkpoint_path=\"/projects/fta_bootcamp/trained_models/peft_demo//last_two.ckpt\", model=model)\n",
    "\n",
    "# train_acc = trainer.validate(lightning_model, dataloaders=train_loader, verbose=False)\n",
    "# val_acc = trainer.validate(lightning_model, dataloaders=val_loader, verbose=False)\n",
    "test_acc = trainer.test(lightning_model, dataloaders=test_loader, verbose=False)\n",
    "\n",
    "# print(f\"Train acc: {train_acc[0]['val_acc']*100:2.2f}%\")\n",
    "# print(f\"Val acc:   {val_acc[0]['val_acc']*100:2.2f}%\")\n",
    "print(f\"Test acc:  {test_acc[0]['accuracy']*100:2.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enter LoRA!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4215737b4abf48e78531b66b0a68558b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME, num_labels=2)\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use our [LoRA layer](#LoRA_Anchor) implementation from before. Here's our current model *before* adding LoRA layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's wrap the query and value layers of transformer blocks with LoRA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_adaptation_layer(model, adaptation_layer, lora_r, lora_alpha, config):\n",
    "    assign_lora = partial(adaptation_layer5, rank=lora_r, alpha=lora_alpha)\n",
    "\n",
    "    for layer in model.distilbert.transformer.layer:\n",
    "        if config.get(\"lora_query\"):\n",
    "            pass ### TODO: TODO: look at the model architecture and and use assign_lora function ###\n",
    "        if config.get(\"lora_key\"):\n",
    "            pass ### TODO: TODO: look at the model architecture and and use assign_lora function ###\n",
    "        if config.get(\"lora_value\"):\n",
    "            pass ### TODO: TODO: look at the model architecture and and use assign_lora function ###\n",
    "        if config.get(\"lora_projection\"):\n",
    "            pass ### TODO: TODO: look at the model architecture and and use assign_lora function ###\n",
    "        if config.get(\"lora_mlp\"):\n",
    "            ### look at the model architecture and and use assign_lora function (make sure you apply to both linear layers in fnn) ###\n",
    "            pass\n",
    "    if config.get(\"lora_head\"):\n",
    "        ### look at the model architecture and and use assign_lora function. Apply to both pre_classifier and classifier layers) ###\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"lora_query\": True,\n",
    "    \"lora_key\": False,\n",
    "    \"lora_value\": True,\n",
    "    \"lora_projection\": False,\n",
    "    \"lora_mlp\": False,\n",
    "    \"lora_head\": False,\n",
    "}\n",
    "apply_adaptation_layer(model, adaptation_layer=LinearWithLoRA, lora_r=8, lora_alpha=16, config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the model after the LoRA layers are added:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distilbert.embeddings.word_embeddings.weight: False\n",
      "distilbert.embeddings.position_embeddings.weight: False\n",
      "distilbert.embeddings.LayerNorm.weight: False\n",
      "distilbert.embeddings.LayerNorm.bias: False\n",
      "distilbert.transformer.layer.0.attention.q_lin.weight: False\n",
      "distilbert.transformer.layer.0.attention.q_lin.bias: False\n",
      "distilbert.transformer.layer.0.attention.k_lin.weight: False\n",
      "distilbert.transformer.layer.0.attention.k_lin.bias: False\n",
      "distilbert.transformer.layer.0.attention.v_lin.weight: False\n",
      "distilbert.transformer.layer.0.attention.v_lin.bias: False\n",
      "distilbert.transformer.layer.0.attention.out_lin.weight: False\n",
      "distilbert.transformer.layer.0.attention.out_lin.bias: False\n",
      "distilbert.transformer.layer.0.sa_layer_norm.weight: False\n",
      "distilbert.transformer.layer.0.sa_layer_norm.bias: False\n",
      "distilbert.transformer.layer.0.ffn.lin1.weight: False\n",
      "distilbert.transformer.layer.0.ffn.lin1.bias: False\n",
      "distilbert.transformer.layer.0.ffn.lin2.weight: False\n",
      "distilbert.transformer.layer.0.ffn.lin2.bias: False\n",
      "distilbert.transformer.layer.0.output_layer_norm.weight: False\n",
      "distilbert.transformer.layer.0.output_layer_norm.bias: False\n",
      "distilbert.transformer.layer.1.attention.q_lin.weight: False\n",
      "distilbert.transformer.layer.1.attention.q_lin.bias: False\n",
      "distilbert.transformer.layer.1.attention.k_lin.weight: False\n",
      "distilbert.transformer.layer.1.attention.k_lin.bias: False\n",
      "distilbert.transformer.layer.1.attention.v_lin.weight: False\n",
      "distilbert.transformer.layer.1.attention.v_lin.bias: False\n",
      "distilbert.transformer.layer.1.attention.out_lin.weight: False\n",
      "distilbert.transformer.layer.1.attention.out_lin.bias: False\n",
      "distilbert.transformer.layer.1.sa_layer_norm.weight: False\n",
      "distilbert.transformer.layer.1.sa_layer_norm.bias: False\n",
      "distilbert.transformer.layer.1.ffn.lin1.weight: False\n",
      "distilbert.transformer.layer.1.ffn.lin1.bias: False\n",
      "distilbert.transformer.layer.1.ffn.lin2.weight: False\n",
      "distilbert.transformer.layer.1.ffn.lin2.bias: False\n",
      "distilbert.transformer.layer.1.output_layer_norm.weight: False\n",
      "distilbert.transformer.layer.1.output_layer_norm.bias: False\n",
      "distilbert.transformer.layer.2.attention.q_lin.weight: False\n",
      "distilbert.transformer.layer.2.attention.q_lin.bias: False\n",
      "distilbert.transformer.layer.2.attention.k_lin.weight: False\n",
      "distilbert.transformer.layer.2.attention.k_lin.bias: False\n",
      "distilbert.transformer.layer.2.attention.v_lin.weight: False\n",
      "distilbert.transformer.layer.2.attention.v_lin.bias: False\n",
      "distilbert.transformer.layer.2.attention.out_lin.weight: False\n",
      "distilbert.transformer.layer.2.attention.out_lin.bias: False\n",
      "distilbert.transformer.layer.2.sa_layer_norm.weight: False\n",
      "distilbert.transformer.layer.2.sa_layer_norm.bias: False\n",
      "distilbert.transformer.layer.2.ffn.lin1.weight: False\n",
      "distilbert.transformer.layer.2.ffn.lin1.bias: False\n",
      "distilbert.transformer.layer.2.ffn.lin2.weight: False\n",
      "distilbert.transformer.layer.2.ffn.lin2.bias: False\n",
      "distilbert.transformer.layer.2.output_layer_norm.weight: False\n",
      "distilbert.transformer.layer.2.output_layer_norm.bias: False\n",
      "distilbert.transformer.layer.3.attention.q_lin.weight: False\n",
      "distilbert.transformer.layer.3.attention.q_lin.bias: False\n",
      "distilbert.transformer.layer.3.attention.k_lin.weight: False\n",
      "distilbert.transformer.layer.3.attention.k_lin.bias: False\n",
      "distilbert.transformer.layer.3.attention.v_lin.weight: False\n",
      "distilbert.transformer.layer.3.attention.v_lin.bias: False\n",
      "distilbert.transformer.layer.3.attention.out_lin.weight: False\n",
      "distilbert.transformer.layer.3.attention.out_lin.bias: False\n",
      "distilbert.transformer.layer.3.sa_layer_norm.weight: False\n",
      "distilbert.transformer.layer.3.sa_layer_norm.bias: False\n",
      "distilbert.transformer.layer.3.ffn.lin1.weight: False\n",
      "distilbert.transformer.layer.3.ffn.lin1.bias: False\n",
      "distilbert.transformer.layer.3.ffn.lin2.weight: False\n",
      "distilbert.transformer.layer.3.ffn.lin2.bias: False\n",
      "distilbert.transformer.layer.3.output_layer_norm.weight: False\n",
      "distilbert.transformer.layer.3.output_layer_norm.bias: False\n",
      "distilbert.transformer.layer.4.attention.q_lin.weight: False\n",
      "distilbert.transformer.layer.4.attention.q_lin.bias: False\n",
      "distilbert.transformer.layer.4.attention.k_lin.weight: False\n",
      "distilbert.transformer.layer.4.attention.k_lin.bias: False\n",
      "distilbert.transformer.layer.4.attention.v_lin.weight: False\n",
      "distilbert.transformer.layer.4.attention.v_lin.bias: False\n",
      "distilbert.transformer.layer.4.attention.out_lin.weight: False\n",
      "distilbert.transformer.layer.4.attention.out_lin.bias: False\n",
      "distilbert.transformer.layer.4.sa_layer_norm.weight: False\n",
      "distilbert.transformer.layer.4.sa_layer_norm.bias: False\n",
      "distilbert.transformer.layer.4.ffn.lin1.weight: False\n",
      "distilbert.transformer.layer.4.ffn.lin1.bias: False\n",
      "distilbert.transformer.layer.4.ffn.lin2.weight: False\n",
      "distilbert.transformer.layer.4.ffn.lin2.bias: False\n",
      "distilbert.transformer.layer.4.output_layer_norm.weight: False\n",
      "distilbert.transformer.layer.4.output_layer_norm.bias: False\n",
      "distilbert.transformer.layer.5.attention.q_lin.weight: False\n",
      "distilbert.transformer.layer.5.attention.q_lin.bias: False\n",
      "distilbert.transformer.layer.5.attention.k_lin.weight: False\n",
      "distilbert.transformer.layer.5.attention.k_lin.bias: False\n",
      "distilbert.transformer.layer.5.attention.v_lin.weight: False\n",
      "distilbert.transformer.layer.5.attention.v_lin.bias: False\n",
      "distilbert.transformer.layer.5.attention.out_lin.weight: False\n",
      "distilbert.transformer.layer.5.attention.out_lin.bias: False\n",
      "distilbert.transformer.layer.5.sa_layer_norm.weight: False\n",
      "distilbert.transformer.layer.5.sa_layer_norm.bias: False\n",
      "distilbert.transformer.layer.5.ffn.lin1.weight: False\n",
      "distilbert.transformer.layer.5.ffn.lin1.bias: False\n",
      "distilbert.transformer.layer.5.ffn.lin2.weight: False\n",
      "distilbert.transformer.layer.5.ffn.lin2.bias: False\n",
      "distilbert.transformer.layer.5.output_layer_norm.weight: False\n",
      "distilbert.transformer.layer.5.output_layer_norm.bias: False\n",
      "pre_classifier.weight: False\n",
      "pre_classifier.bias: False\n",
      "classifier.weight: False\n",
      "classifier.bias: False\n"
     ]
    }
   ],
   "source": [
    "# Check if linear layers are frozen\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name}: {param.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of trainable parameters: 0\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total number of trainable parameters: {count_parameters(model):,}\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tune with LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "lightning_model = CustomLightningModule(model)\n",
    "callbacks = [\n",
    "    ModelCheckpoint(\n",
    "        dirpath=CHECKPOINT_DIR,\n",
    "        filename=\"lora\",\n",
    "        save_top_k=1, # save top 1 model\n",
    "        mode=\"max\",\n",
    "        monitor=\"val_acc\",\n",
    "    ),\n",
    "]\n",
    "logger = CSVLogger(save_dir=\"logs/\", name=\"my-model\")\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    max_epochs=3,\n",
    "    callbacks=callbacks,\n",
    "    accelerator=\"gpu\",\n",
    "    precision=\"16-mixed\",\n",
    "    devices=1,\n",
    "    logger=logger,\n",
    "    log_every_n_steps=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[61], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Comment cell below if you don't want to go through the training process. You can just load a trained model in the next cell.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m      4\u001b[0m trainer\u001b[38;5;241m.\u001b[39mfit(model\u001b[38;5;241m=\u001b[39mlightning_model,\n\u001b[0;32m----> 5\u001b[0m             train_dataloaders\u001b[38;5;241m=\u001b[39m\u001b[43mtrain_loader\u001b[49m,\n\u001b[1;32m      6\u001b[0m             val_dataloaders\u001b[38;5;241m=\u001b[39mval_loader)\n\u001b[1;32m      8\u001b[0m end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m      9\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m end \u001b[38;5;241m-\u001b[39m start\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_loader' is not defined"
     ]
    }
   ],
   "source": [
    "# Comment cell below if you don't want to go through the training process. You can just load a trained model in the next cell.\n",
    "\n",
    "start = time.time()\n",
    "trainer.fit(model=lightning_model,\n",
    "            train_dataloaders=train_loader,\n",
    "            val_dataloaders=val_loader)\n",
    "\n",
    "end = time.time()\n",
    "elapsed = end - start\n",
    "print(f\"Time elapsed {elapsed/60:.2f} min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for CustomLightningModule:\n\tMissing key(s) in state_dict: \"model.distilbert.transformer.layer.0.attention.q_lin.weight\", \"model.distilbert.transformer.layer.0.attention.q_lin.bias\", \"model.distilbert.transformer.layer.0.attention.v_lin.weight\", \"model.distilbert.transformer.layer.0.attention.v_lin.bias\", \"model.distilbert.transformer.layer.1.attention.q_lin.weight\", \"model.distilbert.transformer.layer.1.attention.q_lin.bias\", \"model.distilbert.transformer.layer.1.attention.v_lin.weight\", \"model.distilbert.transformer.layer.1.attention.v_lin.bias\", \"model.distilbert.transformer.layer.2.attention.q_lin.weight\", \"model.distilbert.transformer.layer.2.attention.q_lin.bias\", \"model.distilbert.transformer.layer.2.attention.v_lin.weight\", \"model.distilbert.transformer.layer.2.attention.v_lin.bias\", \"model.distilbert.transformer.layer.3.attention.q_lin.weight\", \"model.distilbert.transformer.layer.3.attention.q_lin.bias\", \"model.distilbert.transformer.layer.3.attention.v_lin.weight\", \"model.distilbert.transformer.layer.3.attention.v_lin.bias\", \"model.distilbert.transformer.layer.4.attention.q_lin.weight\", \"model.distilbert.transformer.layer.4.attention.q_lin.bias\", \"model.distilbert.transformer.layer.4.attention.v_lin.weight\", \"model.distilbert.transformer.layer.4.attention.v_lin.bias\", \"model.distilbert.transformer.layer.5.attention.q_lin.weight\", \"model.distilbert.transformer.layer.5.attention.q_lin.bias\", \"model.distilbert.transformer.layer.5.attention.v_lin.weight\", \"model.distilbert.transformer.layer.5.attention.v_lin.bias\". \n\tUnexpected key(s) in state_dict: \"model.distilbert.transformer.layer.0.attention.q_lin.linear.weight\", \"model.distilbert.transformer.layer.0.attention.q_lin.linear.bias\", \"model.distilbert.transformer.layer.0.attention.q_lin.lora.W_a\", \"model.distilbert.transformer.layer.0.attention.q_lin.lora.W_b\", \"model.distilbert.transformer.layer.0.attention.v_lin.linear.weight\", \"model.distilbert.transformer.layer.0.attention.v_lin.linear.bias\", \"model.distilbert.transformer.layer.0.attention.v_lin.lora.W_a\", \"model.distilbert.transformer.layer.0.attention.v_lin.lora.W_b\", \"model.distilbert.transformer.layer.1.attention.q_lin.linear.weight\", \"model.distilbert.transformer.layer.1.attention.q_lin.linear.bias\", \"model.distilbert.transformer.layer.1.attention.q_lin.lora.W_a\", \"model.distilbert.transformer.layer.1.attention.q_lin.lora.W_b\", \"model.distilbert.transformer.layer.1.attention.v_lin.linear.weight\", \"model.distilbert.transformer.layer.1.attention.v_lin.linear.bias\", \"model.distilbert.transformer.layer.1.attention.v_lin.lora.W_a\", \"model.distilbert.transformer.layer.1.attention.v_lin.lora.W_b\", \"model.distilbert.transformer.layer.2.attention.q_lin.linear.weight\", \"model.distilbert.transformer.layer.2.attention.q_lin.linear.bias\", \"model.distilbert.transformer.layer.2.attention.q_lin.lora.W_a\", \"model.distilbert.transformer.layer.2.attention.q_lin.lora.W_b\", \"model.distilbert.transformer.layer.2.attention.v_lin.linear.weight\", \"model.distilbert.transformer.layer.2.attention.v_lin.linear.bias\", \"model.distilbert.transformer.layer.2.attention.v_lin.lora.W_a\", \"model.distilbert.transformer.layer.2.attention.v_lin.lora.W_b\", \"model.distilbert.transformer.layer.3.attention.q_lin.linear.weight\", \"model.distilbert.transformer.layer.3.attention.q_lin.linear.bias\", \"model.distilbert.transformer.layer.3.attention.q_lin.lora.W_a\", \"model.distilbert.transformer.layer.3.attention.q_lin.lora.W_b\", \"model.distilbert.transformer.layer.3.attention.v_lin.linear.weight\", \"model.distilbert.transformer.layer.3.attention.v_lin.linear.bias\", \"model.distilbert.transformer.layer.3.attention.v_lin.lora.W_a\", \"model.distilbert.transformer.layer.3.attention.v_lin.lora.W_b\", \"model.distilbert.transformer.layer.4.attention.q_lin.linear.weight\", \"model.distilbert.transformer.layer.4.attention.q_lin.linear.bias\", \"model.distilbert.transformer.layer.4.attention.q_lin.lora.W_a\", \"model.distilbert.transformer.layer.4.attention.q_lin.lora.W_b\", \"model.distilbert.transformer.layer.4.attention.v_lin.linear.weight\", \"model.distilbert.transformer.layer.4.attention.v_lin.linear.bias\", \"model.distilbert.transformer.layer.4.attention.v_lin.lora.W_a\", \"model.distilbert.transformer.layer.4.attention.v_lin.lora.W_b\", \"model.distilbert.transformer.layer.5.attention.q_lin.linear.weight\", \"model.distilbert.transformer.layer.5.attention.q_lin.linear.bias\", \"model.distilbert.transformer.layer.5.attention.q_lin.lora.W_a\", \"model.distilbert.transformer.layer.5.attention.q_lin.lora.W_b\", \"model.distilbert.transformer.layer.5.attention.v_lin.linear.weight\", \"model.distilbert.transformer.layer.5.attention.v_lin.linear.bias\", \"model.distilbert.transformer.layer.5.attention.v_lin.lora.W_a\", \"model.distilbert.transformer.layer.5.attention.v_lin.lora.W_b\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[62], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Load from a saved model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m lightning_model \u001b[38;5;241m=\u001b[39m \u001b[43mCustomLightningModule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_from_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/projects/fta_bootcamp/trained_models/peft_demo/lora.ckpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# train_acc = trainer.validate(lightning_model, dataloaders=train_loader, verbose=False)\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# val_acc = trainer.validate(lightning_model, dataloaders=val_loader, verbose=False)\u001b[39;00m\n\u001b[1;32m      6\u001b[0m test_acc \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mtest(lightning_model, dataloaders\u001b[38;5;241m=\u001b[39mtest_loader, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/fs01/projects/fta_bootcamp/envs/finetune_demo/lib/python3.10/site-packages/lightning/pytorch/utilities/model_helpers.py:125\u001b[0m, in \u001b[0;36m_restricted_classmethod_impl.__get__.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m instance \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_scripting:\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    122\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe classmethod `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmethod\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` cannot be called on an instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    123\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Please call it on the class type and make sure the return value is used.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    124\u001b[0m     )\n\u001b[0;32m--> 125\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/fs01/projects/fta_bootcamp/envs/finetune_demo/lib/python3.10/site-packages/lightning/pytorch/core/module.py:1581\u001b[0m, in \u001b[0;36mLightningModule.load_from_checkpoint\u001b[0;34m(cls, checkpoint_path, map_location, hparams_file, strict, **kwargs)\u001b[0m\n\u001b[1;32m   1492\u001b[0m \u001b[38;5;129m@_restricted_classmethod\u001b[39m\n\u001b[1;32m   1493\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_from_checkpoint\u001b[39m(\n\u001b[1;32m   1494\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1499\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m   1500\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Self:\n\u001b[1;32m   1501\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Primary way of loading a model from a checkpoint. When Lightning saves a checkpoint it stores the arguments\u001b[39;00m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;124;03m    passed to ``__init__``  in the checkpoint under ``\"hyper_parameters\"``.\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1579\u001b[0m \n\u001b[1;32m   1580\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1581\u001b[0m     loaded \u001b[38;5;241m=\u001b[39m \u001b[43m_load_from_checkpoint\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1582\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1583\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1584\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1585\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhparams_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1586\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1587\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1588\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1589\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(Self, loaded)\n",
      "File \u001b[0;32m/fs01/projects/fta_bootcamp/envs/finetune_demo/lib/python3.10/site-packages/lightning/pytorch/core/saving.py:91\u001b[0m, in \u001b[0;36m_load_from_checkpoint\u001b[0;34m(cls, checkpoint_path, map_location, hparams_file, strict, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _load_state(\u001b[38;5;28mcls\u001b[39m, checkpoint, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mcls\u001b[39m, pl\u001b[38;5;241m.\u001b[39mLightningModule):\n\u001b[0;32m---> 91\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43m_load_state\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m     state_dict \u001b[38;5;241m=\u001b[39m checkpoint[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstate_dict\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m state_dict:\n",
      "File \u001b[0;32m/fs01/projects/fta_bootcamp/envs/finetune_demo/lib/python3.10/site-packages/lightning/pytorch/core/saving.py:180\u001b[0m, in \u001b[0;36m_load_state\u001b[0;34m(cls, checkpoint, strict, **cls_kwargs_new)\u001b[0m\n\u001b[1;32m    177\u001b[0m     obj\u001b[38;5;241m.\u001b[39mon_load_checkpoint(checkpoint)\n\u001b[1;32m    179\u001b[0m \u001b[38;5;66;03m# load the state_dict on the model automatically\u001b[39;00m\n\u001b[0;32m--> 180\u001b[0m keys \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstate_dict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m strict:\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m keys\u001b[38;5;241m.\u001b[39mmissing_keys:\n",
      "File \u001b[0;32m/fs01/projects/fta_bootcamp/envs/finetune_demo/lib/python3.10/site-packages/torch/nn/modules/module.py:2153\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2148\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2149\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2150\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2153\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2154\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2155\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for CustomLightningModule:\n\tMissing key(s) in state_dict: \"model.distilbert.transformer.layer.0.attention.q_lin.weight\", \"model.distilbert.transformer.layer.0.attention.q_lin.bias\", \"model.distilbert.transformer.layer.0.attention.v_lin.weight\", \"model.distilbert.transformer.layer.0.attention.v_lin.bias\", \"model.distilbert.transformer.layer.1.attention.q_lin.weight\", \"model.distilbert.transformer.layer.1.attention.q_lin.bias\", \"model.distilbert.transformer.layer.1.attention.v_lin.weight\", \"model.distilbert.transformer.layer.1.attention.v_lin.bias\", \"model.distilbert.transformer.layer.2.attention.q_lin.weight\", \"model.distilbert.transformer.layer.2.attention.q_lin.bias\", \"model.distilbert.transformer.layer.2.attention.v_lin.weight\", \"model.distilbert.transformer.layer.2.attention.v_lin.bias\", \"model.distilbert.transformer.layer.3.attention.q_lin.weight\", \"model.distilbert.transformer.layer.3.attention.q_lin.bias\", \"model.distilbert.transformer.layer.3.attention.v_lin.weight\", \"model.distilbert.transformer.layer.3.attention.v_lin.bias\", \"model.distilbert.transformer.layer.4.attention.q_lin.weight\", \"model.distilbert.transformer.layer.4.attention.q_lin.bias\", \"model.distilbert.transformer.layer.4.attention.v_lin.weight\", \"model.distilbert.transformer.layer.4.attention.v_lin.bias\", \"model.distilbert.transformer.layer.5.attention.q_lin.weight\", \"model.distilbert.transformer.layer.5.attention.q_lin.bias\", \"model.distilbert.transformer.layer.5.attention.v_lin.weight\", \"model.distilbert.transformer.layer.5.attention.v_lin.bias\". \n\tUnexpected key(s) in state_dict: \"model.distilbert.transformer.layer.0.attention.q_lin.linear.weight\", \"model.distilbert.transformer.layer.0.attention.q_lin.linear.bias\", \"model.distilbert.transformer.layer.0.attention.q_lin.lora.W_a\", \"model.distilbert.transformer.layer.0.attention.q_lin.lora.W_b\", \"model.distilbert.transformer.layer.0.attention.v_lin.linear.weight\", \"model.distilbert.transformer.layer.0.attention.v_lin.linear.bias\", \"model.distilbert.transformer.layer.0.attention.v_lin.lora.W_a\", \"model.distilbert.transformer.layer.0.attention.v_lin.lora.W_b\", \"model.distilbert.transformer.layer.1.attention.q_lin.linear.weight\", \"model.distilbert.transformer.layer.1.attention.q_lin.linear.bias\", \"model.distilbert.transformer.layer.1.attention.q_lin.lora.W_a\", \"model.distilbert.transformer.layer.1.attention.q_lin.lora.W_b\", \"model.distilbert.transformer.layer.1.attention.v_lin.linear.weight\", \"model.distilbert.transformer.layer.1.attention.v_lin.linear.bias\", \"model.distilbert.transformer.layer.1.attention.v_lin.lora.W_a\", \"model.distilbert.transformer.layer.1.attention.v_lin.lora.W_b\", \"model.distilbert.transformer.layer.2.attention.q_lin.linear.weight\", \"model.distilbert.transformer.layer.2.attention.q_lin.linear.bias\", \"model.distilbert.transformer.layer.2.attention.q_lin.lora.W_a\", \"model.distilbert.transformer.layer.2.attention.q_lin.lora.W_b\", \"model.distilbert.transformer.layer.2.attention.v_lin.linear.weight\", \"model.distilbert.transformer.layer.2.attention.v_lin.linear.bias\", \"model.distilbert.transformer.layer.2.attention.v_lin.lora.W_a\", \"model.distilbert.transformer.layer.2.attention.v_lin.lora.W_b\", \"model.distilbert.transformer.layer.3.attention.q_lin.linear.weight\", \"model.distilbert.transformer.layer.3.attention.q_lin.linear.bias\", \"model.distilbert.transformer.layer.3.attention.q_lin.lora.W_a\", \"model.distilbert.transformer.layer.3.attention.q_lin.lora.W_b\", \"model.distilbert.transformer.layer.3.attention.v_lin.linear.weight\", \"model.distilbert.transformer.layer.3.attention.v_lin.linear.bias\", \"model.distilbert.transformer.layer.3.attention.v_lin.lora.W_a\", \"model.distilbert.transformer.layer.3.attention.v_lin.lora.W_b\", \"model.distilbert.transformer.layer.4.attention.q_lin.linear.weight\", \"model.distilbert.transformer.layer.4.attention.q_lin.linear.bias\", \"model.distilbert.transformer.layer.4.attention.q_lin.lora.W_a\", \"model.distilbert.transformer.layer.4.attention.q_lin.lora.W_b\", \"model.distilbert.transformer.layer.4.attention.v_lin.linear.weight\", \"model.distilbert.transformer.layer.4.attention.v_lin.linear.bias\", \"model.distilbert.transformer.layer.4.attention.v_lin.lora.W_a\", \"model.distilbert.transformer.layer.4.attention.v_lin.lora.W_b\", \"model.distilbert.transformer.layer.5.attention.q_lin.linear.weight\", \"model.distilbert.transformer.layer.5.attention.q_lin.linear.bias\", \"model.distilbert.transformer.layer.5.attention.q_lin.lora.W_a\", \"model.distilbert.transformer.layer.5.attention.q_lin.lora.W_b\", \"model.distilbert.transformer.layer.5.attention.v_lin.linear.weight\", \"model.distilbert.transformer.layer.5.attention.v_lin.linear.bias\", \"model.distilbert.transformer.layer.5.attention.v_lin.lora.W_a\", \"model.distilbert.transformer.layer.5.attention.v_lin.lora.W_b\". "
     ]
    }
   ],
   "source": [
    "# Load from a saved model\n",
    "lightning_model = CustomLightningModule.load_from_checkpoint(checkpoint_path=\"/projects/fta_bootcamp/trained_models/peft_demo/lora.ckpt\", model=model)\n",
    "\n",
    "# train_acc = trainer.validate(lightning_model, dataloaders=train_loader, verbose=False)\n",
    "# val_acc = trainer.validate(lightning_model, dataloaders=val_loader, verbose=False)\n",
    "test_acc = trainer.test(lightning_model, dataloaders=test_loader, verbose=False)\n",
    "\n",
    "# print(f\"Train acc: {train_acc[0]['val_acc']*100:2.2f}%\")\n",
    "# print(f\"Val acc:   {val_acc[0]['val_acc']*100:2.2f}%\")\n",
    "print(f\"Test acc:  {test_acc[0]['accuracy']*100:2.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using HF's LoRA\n",
    "\n",
    "We can replace our custom LoRA implementation with an implementation from the [peft library](https://github.com/huggingface/peft). Peft is an open-source, one-stop-shop library from HuggingFace for *parameter efficient fine-tuning* (PEFT) and is integrated with the their [transformers library](https://github.com/huggingface/transformers) for easy model training and inference. \n",
    "\n",
    "Here's a sample snippet for how to prepare a model for PEFT training with LoRA. We can easily fine-tune the DistillBert model we had before with this implementation of the LoRA layer, instead of our custom layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "    inference_mode=False,\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"q_lin\", \"v_lin\"],\n",
    ")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's this [DoRA](https://arxiv.org/pdf/2402.09353) thing I keep hearing about?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME, num_labels=2)\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code inspired by https://github.com/catid/dora/blob/main/dora.py\n",
    "class LinearWithDoRA(torch.nn.Module):\n",
    "    def __init__(self, linear, rank, alpha):\n",
    "        super().__init__()\n",
    "        self.linear = linear\n",
    "        self.lora = LoRALayer(\n",
    "            linear.in_features, linear.out_features, rank, alpha,\n",
    "        )\n",
    "\n",
    "        self.m = nn.Parameter(\n",
    "            self.linear.weight.norm(p=2, dim=0, keepdim=True))\n",
    "\n",
    "    def forward(self, x):\n",
    "        lora = self.lora.W_a @ self.lora.W_b\n",
    "        combined_weight = self.linear.weight + self.lora.alpha*lora.T\n",
    "        column_norm = combined_weight.norm(p=2, dim=0, keepdim=True)\n",
    "        V = combined_weight / column_norm\n",
    "        new_weight = self.m * V\n",
    "        return F.linear(x, new_weight, self.linear.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"lora_query\": True,\n",
    "    \"lora_key\": False,\n",
    "    \"lora_value\": True,\n",
    "    \"lora_projection\": False,\n",
    "    \"lora_mlp\": False,\n",
    "    \"lora_head\": False,\n",
    "}\n",
    "apply_adaptation_layer(model, adaptation_layer=LinearWithDoRA, lora_r=8, lora_alpha=16, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total number of trainable parameters: {count_parameters(model):,}\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetune with DoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lightning_model = CustomLightningModule(model)\n",
    "\n",
    "callbacks = [\n",
    "    ModelCheckpoint(\n",
    "        dirpath=\"\",\n",
    "        filename=\"dora\",\n",
    "        save_top_k=1, # save top 1 model\n",
    "        mode=\"max\",\n",
    "        monitor=\"val_acc\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "logger = CSVLogger(save_dir=\"logs/\", name=\"my-model\")\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    max_epochs=3,\n",
    "    callbacks=callbacks,\n",
    "    accelerator=\"gpu\",\n",
    "    precision=\"16-mixed\",\n",
    "    devices=1,\n",
    "    logger=logger,\n",
    "    log_every_n_steps=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comment cell below if you don't want to go through the training process. You can just load a trained model in the next cell.\n",
    "\n",
    "start = time.time()\n",
    "trainer.fit(model=lightning_model,\n",
    "            train_dataloaders=train_loader,\n",
    "            val_dataloaders=val_loader)\n",
    "\n",
    "end = time.time()\n",
    "elapsed = end - start\n",
    "print(f\"Time elapsed {elapsed/60:.2f} min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from a saved model\n",
    "lightning_model = CustomLightningModule.load_from_checkpoint(checkpoint_path=\"/projects/fta_bootcamp/trained_models/peft_demo/dora.ckpt\", model=model)\n",
    "\n",
    "# train_acc = trainer.validate(lightning_model, dataloaders=train_loader, verbose=False)\n",
    "# val_acc = trainer.validate(lightning_model, dataloaders=val_loader, verbose=False)\n",
    "test_acc = trainer.test(lightning_model, dataloaders=test_loader, verbose=False)\n",
    "\n",
    "# print(f\"Train acc: {train_acc[0]['val_acc']*100:2.2f}%\")\n",
    "# print(f\"Val acc:   {val_acc[0]['val_acc']*100:2.2f}%\")\n",
    "print(f\"Test acc:  {test_acc[0]['accuracy']*100:2.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "finetune_demo",
   "language": "python",
   "name": "finetune_demo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
